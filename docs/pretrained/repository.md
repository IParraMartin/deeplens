# Pretrained SAE Models

## GPT-2 Small (Layer 3)

- **Architecture**: TopK SAE
- **Hidden Size**: 24,576
- **Non-zero Fraction**: 3%
- **Training Dataset**: 1.1M Activations
- **Context Window**: 1,024

[Download Model](https://github.com/iparramartin/deeplens/releases/download/v1.0.0/gpt2_L3_1M.pt){ .md-button .md-button--primary }
[Download Config](https://github.com/iparramartin/deeplens/releases/download/v1.0.0/config.yaml){ .md-button }