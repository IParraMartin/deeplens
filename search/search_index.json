{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DeepLens","text":"<p>An end-to-end library for mechanistic interpretability research on transformer language models.</p>"},{"location":"#what-is-this","title":"What is this?","text":"<p>DeepLens is a comprehensive toolkit that provides everything you need to understand the internal computations of transformer models. From extracting activations to training sparse autoencoders and analyzing learned features, DeepLens offers a complete pipeline for mechanistic interpretability research. Whether you're investigating individual neurons, discovering interpretable features, or running intervention experiments, this library streamlines the entire workflow.</p> <p>It includes a full set of tools that allow end-to-end interpretability pipelines: from feature extraction, to feature steering. The library includes Sparse Autoencoders (TopK and L1), feature extractors, feature dataset modules, and intervention modules. </p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Activation Extraction \u2014 Extract and cache any internal activation from transformer models for analysis</li> <li>Dataset Building \u2014 Construct custom datasets from model activations for training and analysis</li> <li>SAE Training \u2014 Train sparse autoencoders (SAEs) from scratch to discover interpretable feature directions</li> <li>Feature Analysis \u2014 Analyze learned features, compute activation patterns, and understand what features represent</li> <li>Feature Interventions \u2014 Scale, ablate, or modify specific features and observe downstream effects on model behavior</li> <li>End-to-End Pipeline \u2014 Seamlessly go from raw model activations to trained SAEs to mechanistic insights</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from deeplens.extractor import ExtractSingleSample\nfrom deeplens.intervene import InterveneFeatures, ReinjectSingleSample\nfrom deeplens.utils.analysis import plot_topk_distribution, get_top_k_tokens\n\nHF_MODEL = \"gpt2\"\nSAE_MODEL_PT_FILE = \"yourfile.pt\"\nSAE_CONFIG_YAML_FILE = \"yourfile.yaml\"\nLAYER = -1\nTOKEN_POSITION = -1\nALPHA = 100.0\nTEXT = \"Hellow world!\"\n\nextractor = ExtractSingleSample(hf_model=HF_MODEL, layer=LAYER)\nintervene = InterveneFeatures(sae_model=SAE_MODEL_PT_FILE, sae_config=SAE_CONFIG_YAML_FILE)\nreinject = ReinjectSingleSample(hf_model=HF_MODEL)\n\n# Extract the activations\nacts = extractor.get_mlp_acts(TEXT)\n\n# Get alive features from the extracted activations\nfeatures = intervene.get_alive_features(acts, token_position=TOKEN_POSITION)\nprint(f\"{len(features)} alive features discovered at position {TOKEN_POSITION}.\")\n\n# intervene on the features and return the modified activations for the \n# selected token position\nprint(f\"Modifying feature {features[0].item()}\")\n\n# Modify the selected feature\n_, _, modified_decoded = intervene.intervene_feature(\n    activations=acts, \n    feature=features[0].item(),\n    alpha=ALPHA, \n    token_positions=TOKEN_POSITION\n)\n\n# Compute the output logits\nout = reinject.reinject_and_generate(\n      text=TEXT,\n      modified_activations=modified_decoded,\n      layer=LAYER,\n      generate=False\n    )\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Install from source:</p> <pre><code>git clone https://github.com/iparramartin/deeplens\ncd deeplens\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\npip install -r requirements.txt\npip install -e .\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Getting Started guide for a walkthrough of the core concepts, or dive into the tutorials for hands-on examples.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this library in your research, please cite:</p> <pre><code>@software{deeplens2026-parra,\n  author = {I\u00f1igo Parra},\n  title = {DeepLens: An End-to-end Tool for Mechanistic Interpretability},\n  year = {2026},\n  url = {https://github.com/iparramartin/deeplens}\n}\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to DeepLens! This guide will help you get up and running with mechanistic interpretability using Sparse Autoencoders (SAEs).</p>"},{"location":"getting-started/#what-is-deeplens","title":"What is DeepLens?","text":"<p>DeepLens is a comprehensive library for mechanistic interpretability that provides end-to-end tools for:</p> <ul> <li>Feature Extraction: Extract MLP activations from transformer models</li> <li>SAE Training: Train sparse autoencoders with TopK or L1 regularization</li> <li>Feature Analysis: Decode and analyze learned features</li> <li>Feature Intervention: Manipulate features to understand their causal effects</li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.11 or higher</li> <li>CUDA-compatible GPU (recommended for training)</li> <li>Basic knowledge of <code>transformers</code> and <code>PyTorch</code></li> </ul>"},{"location":"getting-started/#basic-workflow","title":"Basic Workflow","text":"<p>DeepLens follows a simple four-step workflow:</p> <ol> <li>Collect Activations</li> <li>Create a Dataset</li> <li>Train a Sparse Autoencoder</li> <li>Analyze Features</li> <li>Intervene on Features</li> </ol>"},{"location":"getting-started/#currently-supported-models","title":"Currently Supported Models","text":"<p>Currently, DeepLens supports:</p> <ul> <li><code>gpt2</code></li> <li><code>gpt2-medium</code></li> <li><code>gpt2-large</code></li> <li><code>gpt2-xl</code></li> <li><code>meta-llama/Llama-2-7b-chat-hf</code></li> <li><code>meta-llama/Llama-3.2-1B</code></li> <li><code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code></li> <li><code>microsoft/phi-2</code></li> <li><code>microsoft/Phi-3.5-mini-instruct</code></li> <li><code>microsoft/Phi-4-mini-instruct</code></li> <li><code>mistralai/Mistral-7B-v0.1</code></li> <li><code>google/gemma-3-270m</code></li> <li><code>google/gemma-7b-it</code></li> <li><code>tiiuae/falcon-7b</code></li> <li><code>Qwen/Qwen2.5-7B-Instruct</code></li> <li><code>deepseek-ai/DeepSeek-R1-0528-Qwen3-8B</code></li> <li><code>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</code></li> </ul> <p>If any errors arise, feel free to open an issue on GitHub.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have DeepLens installed, explore these resources:</p> <ul> <li>Quickstart Guide: Complete example workflows</li> <li>Tutorials: In-depth guides for each component</li> <li>API Reference: Detailed API documentation</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the documentation</li> <li>Search existing GitHub issues</li> <li>Open a new issue with a minimal reproducible example</li> </ol> <p>Now let's learn how to Install DeepLens!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<p>Before installing any dependencies, I recommend creating a new virtual environment to avoid library conflicts.</p> <pre><code>conda create -n deeplens python=3.11\nconda activate deeplens\n</code></pre> <p>The library should work with Python 3.11+, but I recommend using Python 3.11 specifically, as it was used during development. This will help prevent version mismatches and dependency conflicts.</p>"},{"location":"installation/#standard-installation","title":"Standard Installation","text":"<p>To install DeepLens and its dependencies, run the following commands:</p> <pre><code>pip install deeplens-sae\n\n# FOR WINDOWS\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n# OR FOR MAC\npip install torch torchvision\n</code></pre>"},{"location":"installation/#alternative-installation","title":"Alternative Installation","text":"<p>If you encounter any errors with the standard installation, try the manual installation method:</p> <pre><code>pip install -r requirements.txt\n\n# FOR WINDOWS\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n# OR FOR MAC\npip install torch torchvision\n\npip install -e .\n</code></pre>"},{"location":"installation/#common-version-issues","title":"Common Version Issues","text":"<p>During installation, you may encounter version conflicts. The most common issues involve <code>numpy</code> and <code>torch</code> versions.</p>"},{"location":"installation/#numpy-version","title":"NumPy Version","text":"<p>We recommend installing <code>numpy&lt;2</code>, as this version was used during development and has shown the fewest conflicts with other libraries (e.g., <code>scipy</code>).</p>"},{"location":"installation/#pytorch-installation","title":"PyTorch Installation","text":"<p>For CUDA support:</p> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n</code></pre> <p>For MPS/CPU usage:</p> <pre><code>pip install torch torchvision\n</code></pre>"},{"location":"installation/#future-updates","title":"Future Updates","text":"<p>PyPI installation is not yet available. Future versions will support easier installation via pip.</p> <p>Ready to dive deeper? Check out the Quickstart Guide for complete examples!</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will walk you through the core workflow of DeepLens in just a few minutes. You'll learn how to extract activations, train a sparse autoencoder (SAE), and perform feature interventions.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have DeepLens installed. If not, check the Installation Guide.</p>"},{"location":"quickstart/#the-three-step-workflow","title":"The Three-Step Workflow","text":"<p>DeepLens follows a simple three-step pipeline:</p> <ol> <li>Extract activations from a language model</li> <li>Train a sparse autoencoder on those activations</li> <li>Intervene on features to understand their causal effects</li> </ol> <p>Let's walk through each step.</p>"},{"location":"quickstart/#step-1-extract-mlp-activations","title":"Step 1: Extract MLP Activations","text":"<p>First, we'll extract activations from GPT-2's MLP layers using a dataset from HuggingFace. DeepLens handles streaming automatically, so you don't need to download the entire dataset.</p> <pre><code>from deeplens.extractor import FromHuggingFace\n\n# Extract activations from layer 3 of GPT-2\nextractor = FromHuggingFace(\n    hf_model=\"gpt2\",\n    layer=3,\n    dataset_name=\"HuggingFaceFW/fineweb\",\n    num_samples=50000,\n    seq_length=1024,\n    inference_batch_size=16,\n    device=\"auto\",\n    save_features=True  # Saves to 'saved_features' directory\n)\n\n# This will take a few minutes depending on your hardware\nfeatures = extractor.extract_features()\nprint(f\"Extracted features shape: {features.shape}\")\n</code></pre> <p>What's happening:</p> <ul> <li>We're loading GPT-2 and extracting activations from layer 3's MLP</li> <li>Using 50,000 samples from the FineWeb dataset</li> <li>Features are automatically saved to disk for the next step</li> </ul>"},{"location":"quickstart/#step-2-train-a-sparse-autoencoder","title":"Step 2: Train a Sparse Autoencoder","text":"<p>Now we'll train an SAE to discover interpretable features in those activations.</p>"},{"location":"quickstart/#21-create-a-configuration-file","title":"2.1 Create a Configuration File","text":"<p>First, create a <code>config.yaml</code> file with your SAE hyperparameters:</p> <pre><code>input_dims: 3072        # GPT-2 layer 3 MLP dimension\nn_features: 24576       # 8x expansion factor\nactivation: 'relu'\ninput_norm: True\nk: 768                  # Top-k sparsity\nbeta_l1: None           # Use None for TopK, or a value like 0.001 for L1\ntie_weights: False\nunit_norm_decoder: True\n</code></pre>"},{"location":"quickstart/#22-prepare-the-dataset","title":"2.2 Prepare the Dataset","text":"<pre><code>from deeplens.utils.dataset import ActivationsDatasetBuilder\n\n# Load the saved features from Step 1\ndataset = ActivationsDatasetBuilder(\n    activations=\"saved_features/gpt2_layer_3_features.pt\",  # Your saved file\n    splits=[0.8, 0.2],  # 80% train, 20% eval\n    batch_size=16,\n    norm=True\n)\n\ntrain_loader, eval_loader = dataset.get_dataloaders()\n</code></pre>"},{"location":"quickstart/#23-train-the-model","title":"2.3 Train the Model","text":"<pre><code>from deeplens.sae import SparseAutoencoder\nfrom deeplens.train import SAETrainer\nimport torch\n\n# Load configuration and initialize model\nconfig = SAETrainer.config_from_yaml('config.yaml')\nmodel = SparseAutoencoder(**config)\n\n# Set up optimizer\noptimizer = torch.optim.Adam(\n    model.parameters(), \n    lr=3e-4,\n    betas=(0.9, 0.99),\n    weight_decay=0  # Use 1e-4 if tie_weights=False\n)\n\n# Initialize trainer\ntrainer = SAETrainer(\n    model=model,\n    model_name=\"gpt2_layer3_sae\",\n    train_dataloader=train_loader,\n    eval_dataloader=eval_loader,\n    optim=optimizer,\n    epochs=3,\n    bf16=True,\n    random_seed=42,\n    save_checkpoints=True,\n    device=\"auto\",\n    grad_clip_norm=3.0,\n    lrs_type='cosine',\n    eval_steps=1000,\n    warmup_fraction=0.1,\n    save_best_only=True,\n    log_to_wandb=False  # Set to True if you want W&amp;B logging\n)\n\n# Start training\ntrainer.train()\n</code></pre> <p>Training tips:</p> <ul> <li>Training will take from minutes to a hours depending on your dataset size and hardware</li> <li>The best model checkpoint is automatically saved to <code>models/</code></li> <li>Monitor the reconstruction loss and sparsity metrics</li> </ul>"},{"location":"quickstart/#step-3-intervene-on-features","title":"Step 3: Intervene on Features","text":"<p>Now comes the fun part! Let's analyze what features the SAE learned and test their causal effects.</p>"},{"location":"quickstart/#31-extract-features-from-a-single-sample","title":"3.1 Extract Features from a Single Sample","text":"<pre><code>from deeplens.extractor import ExtractSingleSample\n\n# Initialize extractor for the same model and layer\nextractor = ExtractSingleSample(hf_model=\"gpt2\", layer=3)\n\n# Extract activations from a specific text\ntext = \"The Eiffel Tower is located in Paris, France.\"\nactivations = extractor.get_mlp_acts(text)\n</code></pre>"},{"location":"quickstart/#32-find-active-features","title":"3.2 Find Active Features","text":"<pre><code>from deeplens.intervene import InterveneFeatures\n\n# Load your trained SAE\nintervene = InterveneFeatures(\n    sae_model=\"models/gpt2_layer3_sae/best_model.pt\",\n    sae_config=\"config.yaml\"\n)\n\n# Get features active at the last token position\nactive_features = intervene.get_alive_features(\n    activations, \n    token_position=-1\n)\n\nprint(f\"Found {len(active_features)} active features\")\nprint(f\"Top features: {active_features[:5]}\")\n</code></pre>"},{"location":"quickstart/#33-intervene-on-a-specific-feature","title":"3.3 Intervene on a Specific Feature","text":"<pre><code>from deeplens.intervene import ReinjectSingleSample\n\n# Initialize reinjection module\nreinject = ReinjectSingleSample(hf_model=\"gpt2\")\n\n# Pick a feature to modify (e.g., the most active one)\nfeature_to_modify = active_features[0].item()\n\n# Intervene by amplifying the feature\n_, original_acts, modified_acts = intervene.intervene_feature(\n    activations=activations,\n    feature=feature_to_modify,\n    alpha=5.0,  # Amplify 5x\n    token_positions=-1  # Only modify last token\n)\n\n# Generate text with the modified activations\noutput = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=modified_acts,\n    layer=3,\n    generate=True,\n    max_new_tokens=20\n)\n\nprint(\"Original text:\", text)\nprint(\"Modified continuation:\", output)\n</code></pre>"},{"location":"quickstart/#34-compare-original-vs-modified","title":"3.4 Compare Original vs Modified","text":"<pre><code># Generate with original activations\noriginal_output = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=original_acts,\n    layer=3,\n    generate=True,\n    max_new_tokens=20\n)\n\nprint(\"Original:\", original_output)\nprint(\"Modified:\", output)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the full DeepLens pipeline. Here's what to explore next:</p> <ul> <li>Feature Analysis - Learn how to analyze and visualize learned features</li> <li>Intervention Techniques - Explore different ways to modify features</li> <li>Advanced Training - Fine-tune your SAE training for better results</li> <li>API Reference - Dive into the full API documentation</li> </ul>"},{"location":"quickstart/#common-issues","title":"Common Issues","text":"<p>Out of memory during extraction:</p> <ul> <li>Reduce <code>inference_batch_size</code> or <code>seq_length</code></li> <li>Use a smaller model like GPT-2 instead of larger variants</li> </ul> <p>SAE training is slow:</p> <ul> <li>Enable <code>bf16=True</code> for faster training on modern GPUs</li> <li>Reduce <code>num_samples</code> or <code>n_features</code></li> <li>Ensure you're using GPU with <code>device=\"cuda\"</code></li> </ul> <p>Features don't seem interpretable:</p> <ul> <li>Try different sparsity values (adjust <code>k</code> or <code>beta_l1</code>)</li> <li>Train longer (more epochs)</li> <li>Use more training samples</li> <li>Experiment with different layers</li> </ul> <p>Need more help? Check out the tutorials or open an issue on GitHub.</p>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#deeplens.extractor","title":"<code>deeplens.extractor</code>","text":""},{"location":"api/core/#deeplens.extractor.ExtractSingleSample","title":"<code>ExtractSingleSample</code>","text":"<p>Extract MLP activations from individual text samples for analysis and intervention.</p> <p>This class provides functionality to extract activations from single text inputs, useful for interactive analysis, debugging, and testing feature interventions on specific examples.</p> Source code in <code>deeplens/extractor.py</code> <pre><code>class ExtractSingleSample():\n    \"\"\"Extract MLP activations from individual text samples for analysis and intervention.\n\n    This class provides functionality to extract activations from single text inputs,\n    useful for interactive analysis, debugging, and testing feature interventions on\n    specific examples.\n    \"\"\"\n    def __init__(\n            self, \n            hf_model: str = \"gpt2\", \n            layer: int = 3, \n            max_length: int = 1024, \n            device: str = \"auto\",\n            cache_dir: str = 'cache'\n        ) -&gt; None:\n        \"\"\"Initialize the single sample extractor with model configuration.\n\n        Loads the specified model and tokenizer, and configures extraction parameters.\n        The model is set to evaluation mode and moved to the appropriate device.\n\n        Args:\n            hf_model (str, optional): Name or path of the HuggingFace model to load.\n                Should match the model used for sparse autoencoder training for consistency.\n                Defaults to \"gpt2\".\n            layer (int, optional): Index of the transformer layer to extract activations from.\n                Should match the layer used for SAE training. 0-indexed. Defaults to 3.\n            max_length (int, optional): Maximum sequence length for tokenization. Longer\n                sequences will be truncated. Defaults to 1024.\n            device (str, optional): Device for model inference. Can be \"auto\" for automatic\n                selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n            cache_dir (str, optional): Directory to cache downloaded models and datasets.\n                Defaults to 'cache'.\n        \"\"\"\n        os.makedirs(cache_dir, exist_ok=True)\n        self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.layer = layer\n        self.max_length = max_length\n\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        self.model.to(self.device)\n        self.model.eval()\n\n    @torch.no_grad()\n    def get_mlp_acts(self, sample: str) -&gt; torch.Tensor:\n        \"\"\"Extract MLP activations for a single text sample.\n\n        Processes the input text through the model and captures the MLP activations\n        from the configured layer. The hook is automatically removed after extraction.\n\n        Args:\n            sample (str): Input text to process. Can be a word, phrase, or full sentence.\n                Will be tokenized according to the model's tokenizer.\n\n        Returns:\n            torch.Tensor: Activation tensor with shape (sequence_length, hidden_dim),\n                where sequence_length depends on the tokenized length of the input.\n                The batch dimension is squeezed out.\n\n        Note:\n            The activations are automatically moved to CPU to save GPU memory.\n        \"\"\"\n        hook, activations = self.set_forward_hook_and_return_activations(self.layer)\n        tokens = self.tokenize(sample)\n        _ = self.model(**tokens)\n        acts = activations[-1].squeeze()\n        hook.remove()\n        return acts\n\n    def tokenize(self, sample: str) -&gt; dict:\n        \"\"\"Tokenize a single text sample without padding.\n\n        Converts the input text into token IDs suitable for model input. No padding is\n        applied since this is for single sample processing.\n\n        Args:\n            sample (str): Text string to tokenize.\n\n        Returns:\n            dict: Dictionary containing tokenized outputs with 'input_ids', 'attention_mask',\n                and other tokenizer-specific keys as tensors on the configured device.\n                Shape is (1, actual_length) where actual_length \u2264 max_length.\n        \"\"\"\n        return self.tokenizer(\n            sample,\n            truncation=True,\n            padding=False,\n            max_length=self.max_length,\n            return_tensors='pt'\n        ).to(self.device)\n\n    def set_forward_hook_and_return_activations(self, layer_idx) -&gt; tuple:\n        \"\"\"Register a forward hook to capture MLP activations from a specific layer.\n\n        Creates a hook function that captures the output of the MLP activation function\n        at the specified layer during forward passes. Activations are detached and moved\n        to CPU to save GPU memory.\n\n        Args:\n            layer_idx (int): Index of the transformer layer to hook (0-indexed).\n\n        Returns:\n            tuple: A tuple containing:\n                - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later\n                - activations (list): List that will be populated with activation tensors\n                    during forward passes\n        \"\"\"\n        activations = []\n        def hook_fn(module, input, output):\n            activations.append(output.detach().cpu())\n\n        if isinstance(self.model, (\n            transformers.GPT2LMHeadModel, \n            transformers.FalconForCausalLM\n        )):\n            hook = self.model.transformer.h[layer_idx].mlp.act.register_forward_hook(hook_fn)\n        elif isinstance(self.model, (\n            transformers.LlamaForCausalLM, \n            transformers.MistralForCausalLM, \n            transformers.Gemma3ForCausalLM, \n            transformers.GemmaForCausalLM, \n            transformers.Qwen2ForCausalLM,\n            transformers.Qwen3ForCausalLM\n        )):\n            hook = self.model.model.layers[layer_idx].mlp.act_fn.register_forward_hook(hook_fn)\n        elif isinstance(self.model, (\n            transformers.PhiForCausalLM, \n            transformers.Phi3ForCausalLM\n        )):\n            hook = self.model.model.layers[layer_idx].mlp.activation_fn.register_forward_hook(hook_fn)\n        else:\n            raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n        return hook, activations\n</code></pre>"},{"location":"api/core/#deeplens.extractor.ExtractSingleSample.__init__","title":"<code>__init__(hf_model='gpt2', layer=3, max_length=1024, device='auto', cache_dir='cache')</code>","text":"<p>Initialize the single sample extractor with model configuration.</p> <p>Loads the specified model and tokenizer, and configures extraction parameters. The model is set to evaluation mode and moved to the appropriate device.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace model to load. Should match the model used for sparse autoencoder training for consistency. Defaults to \"gpt2\".</p> <code>'gpt2'</code> <code>layer</code> <code>int</code> <p>Index of the transformer layer to extract activations from. Should match the layer used for SAE training. 0-indexed. Defaults to 3.</p> <code>3</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length for tokenization. Longer sequences will be truncated. Defaults to 1024.</p> <code>1024</code> <code>device</code> <code>str</code> <p>Device for model inference. Can be \"auto\" for automatic selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".</p> <code>'auto'</code> <code>cache_dir</code> <code>str</code> <p>Directory to cache downloaded models and datasets. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>deeplens/extractor.py</code> <pre><code>def __init__(\n        self, \n        hf_model: str = \"gpt2\", \n        layer: int = 3, \n        max_length: int = 1024, \n        device: str = \"auto\",\n        cache_dir: str = 'cache'\n    ) -&gt; None:\n    \"\"\"Initialize the single sample extractor with model configuration.\n\n    Loads the specified model and tokenizer, and configures extraction parameters.\n    The model is set to evaluation mode and moved to the appropriate device.\n\n    Args:\n        hf_model (str, optional): Name or path of the HuggingFace model to load.\n            Should match the model used for sparse autoencoder training for consistency.\n            Defaults to \"gpt2\".\n        layer (int, optional): Index of the transformer layer to extract activations from.\n            Should match the layer used for SAE training. 0-indexed. Defaults to 3.\n        max_length (int, optional): Maximum sequence length for tokenization. Longer\n            sequences will be truncated. Defaults to 1024.\n        device (str, optional): Device for model inference. Can be \"auto\" for automatic\n            selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n        cache_dir (str, optional): Directory to cache downloaded models and datasets.\n            Defaults to 'cache'.\n    \"\"\"\n    os.makedirs(cache_dir, exist_ok=True)\n    self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.layer = layer\n    self.max_length = max_length\n\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    self.model.to(self.device)\n    self.model.eval()\n</code></pre>"},{"location":"api/core/#deeplens.extractor.ExtractSingleSample.get_mlp_acts","title":"<code>get_mlp_acts(sample)</code>","text":"<p>Extract MLP activations for a single text sample.</p> <p>Processes the input text through the model and captures the MLP activations from the configured layer. The hook is automatically removed after extraction.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Input text to process. Can be a word, phrase, or full sentence. Will be tokenized according to the model's tokenizer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Activation tensor with shape (sequence_length, hidden_dim), where sequence_length depends on the tokenized length of the input. The batch dimension is squeezed out.</p> Note <p>The activations are automatically moved to CPU to save GPU memory.</p> Source code in <code>deeplens/extractor.py</code> <pre><code>@torch.no_grad()\ndef get_mlp_acts(self, sample: str) -&gt; torch.Tensor:\n    \"\"\"Extract MLP activations for a single text sample.\n\n    Processes the input text through the model and captures the MLP activations\n    from the configured layer. The hook is automatically removed after extraction.\n\n    Args:\n        sample (str): Input text to process. Can be a word, phrase, or full sentence.\n            Will be tokenized according to the model's tokenizer.\n\n    Returns:\n        torch.Tensor: Activation tensor with shape (sequence_length, hidden_dim),\n            where sequence_length depends on the tokenized length of the input.\n            The batch dimension is squeezed out.\n\n    Note:\n        The activations are automatically moved to CPU to save GPU memory.\n    \"\"\"\n    hook, activations = self.set_forward_hook_and_return_activations(self.layer)\n    tokens = self.tokenize(sample)\n    _ = self.model(**tokens)\n    acts = activations[-1].squeeze()\n    hook.remove()\n    return acts\n</code></pre>"},{"location":"api/core/#deeplens.extractor.ExtractSingleSample.set_forward_hook_and_return_activations","title":"<code>set_forward_hook_and_return_activations(layer_idx)</code>","text":"<p>Register a forward hook to capture MLP activations from a specific layer.</p> <p>Creates a hook function that captures the output of the MLP activation function at the specified layer during forward passes. Activations are detached and moved to CPU to save GPU memory.</p> <p>Parameters:</p> Name Type Description Default <code>layer_idx</code> <code>int</code> <p>Index of the transformer layer to hook (0-indexed).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later - activations (list): List that will be populated with activation tensors     during forward passes</p> Source code in <code>deeplens/extractor.py</code> <pre><code>def set_forward_hook_and_return_activations(self, layer_idx) -&gt; tuple:\n    \"\"\"Register a forward hook to capture MLP activations from a specific layer.\n\n    Creates a hook function that captures the output of the MLP activation function\n    at the specified layer during forward passes. Activations are detached and moved\n    to CPU to save GPU memory.\n\n    Args:\n        layer_idx (int): Index of the transformer layer to hook (0-indexed).\n\n    Returns:\n        tuple: A tuple containing:\n            - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later\n            - activations (list): List that will be populated with activation tensors\n                during forward passes\n    \"\"\"\n    activations = []\n    def hook_fn(module, input, output):\n        activations.append(output.detach().cpu())\n\n    if isinstance(self.model, (\n        transformers.GPT2LMHeadModel, \n        transformers.FalconForCausalLM\n    )):\n        hook = self.model.transformer.h[layer_idx].mlp.act.register_forward_hook(hook_fn)\n    elif isinstance(self.model, (\n        transformers.LlamaForCausalLM, \n        transformers.MistralForCausalLM, \n        transformers.Gemma3ForCausalLM, \n        transformers.GemmaForCausalLM, \n        transformers.Qwen2ForCausalLM,\n        transformers.Qwen3ForCausalLM\n    )):\n        hook = self.model.model.layers[layer_idx].mlp.act_fn.register_forward_hook(hook_fn)\n    elif isinstance(self.model, (\n        transformers.PhiForCausalLM, \n        transformers.Phi3ForCausalLM\n    )):\n        hook = self.model.model.layers[layer_idx].mlp.activation_fn.register_forward_hook(hook_fn)\n    else:\n        raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n    return hook, activations\n</code></pre>"},{"location":"api/core/#deeplens.extractor.ExtractSingleSample.tokenize","title":"<code>tokenize(sample)</code>","text":"<p>Tokenize a single text sample without padding.</p> <p>Converts the input text into token IDs suitable for model input. No padding is applied since this is for single sample processing.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>str</code> <p>Text string to tokenize.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing tokenized outputs with 'input_ids', 'attention_mask', and other tokenizer-specific keys as tensors on the configured device. Shape is (1, actual_length) where actual_length \u2264 max_length.</p> Source code in <code>deeplens/extractor.py</code> <pre><code>def tokenize(self, sample: str) -&gt; dict:\n    \"\"\"Tokenize a single text sample without padding.\n\n    Converts the input text into token IDs suitable for model input. No padding is\n    applied since this is for single sample processing.\n\n    Args:\n        sample (str): Text string to tokenize.\n\n    Returns:\n        dict: Dictionary containing tokenized outputs with 'input_ids', 'attention_mask',\n            and other tokenizer-specific keys as tensors on the configured device.\n            Shape is (1, actual_length) where actual_length \u2264 max_length.\n    \"\"\"\n    return self.tokenizer(\n        sample,\n        truncation=True,\n        padding=False,\n        max_length=self.max_length,\n        return_tensors='pt'\n    ).to(self.device)\n</code></pre>"},{"location":"api/core/#deeplens.extractor.FromHuggingFace","title":"<code>FromHuggingFace</code>","text":"<p>Extract MLP activations from transformer models using HuggingFace datasets.</p> <p>This class loads a pre-trained transformer model and processes samples from a streaming dataset to extract and save intermediate layer activations. Designed for collecting training data for sparse autoencoders.</p> Source code in <code>deeplens/extractor.py</code> <pre><code>class FromHuggingFace():\n    \"\"\"Extract MLP activations from transformer models using HuggingFace datasets.\n\n    This class loads a pre-trained transformer model and processes samples from a streaming\n    dataset to extract and save intermediate layer activations. Designed for collecting\n    training data for sparse autoencoders.\n    \"\"\"\n    def __init__(\n            self, \n            hf_model: str = \"gpt2\", \n            layer: int = 6,\n            dataset_name: str = \"HuggingFaceFW/fineweb\",\n            num_samples: int = 100000,\n            seq_length: int = 128,\n            inference_batch_size: int = 16, \n            device: str = \"auto\",\n            save_features: bool = True,\n            cache_dir: str = 'cache'\n        ) -&gt; None:\n        \"\"\"Initialize the activation extractor with model and dataset configuration.\n\n        Loads the specified model and tokenizer, sets up dataset streaming, and configures\n        extraction parameters. The model is set to evaluation mode and moved to the\n        appropriate device.\n\n        Args:\n            hf_model (str, optional): Name or path of the HuggingFace model to load.\n                Should be a valid model identifier (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n                Defaults to \"gpt2\".\n            layer (int, optional): Index of the transformer layer to extract activations from.\n                0-indexed. Defaults to 6.\n            dataset_name (str, optional): Name of the HuggingFace dataset to stream.\n                Must be a valid dataset identifier. Defaults to \"HuggingFaceFW/fineweb\".\n            num_samples (int, optional): Number of samples to extract from the dataset.\n                Defaults to 100000.\n            seq_length (int, optional): Maximum sequence length for tokenization. Sequences\n                will be truncated or padded to this length. Defaults to 128.\n            inference_batch_size (int, optional): Batch size for processing samples through\n                the model. Higher values increase memory usage but improve speed.\n                Defaults to 16.\n            device (str, optional): Device for model inference. Can be \"auto\" for automatic\n                selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n            save_features (bool, optional): Whether to save extracted features to disk in\n                the 'saved_features' directory. Defaults to True.\n            cache_dir (str, optional): Directory to cache downloaded models and datasets.\n                Defaults to 'cache'.\n        \"\"\"\n        os.makedirs(cache_dir, exist_ok=True)\n        self.model_name = hf_model.split('/')[-1]\n        self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.layer = layer\n        self.batch_size = inference_batch_size\n        self.save_features = save_features\n        self.seq_length = seq_length\n        self.num_samples = num_samples\n        self.dataset = load_dataset(\n            dataset_name, \n            split='train',\n            streaming=True,\n            cache_dir=cache_dir\n        ).take(num_samples)\n\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        self.model.to(self.device)\n        self.model.eval()\n\n    def tokenize(self, examples) -&gt; dict:\n        \"\"\"Tokenize text examples with padding and truncation.\n\n        Converts raw text into token IDs suitable for model input, applying padding to\n        seq_length and truncation as needed.\n\n        Args:\n            examples (dict): Dictionary containing a 'text' key with a list of text strings\n                to tokenize.\n\n        Returns:\n            dict: Dictionary with tokenized outputs including 'input_ids', 'attention_mask',\n                and other tokenizer-specific keys. All tensors have shape (batch_size, seq_length).\n        \"\"\"\n        return self.tokenizer(\n            examples['text'],\n            truncation=True,\n            padding='max_length',\n            max_length=self.seq_length,\n            return_tensors='pt'\n        )\n\n    def set_forward_hook_and_return_activations(self, layer_idx) -&gt; tuple:\n        \"\"\"Register a forward hook to capture MLP activations from a specific layer.\n\n        Creates a hook function that captures the output of the MLP activation function\n        at the specified layer during forward passes. Activations are detached and moved\n        to CPU to save GPU memory.\n\n        Args:\n            layer_idx (int): Index of the transformer layer to hook (0-indexed).\n\n        Returns:\n            tuple: A tuple containing:\n                - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later\n                - activations (list): List that will be populated with activation tensors\n                    during forward passes\n        \"\"\"\n        activations = []\n        def hook_fn(module, input, output):\n            activations.append(output.detach().cpu())\n\n        if isinstance(self.model, (\n            transformers.GPT2LMHeadModel, \n            transformers.FalconForCausalLM\n        )):\n            hook = self.model.transformer.h[layer_idx].mlp.act.register_forward_hook(hook_fn)\n        elif isinstance(self.model, (\n            transformers.LlamaForCausalLM, \n            transformers.MistralForCausalLM, \n            transformers.Gemma3ForCausalLM, \n            transformers.GemmaForCausalLM, \n            transformers.Qwen2ForCausalLM,\n            transformers.Qwen3ForCausalLM\n        )):\n            hook = self.model.model.layers[layer_idx].mlp.act_fn.register_forward_hook(hook_fn)\n        elif isinstance(self.model, (\n            transformers.PhiForCausalLM, \n            transformers.Phi3ForCausalLM\n        )):\n            hook = self.model.model.layers[layer_idx].mlp.activation_fn.register_forward_hook(hook_fn)\n        else:\n            raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n        return hook, activations\n\n    @torch.no_grad()\n    def extract_features(self) -&gt; torch.Tensor:\n        \"\"\"Extract MLP activations from the specified layer across the entire dataset.\n\n        Processes the dataset in batches, extracting activations from the configured layer.\n        Filters out padding tokens to ensure only valid activations are collected. Optionally\n        saves the extracted features to disk.\n\n        The extraction process:\n        1. Batches text samples for efficient processing\n        2. Tokenizes and pads/truncates to seq_length\n        3. Runs forward pass and captures activations via hook\n        4. Filters out activations from padding tokens using attention mask\n        5. Concatenates all valid activations into a single tensor\n\n        Returns:\n            torch.Tensor: Concatenated activation tensor with shape (total_tokens, hidden_dim),\n                where total_tokens is the sum of all non-padding tokens across all samples.\n                The tensor is saved to 'saved_features/features_layer_{layer}_{num_tokens}.pt'\n                if save_features=True.\n\n        Note:\n            The hook is automatically removed after extraction to prevent memory leaks.\n            Progress is displayed via tqdm progress bar.\n        \"\"\"\n        hook, activations = self.set_forward_hook_and_return_activations(self.layer)\n        all_activations = []\n        batch_texts = []     \n        for example in tqdm(self.dataset, desc=f\"Extracting from L{self.layer}\", total=self.num_samples):\n            batch_texts.append(example['text'])\n            if len(batch_texts) == self.batch_size:\n                tokens = self.tokenize({'text': batch_texts})\n                tokens = {k: v.to(self.device) for k, v in tokens.items()}\n                _ = self.model(**tokens)\n                batch_acts = activations[-1]\n                attention_mask = tokens[\"attention_mask\"].cpu()\n                for i in range(batch_acts.shape[0]):\n                    non_pad_mask = attention_mask[i].bool()\n                    valid_acts = batch_acts[i][non_pad_mask]\n                    all_activations.append(valid_acts)\n                batch_texts = []\n\n        # for residual text not batched\n        if batch_texts:\n            tokens = self.tokenize({'text': batch_texts})\n            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n            _ = self.model(**tokens)\n            batch_acts = activations[-1]\n            attention_mask = tokens[\"attention_mask\"].cpu()\n            for i in range(batch_acts.shape[0]):\n                non_pad_mask = attention_mask[i].bool()\n                valid_acts = batch_acts[i][non_pad_mask]\n                all_activations.append(valid_acts)\n\n        hook.remove()\n\n        features = torch.cat(all_activations, dim=0)\n        print(f\"Extracting features... (shape: {features.shape})\")\n\n        if self.save_features:\n            os.makedirs(f'saved_features/{self.model_name}', exist_ok=True)\n            save_path = f\"saved_features/{self.model_name}/features_layer_{self.layer}_{features.shape[0]}.pt\"\n            torch.save(features, save_path)\n            print(f\"Features saved to {save_path}\")\n\n        return features\n</code></pre>"},{"location":"api/core/#deeplens.extractor.FromHuggingFace.__init__","title":"<code>__init__(hf_model='gpt2', layer=6, dataset_name='HuggingFaceFW/fineweb', num_samples=100000, seq_length=128, inference_batch_size=16, device='auto', save_features=True, cache_dir='cache')</code>","text":"<p>Initialize the activation extractor with model and dataset configuration.</p> <p>Loads the specified model and tokenizer, sets up dataset streaming, and configures extraction parameters. The model is set to evaluation mode and moved to the appropriate device.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace model to load. Should be a valid model identifier (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\"). Defaults to \"gpt2\".</p> <code>'gpt2'</code> <code>layer</code> <code>int</code> <p>Index of the transformer layer to extract activations from. 0-indexed. Defaults to 6.</p> <code>6</code> <code>dataset_name</code> <code>str</code> <p>Name of the HuggingFace dataset to stream. Must be a valid dataset identifier. Defaults to \"HuggingFaceFW/fineweb\".</p> <code>'HuggingFaceFW/fineweb'</code> <code>num_samples</code> <code>int</code> <p>Number of samples to extract from the dataset. Defaults to 100000.</p> <code>100000</code> <code>seq_length</code> <code>int</code> <p>Maximum sequence length for tokenization. Sequences will be truncated or padded to this length. Defaults to 128.</p> <code>128</code> <code>inference_batch_size</code> <code>int</code> <p>Batch size for processing samples through the model. Higher values increase memory usage but improve speed. Defaults to 16.</p> <code>16</code> <code>device</code> <code>str</code> <p>Device for model inference. Can be \"auto\" for automatic selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".</p> <code>'auto'</code> <code>save_features</code> <code>bool</code> <p>Whether to save extracted features to disk in the 'saved_features' directory. Defaults to True.</p> <code>True</code> <code>cache_dir</code> <code>str</code> <p>Directory to cache downloaded models and datasets. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>deeplens/extractor.py</code> <pre><code>def __init__(\n        self, \n        hf_model: str = \"gpt2\", \n        layer: int = 6,\n        dataset_name: str = \"HuggingFaceFW/fineweb\",\n        num_samples: int = 100000,\n        seq_length: int = 128,\n        inference_batch_size: int = 16, \n        device: str = \"auto\",\n        save_features: bool = True,\n        cache_dir: str = 'cache'\n    ) -&gt; None:\n    \"\"\"Initialize the activation extractor with model and dataset configuration.\n\n    Loads the specified model and tokenizer, sets up dataset streaming, and configures\n    extraction parameters. The model is set to evaluation mode and moved to the\n    appropriate device.\n\n    Args:\n        hf_model (str, optional): Name or path of the HuggingFace model to load.\n            Should be a valid model identifier (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n            Defaults to \"gpt2\".\n        layer (int, optional): Index of the transformer layer to extract activations from.\n            0-indexed. Defaults to 6.\n        dataset_name (str, optional): Name of the HuggingFace dataset to stream.\n            Must be a valid dataset identifier. Defaults to \"HuggingFaceFW/fineweb\".\n        num_samples (int, optional): Number of samples to extract from the dataset.\n            Defaults to 100000.\n        seq_length (int, optional): Maximum sequence length for tokenization. Sequences\n            will be truncated or padded to this length. Defaults to 128.\n        inference_batch_size (int, optional): Batch size for processing samples through\n            the model. Higher values increase memory usage but improve speed.\n            Defaults to 16.\n        device (str, optional): Device for model inference. Can be \"auto\" for automatic\n            selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n        save_features (bool, optional): Whether to save extracted features to disk in\n            the 'saved_features' directory. Defaults to True.\n        cache_dir (str, optional): Directory to cache downloaded models and datasets.\n            Defaults to 'cache'.\n    \"\"\"\n    os.makedirs(cache_dir, exist_ok=True)\n    self.model_name = hf_model.split('/')[-1]\n    self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    self.layer = layer\n    self.batch_size = inference_batch_size\n    self.save_features = save_features\n    self.seq_length = seq_length\n    self.num_samples = num_samples\n    self.dataset = load_dataset(\n        dataset_name, \n        split='train',\n        streaming=True,\n        cache_dir=cache_dir\n    ).take(num_samples)\n\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    self.model.to(self.device)\n    self.model.eval()\n</code></pre>"},{"location":"api/core/#deeplens.extractor.FromHuggingFace.extract_features","title":"<code>extract_features()</code>","text":"<p>Extract MLP activations from the specified layer across the entire dataset.</p> <p>Processes the dataset in batches, extracting activations from the configured layer. Filters out padding tokens to ensure only valid activations are collected. Optionally saves the extracted features to disk.</p> <p>The extraction process: 1. Batches text samples for efficient processing 2. Tokenizes and pads/truncates to seq_length 3. Runs forward pass and captures activations via hook 4. Filters out activations from padding tokens using attention mask 5. Concatenates all valid activations into a single tensor</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Concatenated activation tensor with shape (total_tokens, hidden_dim), where total_tokens is the sum of all non-padding tokens across all samples. The tensor is saved to 'saved_features/features_layer_{layer}_{num_tokens}.pt' if save_features=True.</p> Note <p>The hook is automatically removed after extraction to prevent memory leaks. Progress is displayed via tqdm progress bar.</p> Source code in <code>deeplens/extractor.py</code> <pre><code>@torch.no_grad()\ndef extract_features(self) -&gt; torch.Tensor:\n    \"\"\"Extract MLP activations from the specified layer across the entire dataset.\n\n    Processes the dataset in batches, extracting activations from the configured layer.\n    Filters out padding tokens to ensure only valid activations are collected. Optionally\n    saves the extracted features to disk.\n\n    The extraction process:\n    1. Batches text samples for efficient processing\n    2. Tokenizes and pads/truncates to seq_length\n    3. Runs forward pass and captures activations via hook\n    4. Filters out activations from padding tokens using attention mask\n    5. Concatenates all valid activations into a single tensor\n\n    Returns:\n        torch.Tensor: Concatenated activation tensor with shape (total_tokens, hidden_dim),\n            where total_tokens is the sum of all non-padding tokens across all samples.\n            The tensor is saved to 'saved_features/features_layer_{layer}_{num_tokens}.pt'\n            if save_features=True.\n\n    Note:\n        The hook is automatically removed after extraction to prevent memory leaks.\n        Progress is displayed via tqdm progress bar.\n    \"\"\"\n    hook, activations = self.set_forward_hook_and_return_activations(self.layer)\n    all_activations = []\n    batch_texts = []     \n    for example in tqdm(self.dataset, desc=f\"Extracting from L{self.layer}\", total=self.num_samples):\n        batch_texts.append(example['text'])\n        if len(batch_texts) == self.batch_size:\n            tokens = self.tokenize({'text': batch_texts})\n            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n            _ = self.model(**tokens)\n            batch_acts = activations[-1]\n            attention_mask = tokens[\"attention_mask\"].cpu()\n            for i in range(batch_acts.shape[0]):\n                non_pad_mask = attention_mask[i].bool()\n                valid_acts = batch_acts[i][non_pad_mask]\n                all_activations.append(valid_acts)\n            batch_texts = []\n\n    # for residual text not batched\n    if batch_texts:\n        tokens = self.tokenize({'text': batch_texts})\n        tokens = {k: v.to(self.device) for k, v in tokens.items()}\n        _ = self.model(**tokens)\n        batch_acts = activations[-1]\n        attention_mask = tokens[\"attention_mask\"].cpu()\n        for i in range(batch_acts.shape[0]):\n            non_pad_mask = attention_mask[i].bool()\n            valid_acts = batch_acts[i][non_pad_mask]\n            all_activations.append(valid_acts)\n\n    hook.remove()\n\n    features = torch.cat(all_activations, dim=0)\n    print(f\"Extracting features... (shape: {features.shape})\")\n\n    if self.save_features:\n        os.makedirs(f'saved_features/{self.model_name}', exist_ok=True)\n        save_path = f\"saved_features/{self.model_name}/features_layer_{self.layer}_{features.shape[0]}.pt\"\n        torch.save(features, save_path)\n        print(f\"Features saved to {save_path}\")\n\n    return features\n</code></pre>"},{"location":"api/core/#deeplens.extractor.FromHuggingFace.set_forward_hook_and_return_activations","title":"<code>set_forward_hook_and_return_activations(layer_idx)</code>","text":"<p>Register a forward hook to capture MLP activations from a specific layer.</p> <p>Creates a hook function that captures the output of the MLP activation function at the specified layer during forward passes. Activations are detached and moved to CPU to save GPU memory.</p> <p>Parameters:</p> Name Type Description Default <code>layer_idx</code> <code>int</code> <p>Index of the transformer layer to hook (0-indexed).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later - activations (list): List that will be populated with activation tensors     during forward passes</p> Source code in <code>deeplens/extractor.py</code> <pre><code>def set_forward_hook_and_return_activations(self, layer_idx) -&gt; tuple:\n    \"\"\"Register a forward hook to capture MLP activations from a specific layer.\n\n    Creates a hook function that captures the output of the MLP activation function\n    at the specified layer during forward passes. Activations are detached and moved\n    to CPU to save GPU memory.\n\n    Args:\n        layer_idx (int): Index of the transformer layer to hook (0-indexed).\n\n    Returns:\n        tuple: A tuple containing:\n            - hook (torch.utils.hooks.RemovableHandle): Handle to remove the hook later\n            - activations (list): List that will be populated with activation tensors\n                during forward passes\n    \"\"\"\n    activations = []\n    def hook_fn(module, input, output):\n        activations.append(output.detach().cpu())\n\n    if isinstance(self.model, (\n        transformers.GPT2LMHeadModel, \n        transformers.FalconForCausalLM\n    )):\n        hook = self.model.transformer.h[layer_idx].mlp.act.register_forward_hook(hook_fn)\n    elif isinstance(self.model, (\n        transformers.LlamaForCausalLM, \n        transformers.MistralForCausalLM, \n        transformers.Gemma3ForCausalLM, \n        transformers.GemmaForCausalLM, \n        transformers.Qwen2ForCausalLM,\n        transformers.Qwen3ForCausalLM\n    )):\n        hook = self.model.model.layers[layer_idx].mlp.act_fn.register_forward_hook(hook_fn)\n    elif isinstance(self.model, (\n        transformers.PhiForCausalLM, \n        transformers.Phi3ForCausalLM\n    )):\n        hook = self.model.model.layers[layer_idx].mlp.activation_fn.register_forward_hook(hook_fn)\n    else:\n        raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n    return hook, activations\n</code></pre>"},{"location":"api/core/#deeplens.extractor.FromHuggingFace.tokenize","title":"<code>tokenize(examples)</code>","text":"<p>Tokenize text examples with padding and truncation.</p> <p>Converts raw text into token IDs suitable for model input, applying padding to seq_length and truncation as needed.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>dict</code> <p>Dictionary containing a 'text' key with a list of text strings to tokenize.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with tokenized outputs including 'input_ids', 'attention_mask', and other tokenizer-specific keys. All tensors have shape (batch_size, seq_length).</p> Source code in <code>deeplens/extractor.py</code> <pre><code>def tokenize(self, examples) -&gt; dict:\n    \"\"\"Tokenize text examples with padding and truncation.\n\n    Converts raw text into token IDs suitable for model input, applying padding to\n    seq_length and truncation as needed.\n\n    Args:\n        examples (dict): Dictionary containing a 'text' key with a list of text strings\n            to tokenize.\n\n    Returns:\n        dict: Dictionary with tokenized outputs including 'input_ids', 'attention_mask',\n            and other tokenizer-specific keys. All tensors have shape (batch_size, seq_length).\n    \"\"\"\n    return self.tokenizer(\n        examples['text'],\n        truncation=True,\n        padding='max_length',\n        max_length=self.seq_length,\n        return_tensors='pt'\n    )\n</code></pre>"},{"location":"api/core/#deeplens.intervene","title":"<code>deeplens.intervene</code>","text":""},{"location":"api/core/#deeplens.intervene.InterveneFeatures","title":"<code>InterveneFeatures</code>","text":"<p>Manipulate and intervene on sparse autoencoder latent features.</p> <p>This class loads a trained sparse autoencoder and provides methods to analyze and modify its latent feature space, enabling causal analysis of feature effects on model behavior.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>class InterveneFeatures():\n    \"\"\"Manipulate and intervene on sparse autoencoder latent features.\n\n    This class loads a trained sparse autoencoder and provides methods to analyze\n    and modify its latent feature space, enabling causal analysis of feature effects\n    on model behavior.\n    \"\"\"\n    def __init__(\n            self,\n            sae_model: str = None,\n            sae_config: str | dict = None,\n            device: str = \"auto\"\n        ):\n        \"\"\"Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.\n\n        This class provides functionality to load a trained sparse autoencoder model and\n        intervene on its latent feature space to analyze and modify activations.\n\n        Args:\n            sae_model (str, optional): Path to the trained sparse autoencoder model weights file.\n                Should be a .pt or .pth file containing the model state dict. Defaults to None.\n            sae_config (str | dict, optional): Configuration for the sparse autoencoder.\n                Can be either a dictionary containing model hyperparameters or a path to a\n                YAML configuration file. Defaults to None.\n            device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n                selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n        \"\"\"\n        self.model_dir = sae_model\n\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        if isinstance(sae_config, dict):\n            self.model_config = sae_config\n        elif isinstance(sae_config, str) and sae_config.endswith(\".yaml\"):\n            self.model_config = self.config_from_yaml(sae_config)\n        else:\n            raise ValueError(\"sae_config must be dict or path to .yaml file.\")\n\n        self.model = self.load_model()\n\n    @torch.no_grad()\n    def get_decoded(self, activations) -&gt; torch.Tensor:\n        \"\"\"Encode input activations through the sparse autoencoder to get latent features.\n\n        Passes the input activations through the sparse autoencoder's forward pass and\n        returns the latent feature representation (z) from the encoded space.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode. Can be a\n                PyTorch tensor or any array-like structure that can be converted to a tensor.\n\n        Returns:\n            torch.Tensor: The latent feature representation (z) from the sparse autoencoder's\n                encoded space.\n        \"\"\"\n        if not isinstance(activations, torch.Tensor):\n            activations = torch.Tensor(activations)\n        activations = activations.to(self.device)\n        _, z, _ = self.model(activations)\n        return z\n\n    @torch.no_grad()\n    def get_alive_features(\n            self, \n            activations: torch.Tensor, \n            token_position: int = -1, \n            k: int | None = None,\n            return_values: bool = False\n        ) -&gt; torch.Tensor:\n        \"\"\"Get indices of non-zero (active) features in the latent space for a specific token.\n\n        Encodes the input activations through the sparse autoencoder and identifies which\n        latent features are active (non-zero) at the specified token position.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode. Can be a\n                PyTorch tensor or any array-like structure that can be converted to a tensor.\n            token_position (int, optional): Position of the token in the sequence to analyze.\n                Use -1 for the last token. Defaults to -1.\n            k (int, optional): If provided, returns only the top-k most active features\n                instead of all non-zero features. Defaults to None.\n            return_values (bool, optional): If True, returns both indices and values.\n                Defaults to False.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False,\n                returns a 1D tensor containing the indices of non-zero features. If True,\n                returns a tuple of (indices, values).\n        \"\"\"\n        z = self.get_decoded(activations)\n        z_token = z[token_position]\n        if k is not None:\n            topk_result = torch.topk(z_token, k=k)\n            feature_idxs = topk_result.indices\n            feature_vals = topk_result.values\n        else:\n            feature_idxs = torch.nonzero(z_token != 0, as_tuple=False).squeeze(-1)\n            feature_vals = z_token[feature_idxs]\n        if return_values:\n            return feature_idxs, feature_vals\n        return feature_idxs\n\n    @torch.no_grad()\n    def intervene_feature(\n            self, \n            activations, \n            feature: int, \n            alpha: float = 2.0,\n            token_positions: int | list[int] | None = None\n        ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Intervene on a specific latent feature by scaling its activation.\n\n        Encodes the input activations, multiplies the specified feature by alpha at the\n        given token positions, and returns both the original and modified decoded outputs\n        for comparison.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode and modify.\n                Can be a PyTorch tensor or any array-like structure.\n            feature (int): Index of the latent feature to intervene on.\n            alpha (float, optional): Scaling factor to apply to the feature. Values &gt; 1\n                amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.\n            token_positions (int | list[int] | None, optional): Token position(s) at which\n                to apply the intervention. If None, applies to all tokens. If int, applies\n                to a single position. If list, applies to multiple positions. Defaults to None.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n                - activations: The original input activations\n                - original_decoded: Decoded output without intervention\n                - modified_decoded: Decoded output with the feature intervention applied\n        \"\"\"\n        if not isinstance(activations, torch.Tensor):\n            activations = torch.Tensor(activations).unsqueeze(0)\n\n        activations = activations.to(self.device)\n        _, z, _ = self.model(activations)\n        modified = z.clone()\n\n        if token_positions is None:\n            modified[:, feature] *= alpha\n        elif isinstance(token_positions, int):\n            modified[token_positions, feature] *= alpha\n        else:\n            for pos in token_positions:\n                modified[pos, feature] *= alpha\n\n        original_decoded = self.model.decode(z)\n        modified_decoded = self.model.decode(modified)\n        return activations, original_decoded, modified_decoded\n\n    def load_model(self) -&gt; torch.nn.Module:\n        \"\"\"Load the sparse autoencoder model from disk.\n\n        Loads the model weights from the specified path and initializes a\n        SparseAutoencoder instance with the provided configuration.\n\n        Returns:\n            torch.nn.Module: The loaded sparse autoencoder model moved to the\n                appropriate device.\n        \"\"\"\n        weights = torch.load(self.model_dir, map_location=self.device)\n        model = SparseAutoencoder(**self.model_config)\n        model.load_state_dict(state_dict=weights)\n        return model.to(self.device)\n\n    def config_from_yaml(self, file) -&gt; dict:\n        \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n        Reads and parses a YAML configuration file containing the hyperparameters\n        for the sparse autoencoder model.\n\n        Args:\n            file (str): Path to the YAML configuration file.\n\n        Returns:\n            dict: Dictionary containing the model configuration parameters.\n        \"\"\"\n        try:\n            with open(file, \"r\") as f:\n                config = yaml.safe_load(f)\n            return config\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Config file not found: {file}\")\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Invalid YAML in {file}: {e}\")\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.__init__","title":"<code>__init__(sae_model=None, sae_config=None, device='auto')</code>","text":"<p>Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.</p> <p>This class provides functionality to load a trained sparse autoencoder model and intervene on its latent feature space to analyze and modify activations.</p> <p>Parameters:</p> Name Type Description Default <code>sae_model</code> <code>str</code> <p>Path to the trained sparse autoencoder model weights file. Should be a .pt or .pth file containing the model state dict. Defaults to None.</p> <code>None</code> <code>sae_config</code> <code>str | dict</code> <p>Configuration for the sparse autoencoder. Can be either a dictionary containing model hyperparameters or a path to a YAML configuration file. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to run computations on. Can be \"auto\" for automatic selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".</p> <code>'auto'</code> Source code in <code>deeplens/intervene.py</code> <pre><code>def __init__(\n        self,\n        sae_model: str = None,\n        sae_config: str | dict = None,\n        device: str = \"auto\"\n    ):\n    \"\"\"Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.\n\n    This class provides functionality to load a trained sparse autoencoder model and\n    intervene on its latent feature space to analyze and modify activations.\n\n    Args:\n        sae_model (str, optional): Path to the trained sparse autoencoder model weights file.\n            Should be a .pt or .pth file containing the model state dict. Defaults to None.\n        sae_config (str | dict, optional): Configuration for the sparse autoencoder.\n            Can be either a dictionary containing model hyperparameters or a path to a\n            YAML configuration file. Defaults to None.\n        device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n            selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n    \"\"\"\n    self.model_dir = sae_model\n\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    if isinstance(sae_config, dict):\n        self.model_config = sae_config\n    elif isinstance(sae_config, str) and sae_config.endswith(\".yaml\"):\n        self.model_config = self.config_from_yaml(sae_config)\n    else:\n        raise ValueError(\"sae_config must be dict or path to .yaml file.\")\n\n    self.model = self.load_model()\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.config_from_yaml","title":"<code>config_from_yaml(file)</code>","text":"<p>Load sparse autoencoder configuration from a YAML file.</p> <p>Reads and parses a YAML configuration file containing the hyperparameters for the sparse autoencoder model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the model configuration parameters.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def config_from_yaml(self, file) -&gt; dict:\n    \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n    Reads and parses a YAML configuration file containing the hyperparameters\n    for the sparse autoencoder model.\n\n    Args:\n        file (str): Path to the YAML configuration file.\n\n    Returns:\n        dict: Dictionary containing the model configuration parameters.\n    \"\"\"\n    try:\n        with open(file, \"r\") as f:\n            config = yaml.safe_load(f)\n        return config\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Config file not found: {file}\")\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Invalid YAML in {file}: {e}\")\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.get_alive_features","title":"<code>get_alive_features(activations, token_position=-1, k=None, return_values=False)</code>","text":"<p>Get indices of non-zero (active) features in the latent space for a specific token.</p> <p>Encodes the input activations through the sparse autoencoder and identifies which latent features are active (non-zero) at the specified token position.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode. Can be a PyTorch tensor or any array-like structure that can be converted to a tensor.</p> required <code>token_position</code> <code>int</code> <p>Position of the token in the sequence to analyze. Use -1 for the last token. Defaults to -1.</p> <code>-1</code> <code>k</code> <code>int</code> <p>If provided, returns only the top-k most active features instead of all non-zero features. Defaults to None.</p> <code>None</code> <code>return_values</code> <code>bool</code> <p>If True, returns both indices and values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False, returns a 1D tensor containing the indices of non-zero features. If True, returns a tuple of (indices, values).</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef get_alive_features(\n        self, \n        activations: torch.Tensor, \n        token_position: int = -1, \n        k: int | None = None,\n        return_values: bool = False\n    ) -&gt; torch.Tensor:\n    \"\"\"Get indices of non-zero (active) features in the latent space for a specific token.\n\n    Encodes the input activations through the sparse autoencoder and identifies which\n    latent features are active (non-zero) at the specified token position.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode. Can be a\n            PyTorch tensor or any array-like structure that can be converted to a tensor.\n        token_position (int, optional): Position of the token in the sequence to analyze.\n            Use -1 for the last token. Defaults to -1.\n        k (int, optional): If provided, returns only the top-k most active features\n            instead of all non-zero features. Defaults to None.\n        return_values (bool, optional): If True, returns both indices and values.\n            Defaults to False.\n\n    Returns:\n        torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False,\n            returns a 1D tensor containing the indices of non-zero features. If True,\n            returns a tuple of (indices, values).\n    \"\"\"\n    z = self.get_decoded(activations)\n    z_token = z[token_position]\n    if k is not None:\n        topk_result = torch.topk(z_token, k=k)\n        feature_idxs = topk_result.indices\n        feature_vals = topk_result.values\n    else:\n        feature_idxs = torch.nonzero(z_token != 0, as_tuple=False).squeeze(-1)\n        feature_vals = z_token[feature_idxs]\n    if return_values:\n        return feature_idxs, feature_vals\n    return feature_idxs\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.get_decoded","title":"<code>get_decoded(activations)</code>","text":"<p>Encode input activations through the sparse autoencoder to get latent features.</p> <p>Passes the input activations through the sparse autoencoder's forward pass and returns the latent feature representation (z) from the encoded space.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode. Can be a PyTorch tensor or any array-like structure that can be converted to a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The latent feature representation (z) from the sparse autoencoder's encoded space.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef get_decoded(self, activations) -&gt; torch.Tensor:\n    \"\"\"Encode input activations through the sparse autoencoder to get latent features.\n\n    Passes the input activations through the sparse autoencoder's forward pass and\n    returns the latent feature representation (z) from the encoded space.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode. Can be a\n            PyTorch tensor or any array-like structure that can be converted to a tensor.\n\n    Returns:\n        torch.Tensor: The latent feature representation (z) from the sparse autoencoder's\n            encoded space.\n    \"\"\"\n    if not isinstance(activations, torch.Tensor):\n        activations = torch.Tensor(activations)\n    activations = activations.to(self.device)\n    _, z, _ = self.model(activations)\n    return z\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.intervene_feature","title":"<code>intervene_feature(activations, feature, alpha=2.0, token_positions=None)</code>","text":"<p>Intervene on a specific latent feature by scaling its activation.</p> <p>Encodes the input activations, multiplies the specified feature by alpha at the given token positions, and returns both the original and modified decoded outputs for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode and modify. Can be a PyTorch tensor or any array-like structure.</p> required <code>feature</code> <code>int</code> <p>Index of the latent feature to intervene on.</p> required <code>alpha</code> <code>float</code> <p>Scaling factor to apply to the feature. Values &gt; 1 amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.</p> <code>2.0</code> <code>token_positions</code> <code>int | list[int] | None</code> <p>Token position(s) at which to apply the intervention. If None, applies to all tokens. If int, applies to a single position. If list, applies to multiple positions. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing: - activations: The original input activations - original_decoded: Decoded output without intervention - modified_decoded: Decoded output with the feature intervention applied</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef intervene_feature(\n        self, \n        activations, \n        feature: int, \n        alpha: float = 2.0,\n        token_positions: int | list[int] | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Intervene on a specific latent feature by scaling its activation.\n\n    Encodes the input activations, multiplies the specified feature by alpha at the\n    given token positions, and returns both the original and modified decoded outputs\n    for comparison.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode and modify.\n            Can be a PyTorch tensor or any array-like structure.\n        feature (int): Index of the latent feature to intervene on.\n        alpha (float, optional): Scaling factor to apply to the feature. Values &gt; 1\n            amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.\n        token_positions (int | list[int] | None, optional): Token position(s) at which\n            to apply the intervention. If None, applies to all tokens. If int, applies\n            to a single position. If list, applies to multiple positions. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n            - activations: The original input activations\n            - original_decoded: Decoded output without intervention\n            - modified_decoded: Decoded output with the feature intervention applied\n    \"\"\"\n    if not isinstance(activations, torch.Tensor):\n        activations = torch.Tensor(activations).unsqueeze(0)\n\n    activations = activations.to(self.device)\n    _, z, _ = self.model(activations)\n    modified = z.clone()\n\n    if token_positions is None:\n        modified[:, feature] *= alpha\n    elif isinstance(token_positions, int):\n        modified[token_positions, feature] *= alpha\n    else:\n        for pos in token_positions:\n            modified[pos, feature] *= alpha\n\n    original_decoded = self.model.decode(z)\n    modified_decoded = self.model.decode(modified)\n    return activations, original_decoded, modified_decoded\n</code></pre>"},{"location":"api/core/#deeplens.intervene.InterveneFeatures.load_model","title":"<code>load_model()</code>","text":"<p>Load the sparse autoencoder model from disk.</p> <p>Loads the model weights from the specified path and initializes a SparseAutoencoder instance with the provided configuration.</p> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The loaded sparse autoencoder model moved to the appropriate device.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def load_model(self) -&gt; torch.nn.Module:\n    \"\"\"Load the sparse autoencoder model from disk.\n\n    Loads the model weights from the specified path and initializes a\n    SparseAutoencoder instance with the provided configuration.\n\n    Returns:\n        torch.nn.Module: The loaded sparse autoencoder model moved to the\n            appropriate device.\n    \"\"\"\n    weights = torch.load(self.model_dir, map_location=self.device)\n    model = SparseAutoencoder(**self.model_config)\n    model.load_state_dict(state_dict=weights)\n    return model.to(self.device)\n</code></pre>"},{"location":"api/core/#deeplens.intervene.ReinjectSingleSample","title":"<code>ReinjectSingleSample</code>","text":"<p>Reinject modified activations into a language model for causal inference.</p> <p>This class enables injecting modified activations back into a transformer model's forward pass to observe the causal effects on model predictions and generated text. Useful for validating feature interventions and conducting mechanistic interpretability experiments.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>class ReinjectSingleSample():\n    \"\"\"Reinject modified activations into a language model for causal inference.\n\n    This class enables injecting modified activations back into a transformer model's\n    forward pass to observe the causal effects on model predictions and generated text.\n    Useful for validating feature interventions and conducting mechanistic interpretability\n    experiments.\n    \"\"\"\n    def __init__(\n            self, \n            hf_model: str, \n            device: str = \"auto\", \n            cache_dir: str = 'cache'\n        ):\n        \"\"\"Initialize the ReinjectSingleSample class for causal inference with modified activations.\n\n        Loads a HuggingFace causal language model and tokenizer to enable reinjection of\n        modified activations into the model's forward pass for text generation and analysis.\n\n        Args:\n            hf_model (str): Name or path of the HuggingFace model to load.\n                Should be a valid model identifier from the HuggingFace model hub\n                (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n            device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n                selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n            cache_dir (str, optional): Directory to cache downloaded models.\n                Defaults to 'cache'.\n        \"\"\"\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        os.makedirs(cache_dir, exist_ok=True)\n        self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.model.eval()\n\n    @torch.no_grad()\n    def reinject_and_generate(\n            self, \n            text, \n            modified_activations, \n            layer: int = 3, \n            generate: bool = False, \n            max_new_tokens: int = 25, \n            temperature: float = 1.0\n        ) -&gt; torch.Tensor | str:\n        \"\"\"Reinject modified activations into a model layer and optionally generate text.\n\n        Replaces the activations at the specified layer with the provided modified activations\n        during the forward pass. Can either return logits for the input text or generate\n        new tokens autoregressively.\n\n        Args:\n            text (str): Input text to tokenize and process through the model.\n            modified_activations (torch.Tensor): The modified activations to inject at the\n                specified layer. Should have the appropriate shape for the layer's output.\n            layer (int, optional): Index of the transformer layer where activations should\n                be replaced. Defaults to 3.\n            generate (bool, optional): If True, generates new tokens autoregressively.\n                If False, only returns logits for the input. Defaults to False.\n            max_new_tokens (int, optional): Maximum number of new tokens to generate when\n                generate=True. Defaults to 25.\n            temperature (float, optional): Sampling temperature for generation. Higher values\n                (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic.\n                Set to 0 for greedy decoding. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor | str: If generate=False, returns the model's logits as a tensor.\n                If generate=True, returns the generated text as a string.\n\n        Note:\n            The hook is automatically removed after execution to prevent interference\n            with subsequent forward passes. For generation mode, the hook only affects\n            the first forward pass to avoid applying the intervention to newly generated tokens.\n        \"\"\"\n        modified_activations = modified_activations.to(self.device)\n        call_count = [0]\n        def replacement_hook(module, input, output):\n            if generate and call_count[0] &gt; 0:\n                return output\n            call_count[0] += 1\n            return modified_activations\n\n        mlp_module = self.get_module_for_replacement_hook(layer_idx=layer)\n        hook = mlp_module.register_forward_hook(replacement_hook)\n        tokens = self.tokenizer(text, return_tensors='pt').to(self.device)\n        try:\n            if generate:\n                generated_ids = self.model.generate(\n                    **tokens,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature,\n                    do_sample=temperature &gt; 0\n                )\n                return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n            else:\n                out = self.model(**tokens)\n                return out.logits\n        finally:\n            hook.remove()\n\n    def get_module_for_replacement_hook(self, layer_idx) -&gt; torch.nn.Module:\n        \"\"\"Get the MLP activation module for a specific layer.\n\n        Retrieves the MLP activation function module at the specified layer,\n        which can be used to register forward hooks for activation replacement.\n\n        Args:\n            layer_idx (int): Index of the transformer layer (0-indexed).\n\n        Returns:\n            torch.nn.Module: The MLP activation module at the specified layer.\n        \"\"\"\n        if isinstance(self.model, (\n            transformers.GPT2LMHeadModel, \n            transformers.FalconForCausalLM\n        )):\n            module = self.model.transformer.h[layer_idx].mlp.act\n        elif isinstance(self.model, (\n            transformers.LlamaForCausalLM, \n            transformers.MistralForCausalLM, \n            transformers.Gemma3ForCausalLM, \n            transformers.GemmaForCausalLM, \n            transformers.Qwen2ForCausalLM,\n            transformers.Qwen3ForCausalLM\n        )):\n            module = self.model.model.layers[layer_idx].mlp.act_fn\n        elif isinstance(self.model, (\n            transformers.PhiForCausalLM, \n            transformers.Phi3ForCausalLM\n        )):\n            module = self.model.model.layers[layer_idx].mlp.activation_fn\n        else:\n            raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n        return module\n</code></pre>"},{"location":"api/core/#deeplens.intervene.ReinjectSingleSample.__init__","title":"<code>__init__(hf_model, device='auto', cache_dir='cache')</code>","text":"<p>Initialize the ReinjectSingleSample class for causal inference with modified activations.</p> <p>Loads a HuggingFace causal language model and tokenizer to enable reinjection of modified activations into the model's forward pass for text generation and analysis.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace model to load. Should be a valid model identifier from the HuggingFace model hub (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").</p> required <code>device</code> <code>str</code> <p>Device to run computations on. Can be \"auto\" for automatic selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".</p> <code>'auto'</code> <code>cache_dir</code> <code>str</code> <p>Directory to cache downloaded models. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>deeplens/intervene.py</code> <pre><code>def __init__(\n        self, \n        hf_model: str, \n        device: str = \"auto\", \n        cache_dir: str = 'cache'\n    ):\n    \"\"\"Initialize the ReinjectSingleSample class for causal inference with modified activations.\n\n    Loads a HuggingFace causal language model and tokenizer to enable reinjection of\n    modified activations into the model's forward pass for text generation and analysis.\n\n    Args:\n        hf_model (str): Name or path of the HuggingFace model to load.\n            Should be a valid model identifier from the HuggingFace model hub\n            (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n        device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n            selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n        cache_dir (str, optional): Directory to cache downloaded models.\n            Defaults to 'cache'.\n    \"\"\"\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    os.makedirs(cache_dir, exist_ok=True)\n    self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir).to(self.device)\n    self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.model.eval()\n</code></pre>"},{"location":"api/core/#deeplens.intervene.ReinjectSingleSample.get_module_for_replacement_hook","title":"<code>get_module_for_replacement_hook(layer_idx)</code>","text":"<p>Get the MLP activation module for a specific layer.</p> <p>Retrieves the MLP activation function module at the specified layer, which can be used to register forward hooks for activation replacement.</p> <p>Parameters:</p> Name Type Description Default <code>layer_idx</code> <code>int</code> <p>Index of the transformer layer (0-indexed).</p> required <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The MLP activation module at the specified layer.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def get_module_for_replacement_hook(self, layer_idx) -&gt; torch.nn.Module:\n    \"\"\"Get the MLP activation module for a specific layer.\n\n    Retrieves the MLP activation function module at the specified layer,\n    which can be used to register forward hooks for activation replacement.\n\n    Args:\n        layer_idx (int): Index of the transformer layer (0-indexed).\n\n    Returns:\n        torch.nn.Module: The MLP activation module at the specified layer.\n    \"\"\"\n    if isinstance(self.model, (\n        transformers.GPT2LMHeadModel, \n        transformers.FalconForCausalLM\n    )):\n        module = self.model.transformer.h[layer_idx].mlp.act\n    elif isinstance(self.model, (\n        transformers.LlamaForCausalLM, \n        transformers.MistralForCausalLM, \n        transformers.Gemma3ForCausalLM, \n        transformers.GemmaForCausalLM, \n        transformers.Qwen2ForCausalLM,\n        transformers.Qwen3ForCausalLM\n    )):\n        module = self.model.model.layers[layer_idx].mlp.act_fn\n    elif isinstance(self.model, (\n        transformers.PhiForCausalLM, \n        transformers.Phi3ForCausalLM\n    )):\n        module = self.model.model.layers[layer_idx].mlp.activation_fn\n    else:\n        raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n    return module\n</code></pre>"},{"location":"api/core/#deeplens.intervene.ReinjectSingleSample.reinject_and_generate","title":"<code>reinject_and_generate(text, modified_activations, layer=3, generate=False, max_new_tokens=25, temperature=1.0)</code>","text":"<p>Reinject modified activations into a model layer and optionally generate text.</p> <p>Replaces the activations at the specified layer with the provided modified activations during the forward pass. Can either return logits for the input text or generate new tokens autoregressively.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to tokenize and process through the model.</p> required <code>modified_activations</code> <code>Tensor</code> <p>The modified activations to inject at the specified layer. Should have the appropriate shape for the layer's output.</p> required <code>layer</code> <code>int</code> <p>Index of the transformer layer where activations should be replaced. Defaults to 3.</p> <code>3</code> <code>generate</code> <code>bool</code> <p>If True, generates new tokens autoregressively. If False, only returns logits for the input. Defaults to False.</p> <code>False</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of new tokens to generate when generate=True. Defaults to 25.</p> <code>25</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for generation. Higher values (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic. Set to 0 for greedy decoding. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor | str</code> <p>torch.Tensor | str: If generate=False, returns the model's logits as a tensor. If generate=True, returns the generated text as a string.</p> Note <p>The hook is automatically removed after execution to prevent interference with subsequent forward passes. For generation mode, the hook only affects the first forward pass to avoid applying the intervention to newly generated tokens.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef reinject_and_generate(\n        self, \n        text, \n        modified_activations, \n        layer: int = 3, \n        generate: bool = False, \n        max_new_tokens: int = 25, \n        temperature: float = 1.0\n    ) -&gt; torch.Tensor | str:\n    \"\"\"Reinject modified activations into a model layer and optionally generate text.\n\n    Replaces the activations at the specified layer with the provided modified activations\n    during the forward pass. Can either return logits for the input text or generate\n    new tokens autoregressively.\n\n    Args:\n        text (str): Input text to tokenize and process through the model.\n        modified_activations (torch.Tensor): The modified activations to inject at the\n            specified layer. Should have the appropriate shape for the layer's output.\n        layer (int, optional): Index of the transformer layer where activations should\n            be replaced. Defaults to 3.\n        generate (bool, optional): If True, generates new tokens autoregressively.\n            If False, only returns logits for the input. Defaults to False.\n        max_new_tokens (int, optional): Maximum number of new tokens to generate when\n            generate=True. Defaults to 25.\n        temperature (float, optional): Sampling temperature for generation. Higher values\n            (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic.\n            Set to 0 for greedy decoding. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor | str: If generate=False, returns the model's logits as a tensor.\n            If generate=True, returns the generated text as a string.\n\n    Note:\n        The hook is automatically removed after execution to prevent interference\n        with subsequent forward passes. For generation mode, the hook only affects\n        the first forward pass to avoid applying the intervention to newly generated tokens.\n    \"\"\"\n    modified_activations = modified_activations.to(self.device)\n    call_count = [0]\n    def replacement_hook(module, input, output):\n        if generate and call_count[0] &gt; 0:\n            return output\n        call_count[0] += 1\n        return modified_activations\n\n    mlp_module = self.get_module_for_replacement_hook(layer_idx=layer)\n    hook = mlp_module.register_forward_hook(replacement_hook)\n    tokens = self.tokenizer(text, return_tensors='pt').to(self.device)\n    try:\n        if generate:\n            generated_ids = self.model.generate(\n                **tokens,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=temperature &gt; 0\n            )\n            return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        else:\n            out = self.model(**tokens)\n            return out.logits\n    finally:\n        hook.remove()\n</code></pre>"},{"location":"api/core/#deeplens.sae","title":"<code>deeplens.sae</code>","text":""},{"location":"api/core/#deeplens.sae.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse Autoencoder for learning interpretable features from neural network activations.</p> <p>This implementation supports multiple sparsity methods (L1 regularization or top-k activation), optional weight tying between encoder and decoder, and unit-norm decoder constraints for improved feature interpretability. Designed for mechanistic interpretability research on transformer models.</p> <p>The architecture consists of: - Optional input normalization layer - Encoder: Linear projection to expanded feature space with nonlinear activation - Decoder: Linear projection back to original space (optionally tied to encoder) - Sparsity constraint: Either L1 penalty or top-k activation selection</p> Source code in <code>deeplens/sae.py</code> <pre><code>class SparseAutoencoder(nn.Module):\n    \"\"\"Sparse Autoencoder for learning interpretable features from neural network activations.\n\n    This implementation supports multiple sparsity methods (L1 regularization or top-k activation),\n    optional weight tying between encoder and decoder, and unit-norm decoder constraints for\n    improved feature interpretability. Designed for mechanistic interpretability research\n    on transformer models.\n\n    The architecture consists of:\n    - Optional input normalization layer\n    - Encoder: Linear projection to expanded feature space with nonlinear activation\n    - Decoder: Linear projection back to original space (optionally tied to encoder)\n    - Sparsity constraint: Either L1 penalty or top-k activation selection\n    \"\"\"\n    def __init__(\n            self, \n            input_dims: int = 512, \n            n_features: int = 2048, \n            activation: str = \"relu\",\n            input_norm: bool = True,\n            k: int | None = None,\n            beta_l1: float | None = None,\n            tie_weights: bool = False,\n            unit_norm_decoder: bool = True\n        ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder with specified architecture and sparsity settings.\n\n        Args:\n            input_dims (int, optional): Dimensionality of input activations (e.g., 3072 for\n                GPT-2 layer 3 MLP output). Defaults to 512.\n            n_features (int, optional): Number of learned features in the latent space.\n                Typically set as expansion_factor \u00d7 input_dims where expansion_factor is 2-8.\n                Defaults to 2048.\n            activation (str, optional): Nonlinearity applied after encoder. Must be 'relu'\n                or 'silu'. ReLU is standard for interpretability; SiLU may improve reconstruction.\n                Defaults to \"relu\".\n            input_norm (bool, optional): Whether to apply LayerNorm to inputs before encoding.\n                Helps stabilize training with varying activation magnitudes. Defaults to True.\n            k (int | None, optional): If set, uses top-k sparsity (keeps only k largest activations\n                per sample) instead of L1 regularization. Mutually exclusive with beta_l1.\n                Defaults to None.\n            beta_l1 (float | None, optional): L1 regularization coefficient for sparsity.\n                Higher values encourage sparser activations. Ignored if k is set. Defaults to None.\n            tie_weights (bool, optional): If True, decoder weights are the transpose of encoder\n                weights (no separate decoder parameters). Reduces parameters but may hurt performance.\n                Defaults to False.\n            unit_norm_decoder (bool, optional): If True, constrains decoder weight columns to\n                unit norm. Improves feature interpretability by removing scale ambiguity.\n                Defaults to True.\n        \"\"\"\n        super().__init__()\n        self.norm = nn.LayerNorm(input_dims) if input_norm else nn.Identity()\n        self.encoder = nn.Linear(input_dims, n_features, bias=True)\n        self.decoder = None if tie_weights else nn.Linear(n_features, input_dims, bias=False)\n        self.unit_norm_decoder = unit_norm_decoder\n        self.input_norm = input_norm\n\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n            kaiming_activation = \"relu\"\n        elif activation == \"silu\":\n            self.activation = nn.SiLU()\n            kaiming_activation = \"linear\"\n        else:\n            raise ValueError(\"Activation must be 'relu' or 'silu'\")\n\n        nn.init.kaiming_normal_(self.encoder.weight, nonlinearity=kaiming_activation)\n        if self.decoder is not None:\n            nn.init.xavier_uniform_(self.decoder.weight)\n            if self.unit_norm_decoder:\n                self._renorm_decoder()\n\n        self.k = k\n        self.beta_l1 = beta_l1\n        self.tie_weights = tie_weights\n\n    @torch.no_grad()\n    def _renorm_decoder(self, eps: float = 1e-8) -&gt; None:\n        \"\"\"Normalize decoder weight columns to unit norm for improved interpretability.\n\n        Constrains each feature's decoder weight vector to have L2 norm of 1, removing\n        scale ambiguity from the learned features. This is a common technique in dictionary\n        learning and sparse coding to ensure features represent directions rather than\n        directions with varying magnitudes.\n\n        Args:\n            eps (float, optional): Small epsilon value to prevent division by zero for\n                near-zero norm weights. Defaults to 1e-8.\n\n        Returns:\n            None: Modifies decoder weights in-place. No-op if decoder is None or\n                unit_norm_decoder is False.\n        \"\"\"\n        if self.decoder is not None and self.unit_norm_decoder:\n            W = self.decoder.weight.data\n            norms = W.norm(dim=1, keepdim=True).clamp_min(eps)\n            self.decoder.weight.data = W / norms\n\n    def encode(self, x) -&gt; torch.Tensor:\n        \"\"\"Encode input activations into the sparse latent feature space.\n\n        Applies optional normalization, linear transformation to expanded feature space,\n        and nonlinear activation function.\n\n        Args:\n            x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n                Typically (batch_size, seq_length, input_dims) or (batch_size, input_dims).\n\n        Returns:\n            torch.Tensor: Encoded features with shape (..., n_features) after applying\n                normalization (if enabled), linear encoding, and activation function.\n                These are the pre-sparsity latent activations.\n        \"\"\"\n        x = self.norm(x)\n        return self.activation(self.encoder(x))\n\n    def decode(self, z) -&gt; torch.Tensor:\n        \"\"\"Decode sparse latent features back to the original activation space.\n\n        Applies linear transformation from feature space back to input space. Uses either\n        a separate decoder or transposed encoder weights depending on tie_weights setting.\n\n        Args:\n            z (torch.Tensor): Latent feature activations with shape (..., n_features).\n                Typically the output of encode() or after applying sparsity constraints.\n\n        Returns:\n            torch.Tensor: Reconstructed activations with shape (..., input_dims).\n                Should approximate the original input when z contains sufficient information.\n        \"\"\"\n        if self.tie_weights:\n            return F.linear(z, self.encoder.weight.t(), bias=None)\n        else:\n            return self.decoder(z)\n\n    def post_step(self) -&gt; None:\n        \"\"\"Renormalize decoder weights after each optimization step.\n\n        Should be called after each parameter update (e.g., after optimizer.step()) to\n        maintain the unit norm constraint on decoder weights. This ensures the constraint\n        is enforced throughout training.\n\n        Returns:\n            None: Modifies decoder weights in-place if unit_norm_decoder is True.\n\n        Note:\n            This is a no-op if unit_norm_decoder is False or tie_weights is True.\n        \"\"\"\n        self._renorm_decoder()\n\n    def forward(self, x) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a complete forward pass through the sparse autoencoder.\n\n        Encodes input, applies sparsity constraint (top-k or none), and decodes to\n        reconstruct the input. Returns both the reconstruction and intermediate activations.\n\n        Args:\n            x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n                - x_hat: Reconstructed activations with shape (..., input_dims)\n                - z: Sparse latent activations after sparsity constraint, shape (..., n_features)\n                - z_pre: Dense latent activations before sparsity constraint, shape (..., n_features)\n\n        Note:\n            If k is None, z and z_pre are identical. If k is set, z contains only the\n            top-k activations (others are zeroed).\n        \"\"\"\n        z_pre = self.encode(x)\n        z = self.topk_mask(z_pre, self.k) if self.k is not None else z_pre\n        x_hat = self.decode(z)\n        return x_hat, z, z_pre\n\n    def loss(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n        \"\"\"Compute the training loss with reconstruction and optional sparsity penalty.\n\n        Calculates MSE reconstruction loss and, if using L1 sparsity (k is None), adds\n        L1 penalty on latent activations. Also computes diagnostic metrics for logging.\n\n        Args:\n            x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n\n        Returns:\n            tuple[torch.Tensor, dict]: A tuple containing:\n                - total_loss: Scalar loss tensor for backpropagation. If k is None,\n                    equals mse + beta_l1 * l1_sparsity. If k is set, equals mse only.\n                - logs: Dictionary of detached metrics for logging:\n                    - 'mse': Reconstruction loss (MSE between x_hat and x)\n                    - 'l1': L1 norm of activations (only if k is None)\n                    - 'non_zero_frac': Fraction of non-zero latent activations\n\n        Note:\n            The logs dictionary is intended for monitoring training progress and all\n            values are detached from the computation graph.\n        \"\"\"\n        x_hat, z, _ = self.forward(x)\n        recon = F.mse_loss(x_hat, x)\n        if self.k is None:\n            sparsity = z.abs().mean()\n            total = recon + self.beta_l1 * sparsity\n            logs = {\n                \"mse\": recon.detach(),\n                \"l1\": sparsity.detach(),\n                \"non_zero_frac\": (z != 0).float().mean().detach()\n            }\n        else:\n            total = recon\n            logs = {\n                \"mse\": recon.detach(),\n                \"non_zero_frac\": (z != 0).float().mean().detach()\n            }\n        return total, logs\n\n    def topk_mask(self, z: torch.Tensor, k: int) -&gt; torch.Tensor:\n        \"\"\"Apply top-k sparsity constraint by keeping only the k largest activations.\n\n        Zeros out all but the k largest magnitude activations in the latent space,\n        enforcing a fixed sparsity level. This is an alternative to L1 regularization\n        that provides more predictable and controllable sparsity.\n\n        Args:\n            z (torch.Tensor): Dense latent activations with shape (..., n_features).\n            k (int): Number of top activations to keep per sample. If None, \u22640, or\n                \u2265n_features, returns z unchanged.\n\n        Returns:\n            torch.Tensor: Sparse latent activations with same shape as z, but with all\n                except the k largest (by absolute value) activations set to zero. The\n                values of the top-k activations are preserved from the input.\n\n        Note:\n            Selection is based on absolute value, but original signed values are preserved\n            for the top-k features.\n        \"\"\"\n        if k is None or k &lt;= 0 or k &gt;= z.size(-1):\n            return z\n        vals, idx = torch.topk(z.abs(), k, dim=-1)\n        out = torch.zeros_like(z)\n        return out.scatter(-1, idx, z.gather(-1, idx))\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.__init__","title":"<code>__init__(input_dims=512, n_features=2048, activation='relu', input_norm=True, k=None, beta_l1=None, tie_weights=False, unit_norm_decoder=True)</code>","text":"<p>Initialize the Sparse Autoencoder with specified architecture and sparsity settings.</p> <p>Parameters:</p> Name Type Description Default <code>input_dims</code> <code>int</code> <p>Dimensionality of input activations (e.g., 3072 for GPT-2 layer 3 MLP output). Defaults to 512.</p> <code>512</code> <code>n_features</code> <code>int</code> <p>Number of learned features in the latent space. Typically set as expansion_factor \u00d7 input_dims where expansion_factor is 2-8. Defaults to 2048.</p> <code>2048</code> <code>activation</code> <code>str</code> <p>Nonlinearity applied after encoder. Must be 'relu' or 'silu'. ReLU is standard for interpretability; SiLU may improve reconstruction. Defaults to \"relu\".</p> <code>'relu'</code> <code>input_norm</code> <code>bool</code> <p>Whether to apply LayerNorm to inputs before encoding. Helps stabilize training with varying activation magnitudes. Defaults to True.</p> <code>True</code> <code>k</code> <code>int | None</code> <p>If set, uses top-k sparsity (keeps only k largest activations per sample) instead of L1 regularization. Mutually exclusive with beta_l1. Defaults to None.</p> <code>None</code> <code>beta_l1</code> <code>float | None</code> <p>L1 regularization coefficient for sparsity. Higher values encourage sparser activations. Ignored if k is set. Defaults to None.</p> <code>None</code> <code>tie_weights</code> <code>bool</code> <p>If True, decoder weights are the transpose of encoder weights (no separate decoder parameters). Reduces parameters but may hurt performance. Defaults to False.</p> <code>False</code> <code>unit_norm_decoder</code> <code>bool</code> <p>If True, constrains decoder weight columns to unit norm. Improves feature interpretability by removing scale ambiguity. Defaults to True.</p> <code>True</code> Source code in <code>deeplens/sae.py</code> <pre><code>def __init__(\n        self, \n        input_dims: int = 512, \n        n_features: int = 2048, \n        activation: str = \"relu\",\n        input_norm: bool = True,\n        k: int | None = None,\n        beta_l1: float | None = None,\n        tie_weights: bool = False,\n        unit_norm_decoder: bool = True\n    ) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder with specified architecture and sparsity settings.\n\n    Args:\n        input_dims (int, optional): Dimensionality of input activations (e.g., 3072 for\n            GPT-2 layer 3 MLP output). Defaults to 512.\n        n_features (int, optional): Number of learned features in the latent space.\n            Typically set as expansion_factor \u00d7 input_dims where expansion_factor is 2-8.\n            Defaults to 2048.\n        activation (str, optional): Nonlinearity applied after encoder. Must be 'relu'\n            or 'silu'. ReLU is standard for interpretability; SiLU may improve reconstruction.\n            Defaults to \"relu\".\n        input_norm (bool, optional): Whether to apply LayerNorm to inputs before encoding.\n            Helps stabilize training with varying activation magnitudes. Defaults to True.\n        k (int | None, optional): If set, uses top-k sparsity (keeps only k largest activations\n            per sample) instead of L1 regularization. Mutually exclusive with beta_l1.\n            Defaults to None.\n        beta_l1 (float | None, optional): L1 regularization coefficient for sparsity.\n            Higher values encourage sparser activations. Ignored if k is set. Defaults to None.\n        tie_weights (bool, optional): If True, decoder weights are the transpose of encoder\n            weights (no separate decoder parameters). Reduces parameters but may hurt performance.\n            Defaults to False.\n        unit_norm_decoder (bool, optional): If True, constrains decoder weight columns to\n            unit norm. Improves feature interpretability by removing scale ambiguity.\n            Defaults to True.\n    \"\"\"\n    super().__init__()\n    self.norm = nn.LayerNorm(input_dims) if input_norm else nn.Identity()\n    self.encoder = nn.Linear(input_dims, n_features, bias=True)\n    self.decoder = None if tie_weights else nn.Linear(n_features, input_dims, bias=False)\n    self.unit_norm_decoder = unit_norm_decoder\n    self.input_norm = input_norm\n\n    if activation == \"relu\":\n        self.activation = nn.ReLU()\n        kaiming_activation = \"relu\"\n    elif activation == \"silu\":\n        self.activation = nn.SiLU()\n        kaiming_activation = \"linear\"\n    else:\n        raise ValueError(\"Activation must be 'relu' or 'silu'\")\n\n    nn.init.kaiming_normal_(self.encoder.weight, nonlinearity=kaiming_activation)\n    if self.decoder is not None:\n        nn.init.xavier_uniform_(self.decoder.weight)\n        if self.unit_norm_decoder:\n            self._renorm_decoder()\n\n    self.k = k\n    self.beta_l1 = beta_l1\n    self.tie_weights = tie_weights\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.decode","title":"<code>decode(z)</code>","text":"<p>Decode sparse latent features back to the original activation space.</p> <p>Applies linear transformation from feature space back to input space. Uses either a separate decoder or transposed encoder weights depending on tie_weights setting.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Latent feature activations with shape (..., n_features). Typically the output of encode() or after applying sparsity constraints.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Reconstructed activations with shape (..., input_dims). Should approximate the original input when z contains sufficient information.</p> Source code in <code>deeplens/sae.py</code> <pre><code>def decode(self, z) -&gt; torch.Tensor:\n    \"\"\"Decode sparse latent features back to the original activation space.\n\n    Applies linear transformation from feature space back to input space. Uses either\n    a separate decoder or transposed encoder weights depending on tie_weights setting.\n\n    Args:\n        z (torch.Tensor): Latent feature activations with shape (..., n_features).\n            Typically the output of encode() or after applying sparsity constraints.\n\n    Returns:\n        torch.Tensor: Reconstructed activations with shape (..., input_dims).\n            Should approximate the original input when z contains sufficient information.\n    \"\"\"\n    if self.tie_weights:\n        return F.linear(z, self.encoder.weight.t(), bias=None)\n    else:\n        return self.decoder(z)\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.encode","title":"<code>encode(x)</code>","text":"<p>Encode input activations into the sparse latent feature space.</p> <p>Applies optional normalization, linear transformation to expanded feature space, and nonlinear activation function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activation tensor with shape (..., input_dims). Typically (batch_size, seq_length, input_dims) or (batch_size, input_dims).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Encoded features with shape (..., n_features) after applying normalization (if enabled), linear encoding, and activation function. These are the pre-sparsity latent activations.</p> Source code in <code>deeplens/sae.py</code> <pre><code>def encode(self, x) -&gt; torch.Tensor:\n    \"\"\"Encode input activations into the sparse latent feature space.\n\n    Applies optional normalization, linear transformation to expanded feature space,\n    and nonlinear activation function.\n\n    Args:\n        x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n            Typically (batch_size, seq_length, input_dims) or (batch_size, input_dims).\n\n    Returns:\n        torch.Tensor: Encoded features with shape (..., n_features) after applying\n            normalization (if enabled), linear encoding, and activation function.\n            These are the pre-sparsity latent activations.\n    \"\"\"\n    x = self.norm(x)\n    return self.activation(self.encoder(x))\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Perform a complete forward pass through the sparse autoencoder.</p> <p>Encodes input, applies sparsity constraint (top-k or none), and decodes to reconstruct the input. Returns both the reconstruction and intermediate activations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activation tensor with shape (..., input_dims).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing: - x_hat: Reconstructed activations with shape (..., input_dims) - z: Sparse latent activations after sparsity constraint, shape (..., n_features) - z_pre: Dense latent activations before sparsity constraint, shape (..., n_features)</p> Note <p>If k is None, z and z_pre are identical. If k is set, z contains only the top-k activations (others are zeroed).</p> Source code in <code>deeplens/sae.py</code> <pre><code>def forward(self, x) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Perform a complete forward pass through the sparse autoencoder.\n\n    Encodes input, applies sparsity constraint (top-k or none), and decodes to\n    reconstruct the input. Returns both the reconstruction and intermediate activations.\n\n    Args:\n        x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n            - x_hat: Reconstructed activations with shape (..., input_dims)\n            - z: Sparse latent activations after sparsity constraint, shape (..., n_features)\n            - z_pre: Dense latent activations before sparsity constraint, shape (..., n_features)\n\n    Note:\n        If k is None, z and z_pre are identical. If k is set, z contains only the\n        top-k activations (others are zeroed).\n    \"\"\"\n    z_pre = self.encode(x)\n    z = self.topk_mask(z_pre, self.k) if self.k is not None else z_pre\n    x_hat = self.decode(z)\n    return x_hat, z, z_pre\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.loss","title":"<code>loss(x)</code>","text":"<p>Compute the training loss with reconstruction and optional sparsity penalty.</p> <p>Calculates MSE reconstruction loss and, if using L1 sparsity (k is None), adds L1 penalty on latent activations. Also computes diagnostic metrics for logging.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input activation tensor with shape (..., input_dims).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict]</code> <p>tuple[torch.Tensor, dict]: A tuple containing: - total_loss: Scalar loss tensor for backpropagation. If k is None,     equals mse + beta_l1 * l1_sparsity. If k is set, equals mse only. - logs: Dictionary of detached metrics for logging:     - 'mse': Reconstruction loss (MSE between x_hat and x)     - 'l1': L1 norm of activations (only if k is None)     - 'non_zero_frac': Fraction of non-zero latent activations</p> Note <p>The logs dictionary is intended for monitoring training progress and all values are detached from the computation graph.</p> Source code in <code>deeplens/sae.py</code> <pre><code>def loss(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    \"\"\"Compute the training loss with reconstruction and optional sparsity penalty.\n\n    Calculates MSE reconstruction loss and, if using L1 sparsity (k is None), adds\n    L1 penalty on latent activations. Also computes diagnostic metrics for logging.\n\n    Args:\n        x (torch.Tensor): Input activation tensor with shape (..., input_dims).\n\n    Returns:\n        tuple[torch.Tensor, dict]: A tuple containing:\n            - total_loss: Scalar loss tensor for backpropagation. If k is None,\n                equals mse + beta_l1 * l1_sparsity. If k is set, equals mse only.\n            - logs: Dictionary of detached metrics for logging:\n                - 'mse': Reconstruction loss (MSE between x_hat and x)\n                - 'l1': L1 norm of activations (only if k is None)\n                - 'non_zero_frac': Fraction of non-zero latent activations\n\n    Note:\n        The logs dictionary is intended for monitoring training progress and all\n        values are detached from the computation graph.\n    \"\"\"\n    x_hat, z, _ = self.forward(x)\n    recon = F.mse_loss(x_hat, x)\n    if self.k is None:\n        sparsity = z.abs().mean()\n        total = recon + self.beta_l1 * sparsity\n        logs = {\n            \"mse\": recon.detach(),\n            \"l1\": sparsity.detach(),\n            \"non_zero_frac\": (z != 0).float().mean().detach()\n        }\n    else:\n        total = recon\n        logs = {\n            \"mse\": recon.detach(),\n            \"non_zero_frac\": (z != 0).float().mean().detach()\n        }\n    return total, logs\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.post_step","title":"<code>post_step()</code>","text":"<p>Renormalize decoder weights after each optimization step.</p> <p>Should be called after each parameter update (e.g., after optimizer.step()) to maintain the unit norm constraint on decoder weights. This ensures the constraint is enforced throughout training.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Modifies decoder weights in-place if unit_norm_decoder is True.</p> Note <p>This is a no-op if unit_norm_decoder is False or tie_weights is True.</p> Source code in <code>deeplens/sae.py</code> <pre><code>def post_step(self) -&gt; None:\n    \"\"\"Renormalize decoder weights after each optimization step.\n\n    Should be called after each parameter update (e.g., after optimizer.step()) to\n    maintain the unit norm constraint on decoder weights. This ensures the constraint\n    is enforced throughout training.\n\n    Returns:\n        None: Modifies decoder weights in-place if unit_norm_decoder is True.\n\n    Note:\n        This is a no-op if unit_norm_decoder is False or tie_weights is True.\n    \"\"\"\n    self._renorm_decoder()\n</code></pre>"},{"location":"api/core/#deeplens.sae.SparseAutoencoder.topk_mask","title":"<code>topk_mask(z, k)</code>","text":"<p>Apply top-k sparsity constraint by keeping only the k largest activations.</p> <p>Zeros out all but the k largest magnitude activations in the latent space, enforcing a fixed sparsity level. This is an alternative to L1 regularization that provides more predictable and controllable sparsity.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Dense latent activations with shape (..., n_features).</p> required <code>k</code> <code>int</code> <p>Number of top activations to keep per sample. If None, \u22640, or \u2265n_features, returns z unchanged.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Sparse latent activations with same shape as z, but with all except the k largest (by absolute value) activations set to zero. The values of the top-k activations are preserved from the input.</p> Note <p>Selection is based on absolute value, but original signed values are preserved for the top-k features.</p> Source code in <code>deeplens/sae.py</code> <pre><code>def topk_mask(self, z: torch.Tensor, k: int) -&gt; torch.Tensor:\n    \"\"\"Apply top-k sparsity constraint by keeping only the k largest activations.\n\n    Zeros out all but the k largest magnitude activations in the latent space,\n    enforcing a fixed sparsity level. This is an alternative to L1 regularization\n    that provides more predictable and controllable sparsity.\n\n    Args:\n        z (torch.Tensor): Dense latent activations with shape (..., n_features).\n        k (int): Number of top activations to keep per sample. If None, \u22640, or\n            \u2265n_features, returns z unchanged.\n\n    Returns:\n        torch.Tensor: Sparse latent activations with same shape as z, but with all\n            except the k largest (by absolute value) activations set to zero. The\n            values of the top-k activations are preserved from the input.\n\n    Note:\n        Selection is based on absolute value, but original signed values are preserved\n        for the top-k features.\n    \"\"\"\n    if k is None or k &lt;= 0 or k &gt;= z.size(-1):\n        return z\n    vals, idx = torch.topk(z.abs(), k, dim=-1)\n    out = torch.zeros_like(z)\n    return out.scatter(-1, idx, z.gather(-1, idx))\n</code></pre>"},{"location":"api/core/#deeplens.train","title":"<code>deeplens.train</code>","text":""},{"location":"api/core/#deeplens.train.SAETrainer","title":"<code>SAETrainer</code>","text":"<p>Training framework for Sparse Autoencoders with comprehensive logging and checkpointing.</p> <p>Handles the complete training loop including optimization, learning rate scheduling, gradient clipping, mixed precision training (bfloat16), periodic evaluation, model checkpointing, and Weights &amp; Biases logging. Designed specifically for training sparse autoencoders on neural network activation data.</p> Source code in <code>deeplens/train.py</code> <pre><code>class SAETrainer():\n    \"\"\"Training framework for Sparse Autoencoders with comprehensive logging and checkpointing.\n\n    Handles the complete training loop including optimization, learning rate scheduling,\n    gradient clipping, mixed precision training (bfloat16), periodic evaluation, model\n    checkpointing, and Weights &amp; Biases logging. Designed specifically for training\n    sparse autoencoders on neural network activation data.\n    \"\"\"\n    def __init__(\n            self, \n            train_dataloader: DataLoader = None, \n            eval_dataloader: DataLoader = None, \n            model: torch.nn.Module = None, \n            model_name: str = \"sae\",\n            optim: torch.optim.Optimizer = torch.optim.Adam,\n            epochs: int = 20, \n            bf16: bool = False,\n            random_seed: int = 42,\n            save_checkpoints: bool = True,\n            device: str = \"auto\",\n            grad_clip_norm: float = None,\n            lrs_type: str = None,\n            eval_steps: int = 5000,\n            warmup_fraction: float = 0.1,\n            save_best_only: bool = True,\n            log_to_wandb: bool = True\n        ) -&gt; None:\n        \"\"\"Initialize the SAETrainer with model, data, and training configuration.\n\n        Sets up the training environment including device placement, learning rate scheduling,\n        and Weights &amp; Biases logging if enabled.\n\n        Args:\n            train_dataloader (DataLoader, optional): DataLoader for training data containing\n                batches of activation tensors. Defaults to None.\n            eval_dataloader (DataLoader, optional): DataLoader for evaluation/validation data.\n                Used for periodic evaluation during training. Defaults to None.\n            model (torch.nn.Module, optional): Sparse autoencoder model to train. Should be\n                an instance of SparseAutoencoder or compatible architecture. Defaults to None.\n            model_name (str, optional): Name used for organizing saved checkpoints and W&amp;B\n                project naming. Creates directory structure at saved_models/{model_name}/.\n                Defaults to \"sae\".\n            optim (torch.optim.Optimizer, optional): Initialized optimizer instance with\n                model parameters already attached. Defaults to torch.optim.Adam.\n            epochs (int, optional): Number of complete passes through the training dataset.\n                Defaults to 20.\n            bf16 (bool, optional): Whether to use bfloat16 mixed precision training for\n                faster computation and reduced memory usage. Requires CUDA support.\n                Defaults to False.\n            random_seed (int, optional): Random seed for reproducibility across numpy,\n                PyTorch, and CUDNN. Defaults to 42.\n            save_checkpoints (bool, optional): Whether to save model checkpoints during\n                training. Checkpoints saved when evaluation loss improves. Defaults to True.\n            device (str, optional): Device for training. Can be \"auto\" for automatic selection\n                (prefers cuda &gt; mps &gt; cpu), \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n            grad_clip_norm (float, optional): Maximum gradient norm for gradient clipping.\n                Helps prevent gradient explosion. If None, no clipping is applied.\n                Defaults to None.\n            lrs_type (str, optional): Learning rate scheduler type. Options: 'cosine',\n                'plateau', 'linear'. If None, uses constant learning rate. Defaults to None.\n            eval_steps (int, optional): Evaluate model every N training steps and save\n                checkpoint if loss improves. Defaults to 5000.\n            warmup_fraction (float, optional): Fraction of total training steps to use for\n                learning rate warmup. Applies to 'cosine' and 'linear' schedulers.\n                Defaults to 0.1 (10% warmup).\n            save_best_only (bool, optional): If True, only saves the best checkpoint\n                (overwrites previous best). If False, saves all improving checkpoints.\n                Defaults to True.\n            log_to_wandb (bool, optional): Whether to log metrics to Weights &amp; Biases.\n                Logs training/eval loss, non-zero fraction, learning rate, and config.\n                Defaults to True.\n\n        Note:\n            The optimizer must be initialized with the model's parameters before passing\n            to the trainer. Learning rate and other optimizer settings should be configured\n            in the optimizer instance.\n        \"\"\"\n        self.model = model\n        self.model_name = model_name\n        self.optim = optim\n        self.epochs = epochs\n        self.train_dataloader = train_dataloader\n        self.eval_dataloader = eval_dataloader\n        self.bf16 = bf16\n        self.random_seed = random_seed\n        self.save_checkpoints = save_checkpoints\n        self.grad_clip_norm = grad_clip_norm\n        self.eval_steps = eval_steps\n        self.warmup_fraction = warmup_fraction\n        self.save_best_only = save_best_only\n        self.log_wandb = log_to_wandb\n\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        if lrs_type is not None:\n            self.scheduler = self.set_lr_scheduler(lrs_type)\n        else:\n            self.scheduler = None\n\n        if log_to_wandb:\n            time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            wandb.init(\n                project=f\"sparse-autoencoder\",\n                name=f\"run-{self.model_name}-{time}\",\n                config={\n                    \"epochs\": epochs,\n                    \"in_dims\": self.model.encoder.in_features,\n                    \"sparse_dims\": self.model.encoder.out_features,\n                    \"activations\": self.model.activation,\n                    \"input_norm\": self.model.input_norm,\n                    \"top_k\": self.model.k, \n                    \"l1\": self.model.beta_l1,\n                    \"tie_weights\": self.model.tie_weights,\n                    \"unit_norm_decoder\": self.model.unit_norm_decoder,\n                    \"lr\": optim.param_groups[0]['lr'],\n                    \"lr_scheduler\": lrs_type,\n                    \"warmup_fraction\": warmup_fraction,\n                    \"seed\": random_seed,\n                    \"grad_clip_norm\": grad_clip_norm,\n                    \"bf16\": bf16\n                }\n            )\n\n    def train_one_epoch(\n            self, \n            model: torch.nn.Module, \n            train_dataloader: torch.utils.data.DataLoader, \n            optim: torch.optim.Optimizer, \n            bf16: bool = True,\n            global_step: int = 0,\n            timestamp: str = None,\n            best_loss: float = float('inf')\n        ) -&gt; tuple[int, float]:\n        \"\"\"Execute one complete training epoch with periodic evaluation and checkpointing.\n\n        Iterates through all training batches, performs forward/backward passes with optional\n        mixed precision, applies gradient clipping, updates learning rate, and periodically\n        evaluates and saves checkpoints.\n\n        Args:\n            model (torch.nn.Module): The sparse autoencoder model to train.\n            train_dataloader (torch.utils.data.DataLoader): DataLoader providing training batches.\n            optim (torch.optim.Optimizer): Optimizer for updating model parameters.\n            bf16 (bool, optional): Whether to use bfloat16 mixed precision. Defaults to True.\n            global_step (int, optional): Current global training step count across all epochs.\n                Used for logging and checkpoint naming. Defaults to 0.\n            timestamp (str, optional): Timestamp string for checkpoint directory naming.\n                Format: \"YYYYMMDD_HHMMSS\". Defaults to None.\n            best_loss (float, optional): Best evaluation loss achieved so far. Used to\n                determine when to save new checkpoints. Defaults to float('inf').\n\n        Returns:\n            tuple[int, float]: A tuple containing:\n                - global_step: Updated global step count after this epoch\n                - best_loss: Updated best evaluation loss (may be unchanged)\n\n        Note:\n            Prints training metrics every 100 steps. Evaluates every eval_steps and saves\n            checkpoints when evaluation loss improves. Logs to W&amp;B if enabled.\n        \"\"\"\n        model.train()\n\n        if bf16:\n            scaler = torch.amp.GradScaler(\"cuda\")\n            device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        for idx, inputs in enumerate(train_dataloader):\n            optim.zero_grad()\n            if torch.cuda.is_available() and bf16:\n                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                    inputs = inputs.to(self.device)\n                    loss, logs = model.loss(inputs)\n                scaler.scale(loss).backward()\n\n                if self.grad_clip_norm is not None:\n                    scaler.unscale_(optim)\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), self.grad_clip_norm\n                    )\n\n                scaler.step(optim)\n                scaler.update()\n\n            else:\n                inputs = inputs.to(self.device)\n                loss, logs = model.loss(inputs)\n\n                if self.grad_clip_norm is not None:\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), self.grad_clip_norm\n                    )\n\n                loss.backward()\n                optim.step()\n\n            if self.scheduler is not None and not isinstance(self.scheduler, lr_scheduler.ReduceLROnPlateau):\n                self.scheduler.step()\n\n            model.post_step()\n            global_step += 1\n\n            if self.log_wandb:\n                wandb.log({\n                    \"train/loss\": logs['mse'].item(),\n                    \"train/non_zero_frac\": logs['non_zero_frac'].item(),\n                    \"train/lr\": self.optim.param_groups[0]['lr'],\n                    \"global_step\": global_step\n                }, step=global_step)\n\n            if (idx % 100) == 0:\n                current_lr = self.optim.param_groups[0]['lr']\n                print(\n                    f\"Step [{idx}/{len(train_dataloader)}] - \"\n                    f\"train_loss: {round(logs['mse'].item(), 3)} - \"\n                    f\"train_nz_frac: {round(logs['non_zero_frac'].item(), 3)} - \"\n                    f\"lr: {current_lr:.2e}\"\n                )\n\n            if global_step % self.eval_steps == 0:\n                print(f\"\\n{'='*60}\")\n                print(f\"Intermediate Evaluation at step {global_step}\")\n                print(f\"{'='*60}\")\n                eval_loss = self.evaluate(\n                    model=model,\n                    eval_dataloader=self.eval_dataloader,\n                    bf16=bf16\n                )\n\n                if self.log_wandb:\n                    wandb.log({\n                        \"eval/loss\": eval_loss,\n                        \"global_step\": global_step\n                    }, step=global_step)\n\n                if self.save_checkpoints and eval_loss &lt; best_loss:\n                    if self.save_best_only:\n                        save_path = f\"saved_models/{self.model_name}/run_{timestamp}/best_model.pt\"\n                    else:\n                        save_path = f\"saved_models/{self.model_name}/run_{timestamp}/sae_step_{global_step}.pt\"\n                    torch.save(model.state_dict(), save_path)\n                    print(f\"New best model saved (loss: {eval_loss:.6f})\")\n                    best_loss = eval_loss\n\n                model.train()\n\n        return global_step, best_loss\n\n    @torch.no_grad()\n    def evaluate(\n            self,\n            model: torch.nn.Module, \n            eval_dataloader: torch.utils.data.DataLoader, \n            bf16: bool = True\n        ) -&gt; float:\n        \"\"\"Evaluate the model on the validation dataset without gradient computation.\n\n        Computes average loss across all evaluation batches with optional mixed precision.\n        Useful for monitoring generalization and preventing overfitting.\n\n        Args:\n            model (torch.nn.Module): The sparse autoencoder model to evaluate.\n            eval_dataloader (torch.utils.data.DataLoader): DataLoader providing evaluation batches.\n            bf16 (bool, optional): Whether to use bfloat16 mixed precision for evaluation.\n                Should match training setting. Defaults to True.\n\n        Returns:\n            float: Average evaluation loss across all batches. Computed as total loss\n                divided by number of batches.\n\n        Note:\n            Prints evaluation metrics every 100 steps. Sets model to eval mode and\n            restores training mode afterward if called during training.\n        \"\"\"\n        model.eval()\n        n_batches = 0\n        total_loss = 0.0\n\n        if bf16:\n            device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        for idx, inputs in enumerate(eval_dataloader):\n            if torch.cuda.is_available() and bf16:\n                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                    inputs = inputs.to(self.device)\n                    loss, logs = model.loss(inputs)\n            else:\n                inputs = inputs.to(self.device)\n                loss, logs = model.loss(inputs)\n\n            total_loss += loss.item()\n            n_batches += 1\n\n            if (idx % 100) == 0:\n                current_lr = self.optim.param_groups[0]['lr']\n                print(\n                    f\"Step [{idx}/{len(eval_dataloader)}] - \"\n                    f\"eval_loss: {round(logs['mse'].item(), 3)} - \"\n                    f\"eval_nz_frac: {round(logs['non_zero_frac'].item(), 3)} - \"\n                    f\"lr: {current_lr:.2e}\"\n                )\n\n        avg_loss = total_loss / n_batches\n        print(f\"Avg loss: {avg_loss:.3f}\")\n        return avg_loss\n\n    def train(self) -&gt; None:\n        \"\"\"Execute the complete training loop for all epochs.\n\n        Main training orchestrator that:\n        1. Sets random seed for reproducibility\n        2. Moves model to appropriate device\n        3. Creates checkpoint directories if saving enabled\n        4. Runs training epochs with periodic evaluation\n        5. Saves best models based on evaluation loss\n        6. Applies learning rate scheduling\n        7. Logs metrics to W&amp;B if enabled\n        8. Finalizes W&amp;B run on completion\n\n        Note:\n            Checkpoints saved to saved_models/{model_name}/run_{timestamp}/.\n            Each epoch includes full pass through training data followed by evaluation.\n            Best model determined by lowest evaluation loss.\n        \"\"\"\n        self.set_seed(self.random_seed)\n        self.model.to(self.device)\n\n        if self.save_checkpoints:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            os.makedirs(f\"saved_models/{self.model_name}/run_{timestamp}\", exist_ok=True)\n\n        best_loss = float('inf')\n        global_step = 0\n\n        for epoch in range(self.epochs):\n            print(f\"\\nEpoch [{epoch+1}/{self.epochs}]\")\n            global_step, best_loss = self.train_one_epoch(\n                model=self.model, \n                train_dataloader=self.train_dataloader, \n                optim=self.optim, \n                bf16=self.bf16,\n                global_step=global_step,\n                timestamp=timestamp,\n                best_loss=best_loss\n            )\n\n            print(f\"\\n{'='*60}\")\n            print(f\"End of epoch {epoch+1} evaluation\")\n            print(f\"{'='*60}\")\n            loss = self.evaluate(\n                model=self.model,\n                eval_dataloader=self.eval_dataloader,\n                bf16=self.bf16\n            )\n\n            if self.log_wandb:\n                wandb.log({\n                    \"epoch\": epoch + 1,\n                    \"eval/epoch_loss\": loss,\n                }, step=global_step)\n\n            if self.save_checkpoints and loss &lt; best_loss:\n                if self.save_best_only:\n                    save_path = f\"saved_models/{self.model_name}/run_{timestamp}/best_model.pt\"\n                else:\n                    save_path = f\"saved_models/{self.model_name}/run_{timestamp}/sae_epoch_{epoch+1}.pt\"\n                torch.save(self.model.state_dict(), save_path)\n                print(f\"New best model saved (loss: {loss:.3f})\")\n                best_loss = loss\n\n            if self.scheduler is not None and isinstance(self.scheduler, lr_scheduler.ReduceLROnPlateau):\n                self.scheduler.step(loss)\n\n        if self.log_wandb:\n            wandb.finish()\n\n        print(\"Finished training!\")\n\n    def set_lr_scheduler(self, lr_type: str = 'cosine') -&gt; lr_scheduler:\n        \"\"\"Configure and initialize learning rate scheduler with warmup.\n\n        Creates a learning rate scheduler based on the specified type. 'cosine' and 'linear'\n        schedulers include warmup phase for training stability. 'plateau' reduces learning\n        rate when validation loss plateaus.\n\n        Args:\n            lr_type (str, optional): Type of learning rate scheduler. Options:\n                - 'cosine': Linear warmup followed by cosine annealing to 10% of initial LR\n                - 'plateau': Reduces LR by factor when validation loss stops improving\n                - 'linear': Linear warmup followed by linear decay to 10% of initial LR\n                Defaults to 'cosine'.\n\n        Returns:\n            lr_scheduler: Configured PyTorch learning rate scheduler instance.\n\n        Note:\n            For 'cosine' and 'linear', warmup steps = warmup_fraction * total_steps.\n            'plateau' scheduler requires manual .step(loss) calls, which are handled\n            automatically at epoch end.\n        \"\"\"\n        assert lr_type in ['cosine', 'plateau', 'linear'], \"Use 'cosine', 'plateau', or 'linear'\"\n\n        total_steps = self.epochs * len(self.train_dataloader)\n        warmup_steps = int(total_steps * self.warmup_fraction)\n\n        if lr_type == 'cosine':\n            warmup = lr_scheduler.LinearLR(\n                self.optim, start_factor=0.01, end_factor=1.0,\n                total_iters=warmup_steps\n            )\n            cosine = lr_scheduler.CosineAnnealingLR(\n                self.optim, T_max=total_steps - warmup_steps,\n                eta_min=self.optim.param_groups[0]['lr'] * 0.1\n            )\n            self.scheduler = lr_scheduler.SequentialLR(\n                self.optim, schedulers=[warmup, cosine],\n                milestones=[warmup_steps]\n            )\n        elif lr_type == 'plateau':\n            self.scheduler = lr_scheduler.ReduceLROnPlateau(\n                self.optim, patience=10, threshold=1e-4, min_lr=1e-6\n            )\n        else:\n            warmup = lr_scheduler.LinearLR(\n                self.optim, start_factor=0.01, end_factor=1.0,\n                total_iters=warmup_steps\n            )\n            decay = lr_scheduler.LinearLR(\n                self.optim, start_factor=1.0, end_factor=0.1,\n                total_iters=total_steps - warmup_steps\n            )\n            self.scheduler = lr_scheduler.SequentialLR(\n                self.optim, schedulers=[warmup, decay],\n                milestones=[warmup_steps]\n            )\n        return self.scheduler\n\n    def set_seed(self, seed: int = 1) -&gt; None:\n        \"\"\"Set random seeds for reproducible training across all libraries.\n\n        Configures random number generators for NumPy, PyTorch (CPU and CUDA), and\n        makes CUDNN operations deterministic. Essential for experiment reproducibility.\n\n        Args:\n            seed (int, optional): Random seed value to use. Same seed should produce\n                identical results across runs (assuming same hardware/software).\n                Defaults to 1.\n\n        Note:\n            Setting deterministic=True may reduce performance. Disables CUDNN benchmarking\n            for reproducibility.\n        \"\"\"\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        print(f\"Using random seed: {seed}\")\n\n    @staticmethod\n    def config_from_yaml(file: str) -&gt; dict:\n        \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n        Static utility method for loading model or training configurations from YAML.\n        Useful for maintaining configuration files separately from code.\n\n        Args:\n            file (str): Path to the YAML configuration file.\n\n        Returns:\n            dict: Dictionary containing parsed configuration parameters.\n        \"\"\"\n        with open(file, \"r\") as f:\n            config = yaml.safe_load(f)\n        return config\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.__init__","title":"<code>__init__(train_dataloader=None, eval_dataloader=None, model=None, model_name='sae', optim=torch.optim.Adam, epochs=20, bf16=False, random_seed=42, save_checkpoints=True, device='auto', grad_clip_norm=None, lrs_type=None, eval_steps=5000, warmup_fraction=0.1, save_best_only=True, log_to_wandb=True)</code>","text":"<p>Initialize the SAETrainer with model, data, and training configuration.</p> <p>Sets up the training environment including device placement, learning rate scheduling, and Weights &amp; Biases logging if enabled.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader</code> <code>DataLoader</code> <p>DataLoader for training data containing batches of activation tensors. Defaults to None.</p> <code>None</code> <code>eval_dataloader</code> <code>DataLoader</code> <p>DataLoader for evaluation/validation data. Used for periodic evaluation during training. Defaults to None.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Sparse autoencoder model to train. Should be an instance of SparseAutoencoder or compatible architecture. Defaults to None.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name used for organizing saved checkpoints and W&amp;B project naming. Creates directory structure at saved_models/{model_name}/. Defaults to \"sae\".</p> <code>'sae'</code> <code>optim</code> <code>Optimizer</code> <p>Initialized optimizer instance with model parameters already attached. Defaults to torch.optim.Adam.</p> <code>Adam</code> <code>epochs</code> <code>int</code> <p>Number of complete passes through the training dataset. Defaults to 20.</p> <code>20</code> <code>bf16</code> <code>bool</code> <p>Whether to use bfloat16 mixed precision training for faster computation and reduced memory usage. Requires CUDA support. Defaults to False.</p> <code>False</code> <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility across numpy, PyTorch, and CUDNN. Defaults to 42.</p> <code>42</code> <code>save_checkpoints</code> <code>bool</code> <p>Whether to save model checkpoints during training. Checkpoints saved when evaluation loss improves. Defaults to True.</p> <code>True</code> <code>device</code> <code>str</code> <p>Device for training. Can be \"auto\" for automatic selection (prefers cuda &gt; mps &gt; cpu), \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".</p> <code>'auto'</code> <code>grad_clip_norm</code> <code>float</code> <p>Maximum gradient norm for gradient clipping. Helps prevent gradient explosion. If None, no clipping is applied. Defaults to None.</p> <code>None</code> <code>lrs_type</code> <code>str</code> <p>Learning rate scheduler type. Options: 'cosine', 'plateau', 'linear'. If None, uses constant learning rate. Defaults to None.</p> <code>None</code> <code>eval_steps</code> <code>int</code> <p>Evaluate model every N training steps and save checkpoint if loss improves. Defaults to 5000.</p> <code>5000</code> <code>warmup_fraction</code> <code>float</code> <p>Fraction of total training steps to use for learning rate warmup. Applies to 'cosine' and 'linear' schedulers. Defaults to 0.1 (10% warmup).</p> <code>0.1</code> <code>save_best_only</code> <code>bool</code> <p>If True, only saves the best checkpoint (overwrites previous best). If False, saves all improving checkpoints. Defaults to True.</p> <code>True</code> <code>log_to_wandb</code> <code>bool</code> <p>Whether to log metrics to Weights &amp; Biases. Logs training/eval loss, non-zero fraction, learning rate, and config. Defaults to True.</p> <code>True</code> Note <p>The optimizer must be initialized with the model's parameters before passing to the trainer. Learning rate and other optimizer settings should be configured in the optimizer instance.</p> Source code in <code>deeplens/train.py</code> <pre><code>def __init__(\n        self, \n        train_dataloader: DataLoader = None, \n        eval_dataloader: DataLoader = None, \n        model: torch.nn.Module = None, \n        model_name: str = \"sae\",\n        optim: torch.optim.Optimizer = torch.optim.Adam,\n        epochs: int = 20, \n        bf16: bool = False,\n        random_seed: int = 42,\n        save_checkpoints: bool = True,\n        device: str = \"auto\",\n        grad_clip_norm: float = None,\n        lrs_type: str = None,\n        eval_steps: int = 5000,\n        warmup_fraction: float = 0.1,\n        save_best_only: bool = True,\n        log_to_wandb: bool = True\n    ) -&gt; None:\n    \"\"\"Initialize the SAETrainer with model, data, and training configuration.\n\n    Sets up the training environment including device placement, learning rate scheduling,\n    and Weights &amp; Biases logging if enabled.\n\n    Args:\n        train_dataloader (DataLoader, optional): DataLoader for training data containing\n            batches of activation tensors. Defaults to None.\n        eval_dataloader (DataLoader, optional): DataLoader for evaluation/validation data.\n            Used for periodic evaluation during training. Defaults to None.\n        model (torch.nn.Module, optional): Sparse autoencoder model to train. Should be\n            an instance of SparseAutoencoder or compatible architecture. Defaults to None.\n        model_name (str, optional): Name used for organizing saved checkpoints and W&amp;B\n            project naming. Creates directory structure at saved_models/{model_name}/.\n            Defaults to \"sae\".\n        optim (torch.optim.Optimizer, optional): Initialized optimizer instance with\n            model parameters already attached. Defaults to torch.optim.Adam.\n        epochs (int, optional): Number of complete passes through the training dataset.\n            Defaults to 20.\n        bf16 (bool, optional): Whether to use bfloat16 mixed precision training for\n            faster computation and reduced memory usage. Requires CUDA support.\n            Defaults to False.\n        random_seed (int, optional): Random seed for reproducibility across numpy,\n            PyTorch, and CUDNN. Defaults to 42.\n        save_checkpoints (bool, optional): Whether to save model checkpoints during\n            training. Checkpoints saved when evaluation loss improves. Defaults to True.\n        device (str, optional): Device for training. Can be \"auto\" for automatic selection\n            (prefers cuda &gt; mps &gt; cpu), \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n        grad_clip_norm (float, optional): Maximum gradient norm for gradient clipping.\n            Helps prevent gradient explosion. If None, no clipping is applied.\n            Defaults to None.\n        lrs_type (str, optional): Learning rate scheduler type. Options: 'cosine',\n            'plateau', 'linear'. If None, uses constant learning rate. Defaults to None.\n        eval_steps (int, optional): Evaluate model every N training steps and save\n            checkpoint if loss improves. Defaults to 5000.\n        warmup_fraction (float, optional): Fraction of total training steps to use for\n            learning rate warmup. Applies to 'cosine' and 'linear' schedulers.\n            Defaults to 0.1 (10% warmup).\n        save_best_only (bool, optional): If True, only saves the best checkpoint\n            (overwrites previous best). If False, saves all improving checkpoints.\n            Defaults to True.\n        log_to_wandb (bool, optional): Whether to log metrics to Weights &amp; Biases.\n            Logs training/eval loss, non-zero fraction, learning rate, and config.\n            Defaults to True.\n\n    Note:\n        The optimizer must be initialized with the model's parameters before passing\n        to the trainer. Learning rate and other optimizer settings should be configured\n        in the optimizer instance.\n    \"\"\"\n    self.model = model\n    self.model_name = model_name\n    self.optim = optim\n    self.epochs = epochs\n    self.train_dataloader = train_dataloader\n    self.eval_dataloader = eval_dataloader\n    self.bf16 = bf16\n    self.random_seed = random_seed\n    self.save_checkpoints = save_checkpoints\n    self.grad_clip_norm = grad_clip_norm\n    self.eval_steps = eval_steps\n    self.warmup_fraction = warmup_fraction\n    self.save_best_only = save_best_only\n    self.log_wandb = log_to_wandb\n\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    if lrs_type is not None:\n        self.scheduler = self.set_lr_scheduler(lrs_type)\n    else:\n        self.scheduler = None\n\n    if log_to_wandb:\n        time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        wandb.init(\n            project=f\"sparse-autoencoder\",\n            name=f\"run-{self.model_name}-{time}\",\n            config={\n                \"epochs\": epochs,\n                \"in_dims\": self.model.encoder.in_features,\n                \"sparse_dims\": self.model.encoder.out_features,\n                \"activations\": self.model.activation,\n                \"input_norm\": self.model.input_norm,\n                \"top_k\": self.model.k, \n                \"l1\": self.model.beta_l1,\n                \"tie_weights\": self.model.tie_weights,\n                \"unit_norm_decoder\": self.model.unit_norm_decoder,\n                \"lr\": optim.param_groups[0]['lr'],\n                \"lr_scheduler\": lrs_type,\n                \"warmup_fraction\": warmup_fraction,\n                \"seed\": random_seed,\n                \"grad_clip_norm\": grad_clip_norm,\n                \"bf16\": bf16\n            }\n        )\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.config_from_yaml","title":"<code>config_from_yaml(file)</code>  <code>staticmethod</code>","text":"<p>Load sparse autoencoder configuration from a YAML file.</p> <p>Static utility method for loading model or training configurations from YAML. Useful for maintaining configuration files separately from code.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing parsed configuration parameters.</p> Source code in <code>deeplens/train.py</code> <pre><code>@staticmethod\ndef config_from_yaml(file: str) -&gt; dict:\n    \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n    Static utility method for loading model or training configurations from YAML.\n    Useful for maintaining configuration files separately from code.\n\n    Args:\n        file (str): Path to the YAML configuration file.\n\n    Returns:\n        dict: Dictionary containing parsed configuration parameters.\n    \"\"\"\n    with open(file, \"r\") as f:\n        config = yaml.safe_load(f)\n    return config\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.evaluate","title":"<code>evaluate(model, eval_dataloader, bf16=True)</code>","text":"<p>Evaluate the model on the validation dataset without gradient computation.</p> <p>Computes average loss across all evaluation batches with optional mixed precision. Useful for monitoring generalization and preventing overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The sparse autoencoder model to evaluate.</p> required <code>eval_dataloader</code> <code>DataLoader</code> <p>DataLoader providing evaluation batches.</p> required <code>bf16</code> <code>bool</code> <p>Whether to use bfloat16 mixed precision for evaluation. Should match training setting. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average evaluation loss across all batches. Computed as total loss divided by number of batches.</p> Note <p>Prints evaluation metrics every 100 steps. Sets model to eval mode and restores training mode afterward if called during training.</p> Source code in <code>deeplens/train.py</code> <pre><code>@torch.no_grad()\ndef evaluate(\n        self,\n        model: torch.nn.Module, \n        eval_dataloader: torch.utils.data.DataLoader, \n        bf16: bool = True\n    ) -&gt; float:\n    \"\"\"Evaluate the model on the validation dataset without gradient computation.\n\n    Computes average loss across all evaluation batches with optional mixed precision.\n    Useful for monitoring generalization and preventing overfitting.\n\n    Args:\n        model (torch.nn.Module): The sparse autoencoder model to evaluate.\n        eval_dataloader (torch.utils.data.DataLoader): DataLoader providing evaluation batches.\n        bf16 (bool, optional): Whether to use bfloat16 mixed precision for evaluation.\n            Should match training setting. Defaults to True.\n\n    Returns:\n        float: Average evaluation loss across all batches. Computed as total loss\n            divided by number of batches.\n\n    Note:\n        Prints evaluation metrics every 100 steps. Sets model to eval mode and\n        restores training mode afterward if called during training.\n    \"\"\"\n    model.eval()\n    n_batches = 0\n    total_loss = 0.0\n\n    if bf16:\n        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    for idx, inputs in enumerate(eval_dataloader):\n        if torch.cuda.is_available() and bf16:\n            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                inputs = inputs.to(self.device)\n                loss, logs = model.loss(inputs)\n        else:\n            inputs = inputs.to(self.device)\n            loss, logs = model.loss(inputs)\n\n        total_loss += loss.item()\n        n_batches += 1\n\n        if (idx % 100) == 0:\n            current_lr = self.optim.param_groups[0]['lr']\n            print(\n                f\"Step [{idx}/{len(eval_dataloader)}] - \"\n                f\"eval_loss: {round(logs['mse'].item(), 3)} - \"\n                f\"eval_nz_frac: {round(logs['non_zero_frac'].item(), 3)} - \"\n                f\"lr: {current_lr:.2e}\"\n            )\n\n    avg_loss = total_loss / n_batches\n    print(f\"Avg loss: {avg_loss:.3f}\")\n    return avg_loss\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.set_lr_scheduler","title":"<code>set_lr_scheduler(lr_type='cosine')</code>","text":"<p>Configure and initialize learning rate scheduler with warmup.</p> <p>Creates a learning rate scheduler based on the specified type. 'cosine' and 'linear' schedulers include warmup phase for training stability. 'plateau' reduces learning rate when validation loss plateaus.</p> <p>Parameters:</p> Name Type Description Default <code>lr_type</code> <code>str</code> <p>Type of learning rate scheduler. Options: - 'cosine': Linear warmup followed by cosine annealing to 10% of initial LR - 'plateau': Reduces LR by factor when validation loss stops improving - 'linear': Linear warmup followed by linear decay to 10% of initial LR Defaults to 'cosine'.</p> <code>'cosine'</code> <p>Returns:</p> Name Type Description <code>lr_scheduler</code> <code>lr_scheduler</code> <p>Configured PyTorch learning rate scheduler instance.</p> Note <p>For 'cosine' and 'linear', warmup steps = warmup_fraction * total_steps. 'plateau' scheduler requires manual .step(loss) calls, which are handled automatically at epoch end.</p> Source code in <code>deeplens/train.py</code> <pre><code>def set_lr_scheduler(self, lr_type: str = 'cosine') -&gt; lr_scheduler:\n    \"\"\"Configure and initialize learning rate scheduler with warmup.\n\n    Creates a learning rate scheduler based on the specified type. 'cosine' and 'linear'\n    schedulers include warmup phase for training stability. 'plateau' reduces learning\n    rate when validation loss plateaus.\n\n    Args:\n        lr_type (str, optional): Type of learning rate scheduler. Options:\n            - 'cosine': Linear warmup followed by cosine annealing to 10% of initial LR\n            - 'plateau': Reduces LR by factor when validation loss stops improving\n            - 'linear': Linear warmup followed by linear decay to 10% of initial LR\n            Defaults to 'cosine'.\n\n    Returns:\n        lr_scheduler: Configured PyTorch learning rate scheduler instance.\n\n    Note:\n        For 'cosine' and 'linear', warmup steps = warmup_fraction * total_steps.\n        'plateau' scheduler requires manual .step(loss) calls, which are handled\n        automatically at epoch end.\n    \"\"\"\n    assert lr_type in ['cosine', 'plateau', 'linear'], \"Use 'cosine', 'plateau', or 'linear'\"\n\n    total_steps = self.epochs * len(self.train_dataloader)\n    warmup_steps = int(total_steps * self.warmup_fraction)\n\n    if lr_type == 'cosine':\n        warmup = lr_scheduler.LinearLR(\n            self.optim, start_factor=0.01, end_factor=1.0,\n            total_iters=warmup_steps\n        )\n        cosine = lr_scheduler.CosineAnnealingLR(\n            self.optim, T_max=total_steps - warmup_steps,\n            eta_min=self.optim.param_groups[0]['lr'] * 0.1\n        )\n        self.scheduler = lr_scheduler.SequentialLR(\n            self.optim, schedulers=[warmup, cosine],\n            milestones=[warmup_steps]\n        )\n    elif lr_type == 'plateau':\n        self.scheduler = lr_scheduler.ReduceLROnPlateau(\n            self.optim, patience=10, threshold=1e-4, min_lr=1e-6\n        )\n    else:\n        warmup = lr_scheduler.LinearLR(\n            self.optim, start_factor=0.01, end_factor=1.0,\n            total_iters=warmup_steps\n        )\n        decay = lr_scheduler.LinearLR(\n            self.optim, start_factor=1.0, end_factor=0.1,\n            total_iters=total_steps - warmup_steps\n        )\n        self.scheduler = lr_scheduler.SequentialLR(\n            self.optim, schedulers=[warmup, decay],\n            milestones=[warmup_steps]\n        )\n    return self.scheduler\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.set_seed","title":"<code>set_seed(seed=1)</code>","text":"<p>Set random seeds for reproducible training across all libraries.</p> <p>Configures random number generators for NumPy, PyTorch (CPU and CUDA), and makes CUDNN operations deterministic. Essential for experiment reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value to use. Same seed should produce identical results across runs (assuming same hardware/software). Defaults to 1.</p> <code>1</code> Note <p>Setting deterministic=True may reduce performance. Disables CUDNN benchmarking for reproducibility.</p> Source code in <code>deeplens/train.py</code> <pre><code>def set_seed(self, seed: int = 1) -&gt; None:\n    \"\"\"Set random seeds for reproducible training across all libraries.\n\n    Configures random number generators for NumPy, PyTorch (CPU and CUDA), and\n    makes CUDNN operations deterministic. Essential for experiment reproducibility.\n\n    Args:\n        seed (int, optional): Random seed value to use. Same seed should produce\n            identical results across runs (assuming same hardware/software).\n            Defaults to 1.\n\n    Note:\n        Setting deterministic=True may reduce performance. Disables CUDNN benchmarking\n        for reproducibility.\n    \"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    print(f\"Using random seed: {seed}\")\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.train","title":"<code>train()</code>","text":"<p>Execute the complete training loop for all epochs.</p> <p>Main training orchestrator that: 1. Sets random seed for reproducibility 2. Moves model to appropriate device 3. Creates checkpoint directories if saving enabled 4. Runs training epochs with periodic evaluation 5. Saves best models based on evaluation loss 6. Applies learning rate scheduling 7. Logs metrics to W&amp;B if enabled 8. Finalizes W&amp;B run on completion</p> Note <p>Checkpoints saved to saved_models/{model_name}/run_{timestamp}/. Each epoch includes full pass through training data followed by evaluation. Best model determined by lowest evaluation loss.</p> Source code in <code>deeplens/train.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Execute the complete training loop for all epochs.\n\n    Main training orchestrator that:\n    1. Sets random seed for reproducibility\n    2. Moves model to appropriate device\n    3. Creates checkpoint directories if saving enabled\n    4. Runs training epochs with periodic evaluation\n    5. Saves best models based on evaluation loss\n    6. Applies learning rate scheduling\n    7. Logs metrics to W&amp;B if enabled\n    8. Finalizes W&amp;B run on completion\n\n    Note:\n        Checkpoints saved to saved_models/{model_name}/run_{timestamp}/.\n        Each epoch includes full pass through training data followed by evaluation.\n        Best model determined by lowest evaluation loss.\n    \"\"\"\n    self.set_seed(self.random_seed)\n    self.model.to(self.device)\n\n    if self.save_checkpoints:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        os.makedirs(f\"saved_models/{self.model_name}/run_{timestamp}\", exist_ok=True)\n\n    best_loss = float('inf')\n    global_step = 0\n\n    for epoch in range(self.epochs):\n        print(f\"\\nEpoch [{epoch+1}/{self.epochs}]\")\n        global_step, best_loss = self.train_one_epoch(\n            model=self.model, \n            train_dataloader=self.train_dataloader, \n            optim=self.optim, \n            bf16=self.bf16,\n            global_step=global_step,\n            timestamp=timestamp,\n            best_loss=best_loss\n        )\n\n        print(f\"\\n{'='*60}\")\n        print(f\"End of epoch {epoch+1} evaluation\")\n        print(f\"{'='*60}\")\n        loss = self.evaluate(\n            model=self.model,\n            eval_dataloader=self.eval_dataloader,\n            bf16=self.bf16\n        )\n\n        if self.log_wandb:\n            wandb.log({\n                \"epoch\": epoch + 1,\n                \"eval/epoch_loss\": loss,\n            }, step=global_step)\n\n        if self.save_checkpoints and loss &lt; best_loss:\n            if self.save_best_only:\n                save_path = f\"saved_models/{self.model_name}/run_{timestamp}/best_model.pt\"\n            else:\n                save_path = f\"saved_models/{self.model_name}/run_{timestamp}/sae_epoch_{epoch+1}.pt\"\n            torch.save(self.model.state_dict(), save_path)\n            print(f\"New best model saved (loss: {loss:.3f})\")\n            best_loss = loss\n\n        if self.scheduler is not None and isinstance(self.scheduler, lr_scheduler.ReduceLROnPlateau):\n            self.scheduler.step(loss)\n\n    if self.log_wandb:\n        wandb.finish()\n\n    print(\"Finished training!\")\n</code></pre>"},{"location":"api/core/#deeplens.train.SAETrainer.train_one_epoch","title":"<code>train_one_epoch(model, train_dataloader, optim, bf16=True, global_step=0, timestamp=None, best_loss=float('inf'))</code>","text":"<p>Execute one complete training epoch with periodic evaluation and checkpointing.</p> <p>Iterates through all training batches, performs forward/backward passes with optional mixed precision, applies gradient clipping, updates learning rate, and periodically evaluates and saves checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The sparse autoencoder model to train.</p> required <code>train_dataloader</code> <code>DataLoader</code> <p>DataLoader providing training batches.</p> required <code>optim</code> <code>Optimizer</code> <p>Optimizer for updating model parameters.</p> required <code>bf16</code> <code>bool</code> <p>Whether to use bfloat16 mixed precision. Defaults to True.</p> <code>True</code> <code>global_step</code> <code>int</code> <p>Current global training step count across all epochs. Used for logging and checkpoint naming. Defaults to 0.</p> <code>0</code> <code>timestamp</code> <code>str</code> <p>Timestamp string for checkpoint directory naming. Format: \"YYYYMMDD_HHMMSS\". Defaults to None.</p> <code>None</code> <code>best_loss</code> <code>float</code> <p>Best evaluation loss achieved so far. Used to determine when to save new checkpoints. Defaults to float('inf').</p> <code>float('inf')</code> <p>Returns:</p> Type Description <code>tuple[int, float]</code> <p>tuple[int, float]: A tuple containing: - global_step: Updated global step count after this epoch - best_loss: Updated best evaluation loss (may be unchanged)</p> Note <p>Prints training metrics every 100 steps. Evaluates every eval_steps and saves checkpoints when evaluation loss improves. Logs to W&amp;B if enabled.</p> Source code in <code>deeplens/train.py</code> <pre><code>def train_one_epoch(\n        self, \n        model: torch.nn.Module, \n        train_dataloader: torch.utils.data.DataLoader, \n        optim: torch.optim.Optimizer, \n        bf16: bool = True,\n        global_step: int = 0,\n        timestamp: str = None,\n        best_loss: float = float('inf')\n    ) -&gt; tuple[int, float]:\n    \"\"\"Execute one complete training epoch with periodic evaluation and checkpointing.\n\n    Iterates through all training batches, performs forward/backward passes with optional\n    mixed precision, applies gradient clipping, updates learning rate, and periodically\n    evaluates and saves checkpoints.\n\n    Args:\n        model (torch.nn.Module): The sparse autoencoder model to train.\n        train_dataloader (torch.utils.data.DataLoader): DataLoader providing training batches.\n        optim (torch.optim.Optimizer): Optimizer for updating model parameters.\n        bf16 (bool, optional): Whether to use bfloat16 mixed precision. Defaults to True.\n        global_step (int, optional): Current global training step count across all epochs.\n            Used for logging and checkpoint naming. Defaults to 0.\n        timestamp (str, optional): Timestamp string for checkpoint directory naming.\n            Format: \"YYYYMMDD_HHMMSS\". Defaults to None.\n        best_loss (float, optional): Best evaluation loss achieved so far. Used to\n            determine when to save new checkpoints. Defaults to float('inf').\n\n    Returns:\n        tuple[int, float]: A tuple containing:\n            - global_step: Updated global step count after this epoch\n            - best_loss: Updated best evaluation loss (may be unchanged)\n\n    Note:\n        Prints training metrics every 100 steps. Evaluates every eval_steps and saves\n        checkpoints when evaluation loss improves. Logs to W&amp;B if enabled.\n    \"\"\"\n    model.train()\n\n    if bf16:\n        scaler = torch.amp.GradScaler(\"cuda\")\n        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    for idx, inputs in enumerate(train_dataloader):\n        optim.zero_grad()\n        if torch.cuda.is_available() and bf16:\n            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                inputs = inputs.to(self.device)\n                loss, logs = model.loss(inputs)\n            scaler.scale(loss).backward()\n\n            if self.grad_clip_norm is not None:\n                scaler.unscale_(optim)\n                torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), self.grad_clip_norm\n                )\n\n            scaler.step(optim)\n            scaler.update()\n\n        else:\n            inputs = inputs.to(self.device)\n            loss, logs = model.loss(inputs)\n\n            if self.grad_clip_norm is not None:\n                torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), self.grad_clip_norm\n                )\n\n            loss.backward()\n            optim.step()\n\n        if self.scheduler is not None and not isinstance(self.scheduler, lr_scheduler.ReduceLROnPlateau):\n            self.scheduler.step()\n\n        model.post_step()\n        global_step += 1\n\n        if self.log_wandb:\n            wandb.log({\n                \"train/loss\": logs['mse'].item(),\n                \"train/non_zero_frac\": logs['non_zero_frac'].item(),\n                \"train/lr\": self.optim.param_groups[0]['lr'],\n                \"global_step\": global_step\n            }, step=global_step)\n\n        if (idx % 100) == 0:\n            current_lr = self.optim.param_groups[0]['lr']\n            print(\n                f\"Step [{idx}/{len(train_dataloader)}] - \"\n                f\"train_loss: {round(logs['mse'].item(), 3)} - \"\n                f\"train_nz_frac: {round(logs['non_zero_frac'].item(), 3)} - \"\n                f\"lr: {current_lr:.2e}\"\n            )\n\n        if global_step % self.eval_steps == 0:\n            print(f\"\\n{'='*60}\")\n            print(f\"Intermediate Evaluation at step {global_step}\")\n            print(f\"{'='*60}\")\n            eval_loss = self.evaluate(\n                model=model,\n                eval_dataloader=self.eval_dataloader,\n                bf16=bf16\n            )\n\n            if self.log_wandb:\n                wandb.log({\n                    \"eval/loss\": eval_loss,\n                    \"global_step\": global_step\n                }, step=global_step)\n\n            if self.save_checkpoints and eval_loss &lt; best_loss:\n                if self.save_best_only:\n                    save_path = f\"saved_models/{self.model_name}/run_{timestamp}/best_model.pt\"\n                else:\n                    save_path = f\"saved_models/{self.model_name}/run_{timestamp}/sae_step_{global_step}.pt\"\n                torch.save(model.state_dict(), save_path)\n                print(f\"New best model saved (loss: {eval_loss:.6f})\")\n                best_loss = eval_loss\n\n            model.train()\n\n    return global_step, best_loss\n</code></pre>"},{"location":"api/core/#deeplens.pipeline","title":"<code>deeplens.pipeline</code>","text":""},{"location":"api/core/#deeplens.pipeline.InterveneFeatures","title":"<code>InterveneFeatures</code>","text":"<p>Manipulate and intervene on sparse autoencoder latent features.</p> <p>This class loads a trained sparse autoencoder and provides methods to analyze and modify its latent feature space, enabling causal analysis of feature effects on model behavior.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>class InterveneFeatures():\n    \"\"\"Manipulate and intervene on sparse autoencoder latent features.\n\n    This class loads a trained sparse autoencoder and provides methods to analyze\n    and modify its latent feature space, enabling causal analysis of feature effects\n    on model behavior.\n    \"\"\"\n    def __init__(\n            self,\n            sae_model: str = None,\n            sae_config: str | dict = None,\n            device: str = \"auto\"\n        ):\n        \"\"\"Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.\n\n        This class provides functionality to load a trained sparse autoencoder model and\n        intervene on its latent feature space to analyze and modify activations.\n\n        Args:\n            sae_model (str, optional): Path to the trained sparse autoencoder model weights file.\n                Should be a .pt or .pth file containing the model state dict. Defaults to None.\n            sae_config (str | dict, optional): Configuration for the sparse autoencoder.\n                Can be either a dictionary containing model hyperparameters or a path to a\n                YAML configuration file. Defaults to None.\n            device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n                selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n        \"\"\"\n        self.model_dir = sae_model\n\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        if isinstance(sae_config, dict):\n            self.model_config = sae_config\n        elif isinstance(sae_config, str) and sae_config.endswith(\".yaml\"):\n            self.model_config = self.config_from_yaml(sae_config)\n        else:\n            raise ValueError(\"sae_config must be dict or path to .yaml file.\")\n\n        self.model = self.load_model()\n\n    @torch.no_grad()\n    def get_decoded(self, activations) -&gt; torch.Tensor:\n        \"\"\"Encode input activations through the sparse autoencoder to get latent features.\n\n        Passes the input activations through the sparse autoencoder's forward pass and\n        returns the latent feature representation (z) from the encoded space.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode. Can be a\n                PyTorch tensor or any array-like structure that can be converted to a tensor.\n\n        Returns:\n            torch.Tensor: The latent feature representation (z) from the sparse autoencoder's\n                encoded space.\n        \"\"\"\n        if not isinstance(activations, torch.Tensor):\n            activations = torch.Tensor(activations)\n        activations = activations.to(self.device)\n        _, z, _ = self.model(activations)\n        return z\n\n    @torch.no_grad()\n    def get_alive_features(\n            self, \n            activations: torch.Tensor, \n            token_position: int = -1, \n            k: int | None = None,\n            return_values: bool = False\n        ) -&gt; torch.Tensor:\n        \"\"\"Get indices of non-zero (active) features in the latent space for a specific token.\n\n        Encodes the input activations through the sparse autoencoder and identifies which\n        latent features are active (non-zero) at the specified token position.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode. Can be a\n                PyTorch tensor or any array-like structure that can be converted to a tensor.\n            token_position (int, optional): Position of the token in the sequence to analyze.\n                Use -1 for the last token. Defaults to -1.\n            k (int, optional): If provided, returns only the top-k most active features\n                instead of all non-zero features. Defaults to None.\n            return_values (bool, optional): If True, returns both indices and values.\n                Defaults to False.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False,\n                returns a 1D tensor containing the indices of non-zero features. If True,\n                returns a tuple of (indices, values).\n        \"\"\"\n        z = self.get_decoded(activations)\n        z_token = z[token_position]\n        if k is not None:\n            topk_result = torch.topk(z_token, k=k)\n            feature_idxs = topk_result.indices\n            feature_vals = topk_result.values\n        else:\n            feature_idxs = torch.nonzero(z_token != 0, as_tuple=False).squeeze(-1)\n            feature_vals = z_token[feature_idxs]\n        if return_values:\n            return feature_idxs, feature_vals\n        return feature_idxs\n\n    @torch.no_grad()\n    def intervene_feature(\n            self, \n            activations, \n            feature: int, \n            alpha: float = 2.0,\n            token_positions: int | list[int] | None = None\n        ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Intervene on a specific latent feature by scaling its activation.\n\n        Encodes the input activations, multiplies the specified feature by alpha at the\n        given token positions, and returns both the original and modified decoded outputs\n        for comparison.\n\n        Args:\n            activations (torch.Tensor | array-like): Input activations to encode and modify.\n                Can be a PyTorch tensor or any array-like structure.\n            feature (int): Index of the latent feature to intervene on.\n            alpha (float, optional): Scaling factor to apply to the feature. Values &gt; 1\n                amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.\n            token_positions (int | list[int] | None, optional): Token position(s) at which\n                to apply the intervention. If None, applies to all tokens. If int, applies\n                to a single position. If list, applies to multiple positions. Defaults to None.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n                - activations: The original input activations\n                - original_decoded: Decoded output without intervention\n                - modified_decoded: Decoded output with the feature intervention applied\n        \"\"\"\n        if not isinstance(activations, torch.Tensor):\n            activations = torch.Tensor(activations).unsqueeze(0)\n\n        activations = activations.to(self.device)\n        _, z, _ = self.model(activations)\n        modified = z.clone()\n\n        if token_positions is None:\n            modified[:, feature] *= alpha\n        elif isinstance(token_positions, int):\n            modified[token_positions, feature] *= alpha\n        else:\n            for pos in token_positions:\n                modified[pos, feature] *= alpha\n\n        original_decoded = self.model.decode(z)\n        modified_decoded = self.model.decode(modified)\n        return activations, original_decoded, modified_decoded\n\n    def load_model(self) -&gt; torch.nn.Module:\n        \"\"\"Load the sparse autoencoder model from disk.\n\n        Loads the model weights from the specified path and initializes a\n        SparseAutoencoder instance with the provided configuration.\n\n        Returns:\n            torch.nn.Module: The loaded sparse autoencoder model moved to the\n                appropriate device.\n        \"\"\"\n        weights = torch.load(self.model_dir, map_location=self.device)\n        model = SparseAutoencoder(**self.model_config)\n        model.load_state_dict(state_dict=weights)\n        return model.to(self.device)\n\n    def config_from_yaml(self, file) -&gt; dict:\n        \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n        Reads and parses a YAML configuration file containing the hyperparameters\n        for the sparse autoencoder model.\n\n        Args:\n            file (str): Path to the YAML configuration file.\n\n        Returns:\n            dict: Dictionary containing the model configuration parameters.\n        \"\"\"\n        try:\n            with open(file, \"r\") as f:\n                config = yaml.safe_load(f)\n            return config\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Config file not found: {file}\")\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Invalid YAML in {file}: {e}\")\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.__init__","title":"<code>__init__(sae_model=None, sae_config=None, device='auto')</code>","text":"<p>Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.</p> <p>This class provides functionality to load a trained sparse autoencoder model and intervene on its latent feature space to analyze and modify activations.</p> <p>Parameters:</p> Name Type Description Default <code>sae_model</code> <code>str</code> <p>Path to the trained sparse autoencoder model weights file. Should be a .pt or .pth file containing the model state dict. Defaults to None.</p> <code>None</code> <code>sae_config</code> <code>str | dict</code> <p>Configuration for the sparse autoencoder. Can be either a dictionary containing model hyperparameters or a path to a YAML configuration file. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to run computations on. Can be \"auto\" for automatic selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".</p> <code>'auto'</code> Source code in <code>deeplens/intervene.py</code> <pre><code>def __init__(\n        self,\n        sae_model: str = None,\n        sae_config: str | dict = None,\n        device: str = \"auto\"\n    ):\n    \"\"\"Initialize the InterveneFeatures class for manipulating sparse autoencoder latent features.\n\n    This class provides functionality to load a trained sparse autoencoder model and\n    intervene on its latent feature space to analyze and modify activations.\n\n    Args:\n        sae_model (str, optional): Path to the trained sparse autoencoder model weights file.\n            Should be a .pt or .pth file containing the model state dict. Defaults to None.\n        sae_config (str | dict, optional): Configuration for the sparse autoencoder.\n            Can be either a dictionary containing model hyperparameters or a path to a\n            YAML configuration file. Defaults to None.\n        device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n            selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n    \"\"\"\n    self.model_dir = sae_model\n\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    if isinstance(sae_config, dict):\n        self.model_config = sae_config\n    elif isinstance(sae_config, str) and sae_config.endswith(\".yaml\"):\n        self.model_config = self.config_from_yaml(sae_config)\n    else:\n        raise ValueError(\"sae_config must be dict or path to .yaml file.\")\n\n    self.model = self.load_model()\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.config_from_yaml","title":"<code>config_from_yaml(file)</code>","text":"<p>Load sparse autoencoder configuration from a YAML file.</p> <p>Reads and parses a YAML configuration file containing the hyperparameters for the sparse autoencoder model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the model configuration parameters.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def config_from_yaml(self, file) -&gt; dict:\n    \"\"\"Load sparse autoencoder configuration from a YAML file.\n\n    Reads and parses a YAML configuration file containing the hyperparameters\n    for the sparse autoencoder model.\n\n    Args:\n        file (str): Path to the YAML configuration file.\n\n    Returns:\n        dict: Dictionary containing the model configuration parameters.\n    \"\"\"\n    try:\n        with open(file, \"r\") as f:\n            config = yaml.safe_load(f)\n        return config\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Config file not found: {file}\")\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Invalid YAML in {file}: {e}\")\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.get_alive_features","title":"<code>get_alive_features(activations, token_position=-1, k=None, return_values=False)</code>","text":"<p>Get indices of non-zero (active) features in the latent space for a specific token.</p> <p>Encodes the input activations through the sparse autoencoder and identifies which latent features are active (non-zero) at the specified token position.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode. Can be a PyTorch tensor or any array-like structure that can be converted to a tensor.</p> required <code>token_position</code> <code>int</code> <p>Position of the token in the sequence to analyze. Use -1 for the last token. Defaults to -1.</p> <code>-1</code> <code>k</code> <code>int</code> <p>If provided, returns only the top-k most active features instead of all non-zero features. Defaults to None.</p> <code>None</code> <code>return_values</code> <code>bool</code> <p>If True, returns both indices and values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False, returns a 1D tensor containing the indices of non-zero features. If True, returns a tuple of (indices, values).</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef get_alive_features(\n        self, \n        activations: torch.Tensor, \n        token_position: int = -1, \n        k: int | None = None,\n        return_values: bool = False\n    ) -&gt; torch.Tensor:\n    \"\"\"Get indices of non-zero (active) features in the latent space for a specific token.\n\n    Encodes the input activations through the sparse autoencoder and identifies which\n    latent features are active (non-zero) at the specified token position.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode. Can be a\n            PyTorch tensor or any array-like structure that can be converted to a tensor.\n        token_position (int, optional): Position of the token in the sequence to analyze.\n            Use -1 for the last token. Defaults to -1.\n        k (int, optional): If provided, returns only the top-k most active features\n            instead of all non-zero features. Defaults to None.\n        return_values (bool, optional): If True, returns both indices and values.\n            Defaults to False.\n\n    Returns:\n        torch.Tensor | tuple[torch.Tensor, torch.Tensor]: If return_values is False,\n            returns a 1D tensor containing the indices of non-zero features. If True,\n            returns a tuple of (indices, values).\n    \"\"\"\n    z = self.get_decoded(activations)\n    z_token = z[token_position]\n    if k is not None:\n        topk_result = torch.topk(z_token, k=k)\n        feature_idxs = topk_result.indices\n        feature_vals = topk_result.values\n    else:\n        feature_idxs = torch.nonzero(z_token != 0, as_tuple=False).squeeze(-1)\n        feature_vals = z_token[feature_idxs]\n    if return_values:\n        return feature_idxs, feature_vals\n    return feature_idxs\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.get_decoded","title":"<code>get_decoded(activations)</code>","text":"<p>Encode input activations through the sparse autoencoder to get latent features.</p> <p>Passes the input activations through the sparse autoencoder's forward pass and returns the latent feature representation (z) from the encoded space.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode. Can be a PyTorch tensor or any array-like structure that can be converted to a tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The latent feature representation (z) from the sparse autoencoder's encoded space.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef get_decoded(self, activations) -&gt; torch.Tensor:\n    \"\"\"Encode input activations through the sparse autoencoder to get latent features.\n\n    Passes the input activations through the sparse autoencoder's forward pass and\n    returns the latent feature representation (z) from the encoded space.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode. Can be a\n            PyTorch tensor or any array-like structure that can be converted to a tensor.\n\n    Returns:\n        torch.Tensor: The latent feature representation (z) from the sparse autoencoder's\n            encoded space.\n    \"\"\"\n    if not isinstance(activations, torch.Tensor):\n        activations = torch.Tensor(activations)\n    activations = activations.to(self.device)\n    _, z, _ = self.model(activations)\n    return z\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.intervene_feature","title":"<code>intervene_feature(activations, feature, alpha=2.0, token_positions=None)</code>","text":"<p>Intervene on a specific latent feature by scaling its activation.</p> <p>Encodes the input activations, multiplies the specified feature by alpha at the given token positions, and returns both the original and modified decoded outputs for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | array - like</code> <p>Input activations to encode and modify. Can be a PyTorch tensor or any array-like structure.</p> required <code>feature</code> <code>int</code> <p>Index of the latent feature to intervene on.</p> required <code>alpha</code> <code>float</code> <p>Scaling factor to apply to the feature. Values &gt; 1 amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.</p> <code>2.0</code> <code>token_positions</code> <code>int | list[int] | None</code> <p>Token position(s) at which to apply the intervention. If None, applies to all tokens. If int, applies to a single position. If list, applies to multiple positions. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing: - activations: The original input activations - original_decoded: Decoded output without intervention - modified_decoded: Decoded output with the feature intervention applied</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef intervene_feature(\n        self, \n        activations, \n        feature: int, \n        alpha: float = 2.0,\n        token_positions: int | list[int] | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Intervene on a specific latent feature by scaling its activation.\n\n    Encodes the input activations, multiplies the specified feature by alpha at the\n    given token positions, and returns both the original and modified decoded outputs\n    for comparison.\n\n    Args:\n        activations (torch.Tensor | array-like): Input activations to encode and modify.\n            Can be a PyTorch tensor or any array-like structure.\n        feature (int): Index of the latent feature to intervene on.\n        alpha (float, optional): Scaling factor to apply to the feature. Values &gt; 1\n            amplify the feature, values &lt; 1 suppress it. Defaults to 2.0.\n        token_positions (int | list[int] | None, optional): Token position(s) at which\n            to apply the intervention. If None, applies to all tokens. If int, applies\n            to a single position. If list, applies to multiple positions. Defaults to None.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing:\n            - activations: The original input activations\n            - original_decoded: Decoded output without intervention\n            - modified_decoded: Decoded output with the feature intervention applied\n    \"\"\"\n    if not isinstance(activations, torch.Tensor):\n        activations = torch.Tensor(activations).unsqueeze(0)\n\n    activations = activations.to(self.device)\n    _, z, _ = self.model(activations)\n    modified = z.clone()\n\n    if token_positions is None:\n        modified[:, feature] *= alpha\n    elif isinstance(token_positions, int):\n        modified[token_positions, feature] *= alpha\n    else:\n        for pos in token_positions:\n            modified[pos, feature] *= alpha\n\n    original_decoded = self.model.decode(z)\n    modified_decoded = self.model.decode(modified)\n    return activations, original_decoded, modified_decoded\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.InterveneFeatures.load_model","title":"<code>load_model()</code>","text":"<p>Load the sparse autoencoder model from disk.</p> <p>Loads the model weights from the specified path and initializes a SparseAutoencoder instance with the provided configuration.</p> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The loaded sparse autoencoder model moved to the appropriate device.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def load_model(self) -&gt; torch.nn.Module:\n    \"\"\"Load the sparse autoencoder model from disk.\n\n    Loads the model weights from the specified path and initializes a\n    SparseAutoencoder instance with the provided configuration.\n\n    Returns:\n        torch.nn.Module: The loaded sparse autoencoder model moved to the\n            appropriate device.\n    \"\"\"\n    weights = torch.load(self.model_dir, map_location=self.device)\n    model = SparseAutoencoder(**self.model_config)\n    model.load_state_dict(state_dict=weights)\n    return model.to(self.device)\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.ReinjectSingleSample","title":"<code>ReinjectSingleSample</code>","text":"<p>Reinject modified activations into a language model for causal inference.</p> <p>This class enables injecting modified activations back into a transformer model's forward pass to observe the causal effects on model predictions and generated text. Useful for validating feature interventions and conducting mechanistic interpretability experiments.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>class ReinjectSingleSample():\n    \"\"\"Reinject modified activations into a language model for causal inference.\n\n    This class enables injecting modified activations back into a transformer model's\n    forward pass to observe the causal effects on model predictions and generated text.\n    Useful for validating feature interventions and conducting mechanistic interpretability\n    experiments.\n    \"\"\"\n    def __init__(\n            self, \n            hf_model: str, \n            device: str = \"auto\", \n            cache_dir: str = 'cache'\n        ):\n        \"\"\"Initialize the ReinjectSingleSample class for causal inference with modified activations.\n\n        Loads a HuggingFace causal language model and tokenizer to enable reinjection of\n        modified activations into the model's forward pass for text generation and analysis.\n\n        Args:\n            hf_model (str): Name or path of the HuggingFace model to load.\n                Should be a valid model identifier from the HuggingFace model hub\n                (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n            device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n                selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n            cache_dir (str, optional): Directory to cache downloaded models.\n                Defaults to 'cache'.\n        \"\"\"\n        self.device = get_device(device)\n        print(f\"Running on device: {self.device}\")\n\n        os.makedirs(cache_dir, exist_ok=True)\n        self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.model.eval()\n\n    @torch.no_grad()\n    def reinject_and_generate(\n            self, \n            text, \n            modified_activations, \n            layer: int = 3, \n            generate: bool = False, \n            max_new_tokens: int = 25, \n            temperature: float = 1.0\n        ) -&gt; torch.Tensor | str:\n        \"\"\"Reinject modified activations into a model layer and optionally generate text.\n\n        Replaces the activations at the specified layer with the provided modified activations\n        during the forward pass. Can either return logits for the input text or generate\n        new tokens autoregressively.\n\n        Args:\n            text (str): Input text to tokenize and process through the model.\n            modified_activations (torch.Tensor): The modified activations to inject at the\n                specified layer. Should have the appropriate shape for the layer's output.\n            layer (int, optional): Index of the transformer layer where activations should\n                be replaced. Defaults to 3.\n            generate (bool, optional): If True, generates new tokens autoregressively.\n                If False, only returns logits for the input. Defaults to False.\n            max_new_tokens (int, optional): Maximum number of new tokens to generate when\n                generate=True. Defaults to 25.\n            temperature (float, optional): Sampling temperature for generation. Higher values\n                (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic.\n                Set to 0 for greedy decoding. Defaults to 1.0.\n\n        Returns:\n            torch.Tensor | str: If generate=False, returns the model's logits as a tensor.\n                If generate=True, returns the generated text as a string.\n\n        Note:\n            The hook is automatically removed after execution to prevent interference\n            with subsequent forward passes. For generation mode, the hook only affects\n            the first forward pass to avoid applying the intervention to newly generated tokens.\n        \"\"\"\n        modified_activations = modified_activations.to(self.device)\n        call_count = [0]\n        def replacement_hook(module, input, output):\n            if generate and call_count[0] &gt; 0:\n                return output\n            call_count[0] += 1\n            return modified_activations\n\n        mlp_module = self.get_module_for_replacement_hook(layer_idx=layer)\n        hook = mlp_module.register_forward_hook(replacement_hook)\n        tokens = self.tokenizer(text, return_tensors='pt').to(self.device)\n        try:\n            if generate:\n                generated_ids = self.model.generate(\n                    **tokens,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature,\n                    do_sample=temperature &gt; 0\n                )\n                return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n            else:\n                out = self.model(**tokens)\n                return out.logits\n        finally:\n            hook.remove()\n\n    def get_module_for_replacement_hook(self, layer_idx) -&gt; torch.nn.Module:\n        \"\"\"Get the MLP activation module for a specific layer.\n\n        Retrieves the MLP activation function module at the specified layer,\n        which can be used to register forward hooks for activation replacement.\n\n        Args:\n            layer_idx (int): Index of the transformer layer (0-indexed).\n\n        Returns:\n            torch.nn.Module: The MLP activation module at the specified layer.\n        \"\"\"\n        if isinstance(self.model, (\n            transformers.GPT2LMHeadModel, \n            transformers.FalconForCausalLM\n        )):\n            module = self.model.transformer.h[layer_idx].mlp.act\n        elif isinstance(self.model, (\n            transformers.LlamaForCausalLM, \n            transformers.MistralForCausalLM, \n            transformers.Gemma3ForCausalLM, \n            transformers.GemmaForCausalLM, \n            transformers.Qwen2ForCausalLM,\n            transformers.Qwen3ForCausalLM\n        )):\n            module = self.model.model.layers[layer_idx].mlp.act_fn\n        elif isinstance(self.model, (\n            transformers.PhiForCausalLM, \n            transformers.Phi3ForCausalLM\n        )):\n            module = self.model.model.layers[layer_idx].mlp.activation_fn\n        else:\n            raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n        return module\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.ReinjectSingleSample.__init__","title":"<code>__init__(hf_model, device='auto', cache_dir='cache')</code>","text":"<p>Initialize the ReinjectSingleSample class for causal inference with modified activations.</p> <p>Loads a HuggingFace causal language model and tokenizer to enable reinjection of modified activations into the model's forward pass for text generation and analysis.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace model to load. Should be a valid model identifier from the HuggingFace model hub (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").</p> required <code>device</code> <code>str</code> <p>Device to run computations on. Can be \"auto\" for automatic selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".</p> <code>'auto'</code> <code>cache_dir</code> <code>str</code> <p>Directory to cache downloaded models. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>deeplens/intervene.py</code> <pre><code>def __init__(\n        self, \n        hf_model: str, \n        device: str = \"auto\", \n        cache_dir: str = 'cache'\n    ):\n    \"\"\"Initialize the ReinjectSingleSample class for causal inference with modified activations.\n\n    Loads a HuggingFace causal language model and tokenizer to enable reinjection of\n    modified activations into the model's forward pass for text generation and analysis.\n\n    Args:\n        hf_model (str): Name or path of the HuggingFace model to load.\n            Should be a valid model identifier from the HuggingFace model hub\n            (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\").\n        device (str, optional): Device to run computations on. Can be \"auto\" for automatic\n            selection, \"cuda\" for GPU, or \"cpu\" for CPU. Defaults to \"auto\".\n        cache_dir (str, optional): Directory to cache downloaded models.\n            Defaults to 'cache'.\n    \"\"\"\n    self.device = get_device(device)\n    print(f\"Running on device: {self.device}\")\n\n    os.makedirs(cache_dir, exist_ok=True)\n    self.model = AutoModelForCausalLM.from_pretrained(hf_model, cache_dir=cache_dir).to(self.device)\n    self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.model.eval()\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.ReinjectSingleSample.get_module_for_replacement_hook","title":"<code>get_module_for_replacement_hook(layer_idx)</code>","text":"<p>Get the MLP activation module for a specific layer.</p> <p>Retrieves the MLP activation function module at the specified layer, which can be used to register forward hooks for activation replacement.</p> <p>Parameters:</p> Name Type Description Default <code>layer_idx</code> <code>int</code> <p>Index of the transformer layer (0-indexed).</p> required <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The MLP activation module at the specified layer.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>def get_module_for_replacement_hook(self, layer_idx) -&gt; torch.nn.Module:\n    \"\"\"Get the MLP activation module for a specific layer.\n\n    Retrieves the MLP activation function module at the specified layer,\n    which can be used to register forward hooks for activation replacement.\n\n    Args:\n        layer_idx (int): Index of the transformer layer (0-indexed).\n\n    Returns:\n        torch.nn.Module: The MLP activation module at the specified layer.\n    \"\"\"\n    if isinstance(self.model, (\n        transformers.GPT2LMHeadModel, \n        transformers.FalconForCausalLM\n    )):\n        module = self.model.transformer.h[layer_idx].mlp.act\n    elif isinstance(self.model, (\n        transformers.LlamaForCausalLM, \n        transformers.MistralForCausalLM, \n        transformers.Gemma3ForCausalLM, \n        transformers.GemmaForCausalLM, \n        transformers.Qwen2ForCausalLM,\n        transformers.Qwen3ForCausalLM\n    )):\n        module = self.model.model.layers[layer_idx].mlp.act_fn\n    elif isinstance(self.model, (\n        transformers.PhiForCausalLM, \n        transformers.Phi3ForCausalLM\n    )):\n        module = self.model.model.layers[layer_idx].mlp.activation_fn\n    else:\n        raise NotImplementedError(f\"Model type {type(self.model).__name__} is not currently supported.\")\n\n    return module\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.ReinjectSingleSample.reinject_and_generate","title":"<code>reinject_and_generate(text, modified_activations, layer=3, generate=False, max_new_tokens=25, temperature=1.0)</code>","text":"<p>Reinject modified activations into a model layer and optionally generate text.</p> <p>Replaces the activations at the specified layer with the provided modified activations during the forward pass. Can either return logits for the input text or generate new tokens autoregressively.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to tokenize and process through the model.</p> required <code>modified_activations</code> <code>Tensor</code> <p>The modified activations to inject at the specified layer. Should have the appropriate shape for the layer's output.</p> required <code>layer</code> <code>int</code> <p>Index of the transformer layer where activations should be replaced. Defaults to 3.</p> <code>3</code> <code>generate</code> <code>bool</code> <p>If True, generates new tokens autoregressively. If False, only returns logits for the input. Defaults to False.</p> <code>False</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of new tokens to generate when generate=True. Defaults to 25.</p> <code>25</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for generation. Higher values (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic. Set to 0 for greedy decoding. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor | str</code> <p>torch.Tensor | str: If generate=False, returns the model's logits as a tensor. If generate=True, returns the generated text as a string.</p> Note <p>The hook is automatically removed after execution to prevent interference with subsequent forward passes. For generation mode, the hook only affects the first forward pass to avoid applying the intervention to newly generated tokens.</p> Source code in <code>deeplens/intervene.py</code> <pre><code>@torch.no_grad()\ndef reinject_and_generate(\n        self, \n        text, \n        modified_activations, \n        layer: int = 3, \n        generate: bool = False, \n        max_new_tokens: int = 25, \n        temperature: float = 1.0\n    ) -&gt; torch.Tensor | str:\n    \"\"\"Reinject modified activations into a model layer and optionally generate text.\n\n    Replaces the activations at the specified layer with the provided modified activations\n    during the forward pass. Can either return logits for the input text or generate\n    new tokens autoregressively.\n\n    Args:\n        text (str): Input text to tokenize and process through the model.\n        modified_activations (torch.Tensor): The modified activations to inject at the\n            specified layer. Should have the appropriate shape for the layer's output.\n        layer (int, optional): Index of the transformer layer where activations should\n            be replaced. Defaults to 3.\n        generate (bool, optional): If True, generates new tokens autoregressively.\n            If False, only returns logits for the input. Defaults to False.\n        max_new_tokens (int, optional): Maximum number of new tokens to generate when\n            generate=True. Defaults to 25.\n        temperature (float, optional): Sampling temperature for generation. Higher values\n            (&gt;1.0) make output more random, lower values (&lt;1.0) more deterministic.\n            Set to 0 for greedy decoding. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor | str: If generate=False, returns the model's logits as a tensor.\n            If generate=True, returns the generated text as a string.\n\n    Note:\n        The hook is automatically removed after execution to prevent interference\n        with subsequent forward passes. For generation mode, the hook only affects\n        the first forward pass to avoid applying the intervention to newly generated tokens.\n    \"\"\"\n    modified_activations = modified_activations.to(self.device)\n    call_count = [0]\n    def replacement_hook(module, input, output):\n        if generate and call_count[0] &gt; 0:\n            return output\n        call_count[0] += 1\n        return modified_activations\n\n    mlp_module = self.get_module_for_replacement_hook(layer_idx=layer)\n    hook = mlp_module.register_forward_hook(replacement_hook)\n    tokens = self.tokenizer(text, return_tensors='pt').to(self.device)\n    try:\n        if generate:\n            generated_ids = self.model.generate(\n                **tokens,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=temperature &gt; 0\n            )\n            return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        else:\n            out = self.model(**tokens)\n            return out.logits\n    finally:\n        hook.remove()\n</code></pre>"},{"location":"api/core/#deeplens.pipeline.pipeline","title":"<code>pipeline(text, sae_model, sae_config, layer=3, hf_model='gpt2', feature=-1, alpha=5.0, tok_position=-1, generate=False, max_new_tokens=25, temperature=1.0, verbose=False)</code>","text":"<p>End-to-end pipeline for extracting, intervening on, and analyzing SAE features.</p> <p>This function provides a complete workflow for mechanistic interpretability analysis: extracts MLP activations from a language model, decodes them through a sparse autoencoder, intervenes on specific features, and reinjections the modified activations back into the model to observe behavioral changes.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process. Will be tokenized and processed by the language model.</p> required <code>sae_model</code> <code>str</code> <p>Path to the trained sparse autoencoder model weights file (.pt or .pth).</p> required <code>sae_config</code> <code>str</code> <p>Path to the SAE configuration YAML file or dictionary with hyperparameters.</p> required <code>layer</code> <code>int</code> <p>Transformer layer to extract activations from. Supports negative indexing (e.g., -1 for last layer). Defaults to 3.</p> <code>3</code> <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace transformer model to use (e.g., \"gpt2-medium\"). Defaults to \"gpt2\".</p> <code>'gpt2'</code> <code>feature</code> <code>int</code> <p>Index of the SAE feature to intervene on from the alive features list. Supports negative indexing. Defaults to -1 (last alive feature).</p> <code>-1</code> <code>alpha</code> <code>float</code> <p>Intervention strength multiplier. Larger values create stronger feature manipulations. Can be negative to suppress features. Defaults to 5.0.</p> <code>5.0</code> <code>tok_position</code> <code>int</code> <p>Token position to analyze and intervene on. Supports negative indexing (e.g., -1 for last token). Defaults to -1.</p> <code>-1</code> <code>generate</code> <code>bool</code> <p>Whether to generate continuation text after reinjection. If False, returns only the reconstructed input. Defaults to False.</p> <code>False</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of new tokens to generate if generate=True. Must be positive. Defaults to 25.</p> <code>25</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for text generation. Higher values increase randomness. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print diagnostic information during execution. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>A 3-tuple containing: - original: Model output with original unmodified activations - decoded: Model output with SAE-reconstructed activations (no intervention) - modified: Model output with intervened feature activations</p> Source code in <code>deeplens/pipeline.py</code> <pre><code>def pipeline(\n        text: str,\n        sae_model: str,\n        sae_config: str,\n        layer: int = 3,\n        hf_model: str = \"gpt2\",\n        feature: int = -1,\n        alpha: float = 5.0,\n        tok_position: int = -1,\n        generate: bool = False,\n        max_new_tokens: int = 25,\n        temperature: float = 1.0,\n        verbose: bool = False\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"End-to-end pipeline for extracting, intervening on, and analyzing SAE features.\n\n    This function provides a complete workflow for mechanistic interpretability analysis:\n    extracts MLP activations from a language model, decodes them through a sparse autoencoder,\n    intervenes on specific features, and reinjections the modified activations back into the\n    model to observe behavioral changes.\n\n    Args:\n        text (str): Input text to process. Will be tokenized and processed by the language model.\n        sae_model (str): Path to the trained sparse autoencoder model weights file (.pt or .pth).\n        sae_config (str): Path to the SAE configuration YAML file or dictionary with hyperparameters.\n        layer (int, optional): Transformer layer to extract activations from. Supports negative\n            indexing (e.g., -1 for last layer). Defaults to 3.\n        hf_model (str, optional): Name or path of the HuggingFace transformer model to use\n            (e.g., \"gpt2-medium\"). Defaults to \"gpt2\".\n        feature (int, optional): Index of the SAE feature to intervene on from the alive features\n            list. Supports negative indexing. Defaults to -1 (last alive feature).\n        alpha (float, optional): Intervention strength multiplier. Larger values create stronger\n            feature manipulations. Can be negative to suppress features. Defaults to 5.0.\n        tok_position (int, optional): Token position to analyze and intervene on. Supports\n            negative indexing (e.g., -1 for last token). Defaults to -1.\n        generate (bool, optional): Whether to generate continuation text after reinjection.\n            If False, returns only the reconstructed input. Defaults to False.\n        max_new_tokens (int, optional): Maximum number of new tokens to generate if generate=True.\n            Must be positive. Defaults to 25.\n        temperature (float, optional): Sampling temperature for text generation. Higher values\n            increase randomness. Must be positive. Defaults to 1.0.\n        verbose (bool, optional): Whether to print diagnostic information during execution.\n            Defaults to False.\n\n    Returns:\n        A 3-tuple containing:\n            - original: Model output with original unmodified activations\n            - decoded: Model output with SAE-reconstructed activations (no intervention)\n            - modified: Model output with intervened feature activations\n    \"\"\"\n    if not text or not isinstance(text, str):\n        raise ValueError(\"Text must be a non-empty string\")\n\n    if not isinstance(layer, int):\n        raise ValueError(f\"Layer must be an integer, got {layer}\")\n\n    if not isinstance(feature, int):\n        raise ValueError(f\"Feature must be an integer, got {feature}\")\n\n    if not isinstance(alpha, (int, float)):\n        raise ValueError(f\"Alpha must be numeric, got {alpha}\")\n\n    if not isinstance(max_new_tokens, int) or max_new_tokens &lt;= 0:\n        raise ValueError(f\"max_new_tokens must be positive integer, got {max_new_tokens}\")\n\n\n    try:\n        extractor = ExtractSingleSample(hf_model=hf_model, layer=layer)\n        intervene = InterveneFeatures(sae_model=sae_model, sae_config=sae_config)\n        reinject = ReinjectSingleSample(hf_model=hf_model)\n\n        acts = extractor.get_mlp_acts(text)\n        if acts is None:\n            raise RuntimeError(\"Failed to extract activations. Returned 'None'.\")\n\n        alive_features = intervene.get_alive_features(acts, token_position=tok_position)\n        if alive_features is None or len(alive_features) == 0:\n            raise RuntimeError(f\"No alive features found at position {tok_position}\")\n\n        if feature &lt; -len(alive_features) or feature &gt;= len(alive_features):\n            raise ValueError(\n                f\"Feature index {feature} out of bounds for {len(alive_features)} features\"\n            )\n\n        if verbose:\n            print(f\"{len(alive_features)} alive features discovered at position {tok_position}.\")\n            print(f\"Modifying feature {alive_features[feature].item()}\")\n\n        original, decoded, modified = intervene.intervene_feature(\n            activations=acts,\n            feature=alive_features[feature].item(),\n            alpha=alpha, \n            token_positions=tok_position\n        )\n\n        if original is None or decoded is None or modified is None:\n            raise RuntimeError(\"Feature intervention returned 'None'\")\n\n        feature_versions = [original, decoded, modified]\n        outputs = []\n        for i, feature_acts in enumerate(feature_versions):\n            try:\n                out = reinject.reinject_and_generate(\n                    text=text,\n                    modified_activations=feature_acts,\n                    layer=layer,\n                    generate=generate,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature\n                )\n                outputs.append(out)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to reinject feature version {i}: {e}\")      \n\n        return tuple(outputs)\n\n    except ValueError:\n        raise\n    except Exception as e:\n        raise RuntimeError(f\"Pipeline execution failed: {e}\")\n</code></pre>"},{"location":"api/utils/","title":"Utils API","text":""},{"location":"api/utils/#deeplens.utils.analysis","title":"<code>deeplens.utils.analysis</code>","text":""},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils","title":"<code>AnalysisUtils</code>","text":"<p>Unified analysis utilities for model logits and SAE features.</p> <p>This class provides methods for visualizing and analyzing transformer model outputs, including logit heatmaps, token probability distributions, and sparse autoencoder (SAE) feature extraction.</p> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>class AnalysisUtils():\n    \"\"\"Unified analysis utilities for model logits and SAE features.\n\n    This class provides methods for visualizing and analyzing transformer model\n    outputs, including logit heatmaps, token probability distributions, and\n    sparse autoencoder (SAE) feature extraction.\n    \"\"\"\n    def __init__(\n            self,\n            hf_model: str = None,\n            sae_model: str = None,\n            sae_config: str | dict = None,\n            layer: int = None,\n            cache_dir: str = 'cache'\n        ):\n        \"\"\"Initialize the AnalysisUtils class for analyzing sparse autoencoder results.\n\n        Args:\n            hf_model (str, optional): Name or path of the HuggingFace model for extracting\n                MLP activations (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\"). Required for\n                `get_most_active_features`. Defaults to None.\n            sae_model (str, optional): Path to the trained sparse autoencoder model weights\n                file. Required for `get_most_active_features`. Defaults to None.\n            sae_config (str | dict, optional): Path to the YAML configuration file or a\n                dictionary containing SAE hyperparameters. Required for \n                `get_most_active_features`. Defaults to None.\n            layer (int, optional): Index of the transformer layer to extract activations\n                from (0-indexed). Required for `get_most_active_features`. Defaults to None.\n            cache_dir (str, optional): Directory to cache downloaded models.\n                Defaults to 'cache'.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n        self.hf_model = hf_model\n        self.sae_model = sae_model\n        self.sae_config = sae_config\n        self.layer = layer\n\n        self._mlp_extractor = None\n        self._sae_extractor = None\n\n    @property\n    def mlp_extractor(self):\n        if self._mlp_extractor is None:\n            if self.hf_model is None or self.layer is None:\n                raise ValueError(\"hf_model and layer required for MLP extraction\")\n            self._mlp_extractor = ExtractSingleSample(hf_model=self.hf_model, layer=self.layer)\n        return self._mlp_extractor\n\n    @property\n    def sae_extractor(self):\n        if self._sae_extractor is None:\n            if self.sae_model is None or self.sae_config is None:\n                raise ValueError(\"sae_model and sae_config required for SAE extraction\")\n            self._sae_extractor = InterveneFeatures(\n                sae_model=self.sae_model,\n                sae_config=self.sae_config\n            )\n        return self._sae_extractor\n\n    def generate_logit_heatmap(\n            self,\n            logits: torch.Tensor, \n            save_name: str = None,\n            k: int | None = None\n        ) -&gt; None:\n        \"\"\"Generate and display a heatmap visualization of logits across token positions.\n\n        Creates a color-coded heatmap showing the distribution of logit values across the\n        vocabulary for each token position in a sequence. Uses the 'inferno' colormap for\n        visualization.\n\n        Args:\n            logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n                or similar. Will be automatically squeezed and moved to CPU if needed.\n            save_name (str, optional): Filename (without extension) to save the plot.\n                If provided, saves the figure as a PNG file with 300 DPI. If None, only\n                displays the plot. Defaults to None.\n            k (int): top k vocabulary indexes to plot. Defaults to None.\n\n        Returns:\n            None: Displays the plot and optionally saves it to disk.\n\n        Note:\n            The figure size is set to (20, 6) for optimal visualization of typical\n            sequence lengths and vocabulary sizes.\n        \"\"\"\n        if isinstance(logits, torch.Tensor):\n            logits = logits.squeeze().detach().cpu().numpy()\n\n        if k is not None:\n            if isinstance(logits, np.ndarray):\n                logits = torch.from_numpy(logits)\n            logits = torch.topk(logits, k=k, dim=-1).values.numpy()\n\n        plt.figure(figsize=(20, 6))\n        plt.imshow(logits, cmap=\"inferno\", aspect=\"auto\")\n        plt.colorbar()\n        plt.xlabel(\"Vocabulary Index\", size=18)\n        plt.ylabel(\"Token Position\", size=18)\n        plt.xticks(size=12)\n        plt.yticks(size=12)\n        if save_name is not None:\n            plt.savefig(f\"{save_name}.png\", dpi=300, bbox_inches=\"tight\")\n        plt.show()\n\n    def plot_topk_distribution(\n            self,\n            logits: torch.Tensor, \n            k: int = 20, \n            position: int = 0, \n            save_name: str = None,\n            use_softmax: bool = True,\n            title: str = None\n        ) -&gt; None:\n        \"\"\"Plot a bar chart showing the top-k most probable tokens at a specific sequence position.\n\n        Creates a horizontal bar chart displaying the highest-probability token predictions\n        at a given position in the sequence. Useful for analyzing model predictions and\n        understanding which tokens are most likely at specific positions.\n\n        Args:\n            logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n                or similar. Will be automatically squeezed and moved to CPU if needed.\n            k (int, optional): Number of top predictions to display in the bar chart.\n                Defaults to 20.\n            position (int, optional): Token position in the sequence to analyze. Use 0-based\n                indexing. Defaults to 0 (first token).\n            save_name (str, optional): Filename (without extension) to save the plot.\n                If provided, saves the figure as a PNG file with 300 DPI. If None, only\n                displays the plot. Defaults to None.\n            use_softmax (bool, optional): If True, applies softmax to convert logits to\n                probabilities before plotting. If False, plots raw logit values.\n                Defaults to True.\n            title (str, optional): Custom title for the plot. If None, no title is displayed.\n                Defaults to None.\n\n        Returns:\n            None: Displays the plot and optionally saves it to disk.\n\n        Note:\n            Token labels are rotated 45 degrees for readability. The y-axis label changes\n            based on use_softmax: shows probability notation if True, \"Logit Value\" if False.\n        \"\"\"\n        if isinstance(logits, torch.Tensor):\n            logits = logits.squeeze().detach().cpu().numpy()\n\n        pos_logits = logits[position]\n\n        if use_softmax:\n            exp_logits = np.exp(pos_logits - np.max(pos_logits))\n            pos_logits = exp_logits / np.sum(exp_logits)\n\n        top_idx = np.argsort(pos_logits)[-k:][::-1]\n        top_vals = pos_logits[top_idx]\n        top_tokens = [self.tokenizer.decode([idx]) for idx in top_idx]\n\n        plt.figure(figsize=(12, 6))\n        plt.bar(range(k), top_vals)\n        plt.xticks(range(k), top_tokens, rotation=45, ha='right', fontsize=10)\n        plt.xlabel('Token', fontsize=14)\n        ylabel = r'$P$(token)' if use_softmax else 'Logit Value'\n        plt.ylabel(ylabel, fontsize=14)\n        plt.title(title, fontsize=16)\n        plt.tight_layout()\n        if save_name is not None:\n            plt.savefig(f\"{save_name}.png\", dpi=300, bbox_inches=\"tight\")\n        plt.show()\n\n    def get_top_k_tokens(\n            self,\n            logits: torch.Tensor, \n            k: int = 10, \n            to_dataframe: bool = True,\n            verbose: bool = False\n        ) -&gt; pd.DataFrame | list[dict]:\n        \"\"\"Get the top-k predicted tokens and their probabilities for each position in a sequence.\n\n        Iterates through all positions in the sequence and extracts the k highest-scoring tokens\n        at each position. Useful for detailed analysis of model predictions across the entire\n        sequence.\n\n        Args:\n            logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n                or similar. Will be automatically squeezed and moved to CPU if needed.\n            k (int, optional): Number of top predictions to display per position.\n                Defaults to 10.\n            to_dataframe (bool, optional): If True, returns the top-k predicted tokens at each \n                position and their probabilities in a pandas DataFrame. Defaults to True.\n            verbose (bool, optional): If True, prints the results to the console. Defaults\n                to False. \n\n        Returns:\n            pd.DataFrame | list[dict]: If to_dataframe=True, returns a DataFrame with columns\n                'position', 'rank', 'token', and 'probability'. Otherwise returns a list of\n                dictionaries with the same fields.\n\n        Example Output:\n            Top-10 predicted tokens per position\n\n            Position 0:\n                Token ('the'): 12.45\n                Token ('a'): 11.32\n                ...\n        \"\"\"\n        if isinstance(logits, torch.Tensor):\n            logits = logits.squeeze().detach().cpu().numpy()\n        if verbose:\n            print(f\"Top-{k} predicted tokens per position\")\n\n        exp_logits = np.exp(logits - np.max(logits))\n        logits = exp_logits / np.sum(exp_logits)\n\n        results = {}\n        for pos in range(logits.shape[0]):\n            top_idx = np.argsort(logits[pos])[-k:][::-1]\n            top_vals = logits[pos][top_idx]\n            if verbose:\n                print(f'\\nPosition {pos}:')\n            tokens = []\n            for idx, val in zip(top_idx, top_vals):\n                if self.tokenizer is not None:\n                    token = self.tokenizer.decode([idx])\n                    tokens.append(token)\n                    if verbose:\n                        print(f\"\\t'{token}': {val:.2f}\")\n                else:\n                    if verbose:\n                        print(f\"\\tToken {idx}: {val:.2f}\")\n\n            results[pos] = {\n                'tokens': tokens,\n                'values': top_vals.tolist()\n            }\n\n        out = []\n        for position, data in results.items():\n            for i, (token, prob) in enumerate(zip(data['tokens'], data['values'])):\n                out.append({\n                    'position': position,\n                    'rank': i + 1,\n                    'token': token,\n                    'probability': prob\n                })\n\n        if to_dataframe:\n            return pd.DataFrame(out)\n        else:\n            return out\n\n    def get_most_active_features(\n            self,\n            sentences: list[str], \n            target: int | str | None = None,\n            k: int | None = None,\n            case_sensitive: bool = True,\n            return_values: bool = False\n        ) -&gt; dict[str, FeatureResult]:\n        \"\"\"Extract SAE latent features for specific tokens across multiple sentences.\n\n        This method processes a list of sentences through a transformer model and sparse\n        autoencoder to extract active features at specified token positions. Useful for\n        analyzing which SAE features activate for particular tokens or syntactic patterns.\n\n        Note:\n            Requires `hf_model`, `layer`, `sae_model`, and `sae_config` to be set during\n            class initialization.\n\n        Args:\n            sentences (list[str]): List of input sentences to process. Each sentence will\n                be tokenized and processed independently.\n            target (int | str | None, optional): Specifies which token positions to analyze.\n                - If int: Token position index (supports negative indexing, e.g., -1 for last token).\n                - If str: Token string to match (e.g., \"What\"). Will find all occurrences.\n                - If None: Extracts features for all token positions.\n                Defaults to None.\n            k (int | None, optional): If provided, returns only the top-k most active features\n                by activation magnitude. If None, returns all non-zero features. Defaults to None.\n            case_sensitive (bool, optional): Whether string matching for target tokens should\n                be case-sensitive. Only applies when target is a string. Defaults to True.\n            return_values (bool, optional): Whether the function will return features and activation\n                values of each feature. Defaults to False.\n\n        Returns:\n            dict[str, FeatureResult]: Dictionary mapping descriptive keys to FeatureResult objects.\n                Keys follow the format \"sent_{idx}_pos_{pos}_tok_{token}\" where:\n                - idx: 1-indexed sentence number\n                - pos: 0-indexed token position within the sentence\n                - token: The decoded token string (stripped of whitespace)\n                Values are FeatureResult dataclass instances with:\n                - features: Tensor containing indices of active/top-k features\n                - values: Tensor of activation values (or None if return_values=False)\n        \"\"\"\n        features = {}\n        for idx, sent in enumerate(sentences):\n            acts = self.mlp_extractor.get_mlp_acts(sample=sent)\n            input_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n            num_tokens = acts.shape[0]\n\n            if target is None:\n                positions = list(range(num_tokens))\n            elif isinstance(target, int):\n                pos = target if target &gt;= 0 else num_tokens + target\n                if 0 &lt;= pos &lt; num_tokens:\n                    positions = [pos]\n                else:\n                    raise ValueError(\n                        f\"Position {target} out of range for sentence {idx+1} ({num_tokens} tokens)\"\n                    )\n            elif isinstance(target, str):\n                target_to_match = target if case_sensitive else target.lower()\n                positions = []\n                for i, tok_id in enumerate(input_ids):\n                    decoded = self.tokenizer.decode([tok_id])\n                    decoded_to_match = decoded if case_sensitive else decoded.lower()\n                    if decoded_to_match.strip() == target_to_match.strip():\n                        positions.append(i)\n                if not positions:\n                    print(f\"Warning: '{target}' not found in sentence '{sent}'\")\n                    continue\n            else:\n                raise TypeError(f\"target must be int, str, or None, got {type(target)}\")\n\n            for pos in positions:\n                if pos &gt;= num_tokens:\n                    continue\n\n                vals = None\n                if return_values:\n                    feats, vals = self.sae_extractor.get_alive_features(\n                        activations=acts, \n                        token_position=pos, \n                        k=k,\n                        return_values=True\n                    )\n                else:\n                    feats = self.sae_extractor.get_alive_features(\n                        activations=acts, \n                        token_position=pos, \n                        k=k,\n                        return_values=False\n                    )\n                token_str = self.tokenizer.decode([input_ids[pos]])\n                key = f\"sent_{idx+1}_pos_{pos}_tok_{token_str.strip()}\"\n                features[key] = FeatureResult(\n                    feats.cpu(), \n                    vals.cpu() if vals is not None else None\n                )\n        return features\n</code></pre>"},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils.__init__","title":"<code>__init__(hf_model=None, sae_model=None, sae_config=None, layer=None, cache_dir='cache')</code>","text":"<p>Initialize the AnalysisUtils class for analyzing sparse autoencoder results.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <code>str</code> <p>Name or path of the HuggingFace model for extracting MLP activations (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\"). Required for <code>get_most_active_features</code>. Defaults to None.</p> <code>None</code> <code>sae_model</code> <code>str</code> <p>Path to the trained sparse autoencoder model weights file. Required for <code>get_most_active_features</code>. Defaults to None.</p> <code>None</code> <code>sae_config</code> <code>str | dict</code> <p>Path to the YAML configuration file or a dictionary containing SAE hyperparameters. Required for  <code>get_most_active_features</code>. Defaults to None.</p> <code>None</code> <code>layer</code> <code>int</code> <p>Index of the transformer layer to extract activations from (0-indexed). Required for <code>get_most_active_features</code>. Defaults to None.</p> <code>None</code> <code>cache_dir</code> <code>str</code> <p>Directory to cache downloaded models. Defaults to 'cache'.</p> <code>'cache'</code> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>def __init__(\n        self,\n        hf_model: str = None,\n        sae_model: str = None,\n        sae_config: str | dict = None,\n        layer: int = None,\n        cache_dir: str = 'cache'\n    ):\n    \"\"\"Initialize the AnalysisUtils class for analyzing sparse autoencoder results.\n\n    Args:\n        hf_model (str, optional): Name or path of the HuggingFace model for extracting\n            MLP activations (e.g., \"gpt2\", \"meta-llama/Llama-2-7b\"). Required for\n            `get_most_active_features`. Defaults to None.\n        sae_model (str, optional): Path to the trained sparse autoencoder model weights\n            file. Required for `get_most_active_features`. Defaults to None.\n        sae_config (str | dict, optional): Path to the YAML configuration file or a\n            dictionary containing SAE hyperparameters. Required for \n            `get_most_active_features`. Defaults to None.\n        layer (int, optional): Index of the transformer layer to extract activations\n            from (0-indexed). Required for `get_most_active_features`. Defaults to None.\n        cache_dir (str, optional): Directory to cache downloaded models.\n            Defaults to 'cache'.\n    \"\"\"\n    self.tokenizer = AutoTokenizer.from_pretrained(hf_model, cache_dir=cache_dir)\n    self.hf_model = hf_model\n    self.sae_model = sae_model\n    self.sae_config = sae_config\n    self.layer = layer\n\n    self._mlp_extractor = None\n    self._sae_extractor = None\n</code></pre>"},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils.generate_logit_heatmap","title":"<code>generate_logit_heatmap(logits, save_name=None, k=None)</code>","text":"<p>Generate and display a heatmap visualization of logits across token positions.</p> <p>Creates a color-coded heatmap showing the distribution of logit values across the vocabulary for each token position in a sequence. Uses the 'inferno' colormap for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits tensor with shape (sequence_length, vocab_size) or similar. Will be automatically squeezed and moved to CPU if needed.</p> required <code>save_name</code> <code>str</code> <p>Filename (without extension) to save the plot. If provided, saves the figure as a PNG file with 300 DPI. If None, only displays the plot. Defaults to None.</p> <code>None</code> <code>k</code> <code>int</code> <p>top k vocabulary indexes to plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the plot and optionally saves it to disk.</p> Note <p>The figure size is set to (20, 6) for optimal visualization of typical sequence lengths and vocabulary sizes.</p> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>def generate_logit_heatmap(\n        self,\n        logits: torch.Tensor, \n        save_name: str = None,\n        k: int | None = None\n    ) -&gt; None:\n    \"\"\"Generate and display a heatmap visualization of logits across token positions.\n\n    Creates a color-coded heatmap showing the distribution of logit values across the\n    vocabulary for each token position in a sequence. Uses the 'inferno' colormap for\n    visualization.\n\n    Args:\n        logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n            or similar. Will be automatically squeezed and moved to CPU if needed.\n        save_name (str, optional): Filename (without extension) to save the plot.\n            If provided, saves the figure as a PNG file with 300 DPI. If None, only\n            displays the plot. Defaults to None.\n        k (int): top k vocabulary indexes to plot. Defaults to None.\n\n    Returns:\n        None: Displays the plot and optionally saves it to disk.\n\n    Note:\n        The figure size is set to (20, 6) for optimal visualization of typical\n        sequence lengths and vocabulary sizes.\n    \"\"\"\n    if isinstance(logits, torch.Tensor):\n        logits = logits.squeeze().detach().cpu().numpy()\n\n    if k is not None:\n        if isinstance(logits, np.ndarray):\n            logits = torch.from_numpy(logits)\n        logits = torch.topk(logits, k=k, dim=-1).values.numpy()\n\n    plt.figure(figsize=(20, 6))\n    plt.imshow(logits, cmap=\"inferno\", aspect=\"auto\")\n    plt.colorbar()\n    plt.xlabel(\"Vocabulary Index\", size=18)\n    plt.ylabel(\"Token Position\", size=18)\n    plt.xticks(size=12)\n    plt.yticks(size=12)\n    if save_name is not None:\n        plt.savefig(f\"{save_name}.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils.get_most_active_features","title":"<code>get_most_active_features(sentences, target=None, k=None, case_sensitive=True, return_values=False)</code>","text":"<p>Extract SAE latent features for specific tokens across multiple sentences.</p> <p>This method processes a list of sentences through a transformer model and sparse autoencoder to extract active features at specified token positions. Useful for analyzing which SAE features activate for particular tokens or syntactic patterns.</p> Note <p>Requires <code>hf_model</code>, <code>layer</code>, <code>sae_model</code>, and <code>sae_config</code> to be set during class initialization.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>list[str]</code> <p>List of input sentences to process. Each sentence will be tokenized and processed independently.</p> required <code>target</code> <code>int | str | None</code> <p>Specifies which token positions to analyze. - If int: Token position index (supports negative indexing, e.g., -1 for last token). - If str: Token string to match (e.g., \"What\"). Will find all occurrences. - If None: Extracts features for all token positions. Defaults to None.</p> <code>None</code> <code>k</code> <code>int | None</code> <p>If provided, returns only the top-k most active features by activation magnitude. If None, returns all non-zero features. Defaults to None.</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether string matching for target tokens should be case-sensitive. Only applies when target is a string. Defaults to True.</p> <code>True</code> <code>return_values</code> <code>bool</code> <p>Whether the function will return features and activation values of each feature. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, FeatureResult]</code> <p>dict[str, FeatureResult]: Dictionary mapping descriptive keys to FeatureResult objects. Keys follow the format \"sent_{idx}postok\" where: - idx: 1-indexed sentence number - pos: 0-indexed token position within the sentence - token: The decoded token string (stripped of whitespace) Values are FeatureResult dataclass instances with: - features: Tensor containing indices of active/top-k features - values: Tensor of activation values (or None if return_values=False)</p> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>def get_most_active_features(\n        self,\n        sentences: list[str], \n        target: int | str | None = None,\n        k: int | None = None,\n        case_sensitive: bool = True,\n        return_values: bool = False\n    ) -&gt; dict[str, FeatureResult]:\n    \"\"\"Extract SAE latent features for specific tokens across multiple sentences.\n\n    This method processes a list of sentences through a transformer model and sparse\n    autoencoder to extract active features at specified token positions. Useful for\n    analyzing which SAE features activate for particular tokens or syntactic patterns.\n\n    Note:\n        Requires `hf_model`, `layer`, `sae_model`, and `sae_config` to be set during\n        class initialization.\n\n    Args:\n        sentences (list[str]): List of input sentences to process. Each sentence will\n            be tokenized and processed independently.\n        target (int | str | None, optional): Specifies which token positions to analyze.\n            - If int: Token position index (supports negative indexing, e.g., -1 for last token).\n            - If str: Token string to match (e.g., \"What\"). Will find all occurrences.\n            - If None: Extracts features for all token positions.\n            Defaults to None.\n        k (int | None, optional): If provided, returns only the top-k most active features\n            by activation magnitude. If None, returns all non-zero features. Defaults to None.\n        case_sensitive (bool, optional): Whether string matching for target tokens should\n            be case-sensitive. Only applies when target is a string. Defaults to True.\n        return_values (bool, optional): Whether the function will return features and activation\n            values of each feature. Defaults to False.\n\n    Returns:\n        dict[str, FeatureResult]: Dictionary mapping descriptive keys to FeatureResult objects.\n            Keys follow the format \"sent_{idx}_pos_{pos}_tok_{token}\" where:\n            - idx: 1-indexed sentence number\n            - pos: 0-indexed token position within the sentence\n            - token: The decoded token string (stripped of whitespace)\n            Values are FeatureResult dataclass instances with:\n            - features: Tensor containing indices of active/top-k features\n            - values: Tensor of activation values (or None if return_values=False)\n    \"\"\"\n    features = {}\n    for idx, sent in enumerate(sentences):\n        acts = self.mlp_extractor.get_mlp_acts(sample=sent)\n        input_ids = self.tokenizer.encode(sent, add_special_tokens=False)\n        num_tokens = acts.shape[0]\n\n        if target is None:\n            positions = list(range(num_tokens))\n        elif isinstance(target, int):\n            pos = target if target &gt;= 0 else num_tokens + target\n            if 0 &lt;= pos &lt; num_tokens:\n                positions = [pos]\n            else:\n                raise ValueError(\n                    f\"Position {target} out of range for sentence {idx+1} ({num_tokens} tokens)\"\n                )\n        elif isinstance(target, str):\n            target_to_match = target if case_sensitive else target.lower()\n            positions = []\n            for i, tok_id in enumerate(input_ids):\n                decoded = self.tokenizer.decode([tok_id])\n                decoded_to_match = decoded if case_sensitive else decoded.lower()\n                if decoded_to_match.strip() == target_to_match.strip():\n                    positions.append(i)\n            if not positions:\n                print(f\"Warning: '{target}' not found in sentence '{sent}'\")\n                continue\n        else:\n            raise TypeError(f\"target must be int, str, or None, got {type(target)}\")\n\n        for pos in positions:\n            if pos &gt;= num_tokens:\n                continue\n\n            vals = None\n            if return_values:\n                feats, vals = self.sae_extractor.get_alive_features(\n                    activations=acts, \n                    token_position=pos, \n                    k=k,\n                    return_values=True\n                )\n            else:\n                feats = self.sae_extractor.get_alive_features(\n                    activations=acts, \n                    token_position=pos, \n                    k=k,\n                    return_values=False\n                )\n            token_str = self.tokenizer.decode([input_ids[pos]])\n            key = f\"sent_{idx+1}_pos_{pos}_tok_{token_str.strip()}\"\n            features[key] = FeatureResult(\n                feats.cpu(), \n                vals.cpu() if vals is not None else None\n            )\n    return features\n</code></pre>"},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils.get_top_k_tokens","title":"<code>get_top_k_tokens(logits, k=10, to_dataframe=True, verbose=False)</code>","text":"<p>Get the top-k predicted tokens and their probabilities for each position in a sequence.</p> <p>Iterates through all positions in the sequence and extracts the k highest-scoring tokens at each position. Useful for detailed analysis of model predictions across the entire sequence.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits tensor with shape (sequence_length, vocab_size) or similar. Will be automatically squeezed and moved to CPU if needed.</p> required <code>k</code> <code>int</code> <p>Number of top predictions to display per position. Defaults to 10.</p> <code>10</code> <code>to_dataframe</code> <code>bool</code> <p>If True, returns the top-k predicted tokens at each  position and their probabilities in a pandas DataFrame. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints the results to the console. Defaults to False. </p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | list[dict]</code> <p>pd.DataFrame | list[dict]: If to_dataframe=True, returns a DataFrame with columns 'position', 'rank', 'token', and 'probability'. Otherwise returns a list of dictionaries with the same fields.</p> Example Output <p>Top-10 predicted tokens per position</p> <p>Position 0:     Token ('the'): 12.45     Token ('a'): 11.32     ...</p> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>def get_top_k_tokens(\n        self,\n        logits: torch.Tensor, \n        k: int = 10, \n        to_dataframe: bool = True,\n        verbose: bool = False\n    ) -&gt; pd.DataFrame | list[dict]:\n    \"\"\"Get the top-k predicted tokens and their probabilities for each position in a sequence.\n\n    Iterates through all positions in the sequence and extracts the k highest-scoring tokens\n    at each position. Useful for detailed analysis of model predictions across the entire\n    sequence.\n\n    Args:\n        logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n            or similar. Will be automatically squeezed and moved to CPU if needed.\n        k (int, optional): Number of top predictions to display per position.\n            Defaults to 10.\n        to_dataframe (bool, optional): If True, returns the top-k predicted tokens at each \n            position and their probabilities in a pandas DataFrame. Defaults to True.\n        verbose (bool, optional): If True, prints the results to the console. Defaults\n            to False. \n\n    Returns:\n        pd.DataFrame | list[dict]: If to_dataframe=True, returns a DataFrame with columns\n            'position', 'rank', 'token', and 'probability'. Otherwise returns a list of\n            dictionaries with the same fields.\n\n    Example Output:\n        Top-10 predicted tokens per position\n\n        Position 0:\n            Token ('the'): 12.45\n            Token ('a'): 11.32\n            ...\n    \"\"\"\n    if isinstance(logits, torch.Tensor):\n        logits = logits.squeeze().detach().cpu().numpy()\n    if verbose:\n        print(f\"Top-{k} predicted tokens per position\")\n\n    exp_logits = np.exp(logits - np.max(logits))\n    logits = exp_logits / np.sum(exp_logits)\n\n    results = {}\n    for pos in range(logits.shape[0]):\n        top_idx = np.argsort(logits[pos])[-k:][::-1]\n        top_vals = logits[pos][top_idx]\n        if verbose:\n            print(f'\\nPosition {pos}:')\n        tokens = []\n        for idx, val in zip(top_idx, top_vals):\n            if self.tokenizer is not None:\n                token = self.tokenizer.decode([idx])\n                tokens.append(token)\n                if verbose:\n                    print(f\"\\t'{token}': {val:.2f}\")\n            else:\n                if verbose:\n                    print(f\"\\tToken {idx}: {val:.2f}\")\n\n        results[pos] = {\n            'tokens': tokens,\n            'values': top_vals.tolist()\n        }\n\n    out = []\n    for position, data in results.items():\n        for i, (token, prob) in enumerate(zip(data['tokens'], data['values'])):\n            out.append({\n                'position': position,\n                'rank': i + 1,\n                'token': token,\n                'probability': prob\n            })\n\n    if to_dataframe:\n        return pd.DataFrame(out)\n    else:\n        return out\n</code></pre>"},{"location":"api/utils/#deeplens.utils.analysis.AnalysisUtils.plot_topk_distribution","title":"<code>plot_topk_distribution(logits, k=20, position=0, save_name=None, use_softmax=True, title=None)</code>","text":"<p>Plot a bar chart showing the top-k most probable tokens at a specific sequence position.</p> <p>Creates a horizontal bar chart displaying the highest-probability token predictions at a given position in the sequence. Useful for analyzing model predictions and understanding which tokens are most likely at specific positions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits tensor with shape (sequence_length, vocab_size) or similar. Will be automatically squeezed and moved to CPU if needed.</p> required <code>k</code> <code>int</code> <p>Number of top predictions to display in the bar chart. Defaults to 20.</p> <code>20</code> <code>position</code> <code>int</code> <p>Token position in the sequence to analyze. Use 0-based indexing. Defaults to 0 (first token).</p> <code>0</code> <code>save_name</code> <code>str</code> <p>Filename (without extension) to save the plot. If provided, saves the figure as a PNG file with 300 DPI. If None, only displays the plot. Defaults to None.</p> <code>None</code> <code>use_softmax</code> <code>bool</code> <p>If True, applies softmax to convert logits to probabilities before plotting. If False, plots raw logit values. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>Custom title for the plot. If None, no title is displayed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the plot and optionally saves it to disk.</p> Note <p>Token labels are rotated 45 degrees for readability. The y-axis label changes based on use_softmax: shows probability notation if True, \"Logit Value\" if False.</p> Source code in <code>deeplens/utils/analysis.py</code> <pre><code>def plot_topk_distribution(\n        self,\n        logits: torch.Tensor, \n        k: int = 20, \n        position: int = 0, \n        save_name: str = None,\n        use_softmax: bool = True,\n        title: str = None\n    ) -&gt; None:\n    \"\"\"Plot a bar chart showing the top-k most probable tokens at a specific sequence position.\n\n    Creates a horizontal bar chart displaying the highest-probability token predictions\n    at a given position in the sequence. Useful for analyzing model predictions and\n    understanding which tokens are most likely at specific positions.\n\n    Args:\n        logits (torch.Tensor): Logits tensor with shape (sequence_length, vocab_size)\n            or similar. Will be automatically squeezed and moved to CPU if needed.\n        k (int, optional): Number of top predictions to display in the bar chart.\n            Defaults to 20.\n        position (int, optional): Token position in the sequence to analyze. Use 0-based\n            indexing. Defaults to 0 (first token).\n        save_name (str, optional): Filename (without extension) to save the plot.\n            If provided, saves the figure as a PNG file with 300 DPI. If None, only\n            displays the plot. Defaults to None.\n        use_softmax (bool, optional): If True, applies softmax to convert logits to\n            probabilities before plotting. If False, plots raw logit values.\n            Defaults to True.\n        title (str, optional): Custom title for the plot. If None, no title is displayed.\n            Defaults to None.\n\n    Returns:\n        None: Displays the plot and optionally saves it to disk.\n\n    Note:\n        Token labels are rotated 45 degrees for readability. The y-axis label changes\n        based on use_softmax: shows probability notation if True, \"Logit Value\" if False.\n    \"\"\"\n    if isinstance(logits, torch.Tensor):\n        logits = logits.squeeze().detach().cpu().numpy()\n\n    pos_logits = logits[position]\n\n    if use_softmax:\n        exp_logits = np.exp(pos_logits - np.max(pos_logits))\n        pos_logits = exp_logits / np.sum(exp_logits)\n\n    top_idx = np.argsort(pos_logits)[-k:][::-1]\n    top_vals = pos_logits[top_idx]\n    top_tokens = [self.tokenizer.decode([idx]) for idx in top_idx]\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(k), top_vals)\n    plt.xticks(range(k), top_tokens, rotation=45, ha='right', fontsize=10)\n    plt.xlabel('Token', fontsize=14)\n    ylabel = r'$P$(token)' if use_softmax else 'Logit Value'\n    plt.ylabel(ylabel, fontsize=14)\n    plt.title(title, fontsize=16)\n    plt.tight_layout()\n    if save_name is not None:\n        plt.savefig(f\"{save_name}.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset","title":"<code>deeplens.utils.dataset</code>","text":""},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDataset","title":"<code>ActivationsDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Lightweight PyTorch Dataset wrapper for pre-computed activation tensors.</p> <p>Provides a simple Dataset interface for tensors of neural network activations, enabling use with PyTorch DataLoader for batching and iteration.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>class ActivationsDataset(Dataset):\n    \"\"\"Lightweight PyTorch Dataset wrapper for pre-computed activation tensors.\n\n    Provides a simple Dataset interface for tensors of neural network activations,\n    enabling use with PyTorch DataLoader for batching and iteration.\n    \"\"\"\n    def __init__(self, activations: torch.Tensor):\n        \"\"\"Initialize the dataset with activation tensors.\n\n        Args:\n            activations (torch.Tensor): Tensor containing pre-computed activations\n                with shape (num_samples, feature_dim).\n        \"\"\"\n        super().__init__()\n        self.activations = activations\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of activation samples.\n\n        Returns:\n            int: Number of samples in the dataset.\n        \"\"\"\n        return len(self.activations)\n\n    def __getitem__(self, idx) -&gt; torch.Tensor:\n        \"\"\"Retrieve activation tensor at the specified index.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            torch.Tensor: Activation tensor at the given index.\n        \"\"\"\n        return self.activations[idx]\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieve activation tensor at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Activation tensor at the given index.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __getitem__(self, idx) -&gt; torch.Tensor:\n    \"\"\"Retrieve activation tensor at the specified index.\n\n    Args:\n        idx (int): Index of the sample to retrieve.\n\n    Returns:\n        torch.Tensor: Activation tensor at the given index.\n    \"\"\"\n    return self.activations[idx]\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDataset.__init__","title":"<code>__init__(activations)</code>","text":"<p>Initialize the dataset with activation tensors.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor</code> <p>Tensor containing pre-computed activations with shape (num_samples, feature_dim).</p> required Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __init__(self, activations: torch.Tensor):\n    \"\"\"Initialize the dataset with activation tensors.\n\n    Args:\n        activations (torch.Tensor): Tensor containing pre-computed activations\n            with shape (num_samples, feature_dim).\n    \"\"\"\n    super().__init__()\n    self.activations = activations\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of activation samples.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of samples in the dataset.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of activation samples.\n\n    Returns:\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.activations)\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDatasetBuilder","title":"<code>ActivationsDatasetBuilder</code>","text":"<p>Builder class for creating DataLoaders from saved activation tensors.</p> <p>Loads pre-computed activations from disk, applies optional normalization, and creates train/validation DataLoaders for training sparse autoencoders or other downstream models.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>class ActivationsDatasetBuilder():\n    \"\"\"Builder class for creating DataLoaders from saved activation tensors.\n\n    Loads pre-computed activations from disk, applies optional normalization, and creates\n    train/validation DataLoaders for training sparse autoencoders or other downstream models.\n    \"\"\"\n    def __init__(\n            self, \n            activations: torch.Tensor = None, \n            splits: list = [0.8, 0.2],\n            batch_size: int = 16,\n            norm: bool = True\n        ):\n        \"\"\"Initialize the builder and load activations from disk.\n\n        Args:\n            activations (torch.Tensor | str, optional): Path to a .pt file containing\n                saved activation tensors, or a tensor directly. Defaults to None.\n            splits (list, optional): List of two floats representing train and validation\n                split proportions. Must sum to 1.0. Defaults to [0.8, 0.2].\n            batch_size (int, optional): Number of samples per batch for DataLoaders.\n                Defaults to 16.\n            norm (bool, optional): Whether to apply z-score normalization (standardization)\n                to the activations. Defaults to True.\n        \"\"\"\n        self.activations = torch.load(activations, weights_only=True)\n        self.splits = splits\n        self.batch_size = batch_size\n        self.norm = norm\n        self.normalize()\n\n    def set_tensor_dataset(self) -&gt; Dataset:\n        \"\"\"Create a PyTorch Dataset from the loaded activations.\n\n        Returns:\n            Dataset: ActivationsDataset instance wrapping the activation tensors.\n        \"\"\"\n        return ActivationsDataset(self.activations)\n\n    def get_dataloaders(self, ddp: bool = False) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"Create train and validation DataLoaders from the activations.\n\n        Splits the dataset according to the specified proportions and creates two DataLoaders\n        with appropriate settings for training and evaluation.\n\n        Args:\n            ddp (bool, optional): Turn to True if Distributed Data Parallel (DDP) training is\n                intended. Defaults to False.\n\n        Returns:\n            tuple: A tuple containing (train_loader, eval_loader).\n                Training loader has shuffle=True for randomized batching, evaluation loader\n                has shuffle=False for consistent evaluation.\n        \"\"\"\n        data = self.set_tensor_dataset()\n        train, eval = random_split(data, lengths=self.splits)\n        train_loader = DataLoader(\n            train, \n            batch_size=self.batch_size, \n            shuffle=not ddp, \n            pin_memory=True,\n            sampler=DistributedSampler(train, shuffle=True) if ddp else None\n        )\n        eval_loader = DataLoader(\n            eval, \n            batch_size=self.batch_size, \n            shuffle=False, \n            pin_memory=True, \n            sampler=DistributedSampler(eval, shuffle=False) if ddp else None\n        )\n        return train_loader, eval_loader\n\n    @torch.no_grad()\n    def normalize(self) -&gt; None:\n        \"\"\"Apply z-score normalization to the activations in-place.\n\n        Standardizes the activations by subtracting the mean and dividing by the standard\n        deviation (computed along the batch dimension). Adds small epsilon (1e-8) to prevent\n        division by zero. Only applies if norm=True was set during initialization.\n\n        Returns:\n            None: Modifies self.activations in-place.\n        \"\"\"\n        if self.norm:\n            mean = self.activations.mean(dim=0, keepdim=True)\n            std = self.activations.std(dim=0, keepdim=True)\n            self.activations = (self.activations - mean) / (std + 1e-8)\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDatasetBuilder.__init__","title":"<code>__init__(activations=None, splits=[0.8, 0.2], batch_size=16, norm=True)</code>","text":"<p>Initialize the builder and load activations from disk.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Tensor | str</code> <p>Path to a .pt file containing saved activation tensors, or a tensor directly. Defaults to None.</p> <code>None</code> <code>splits</code> <code>list</code> <p>List of two floats representing train and validation split proportions. Must sum to 1.0. Defaults to [0.8, 0.2].</p> <code>[0.8, 0.2]</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch for DataLoaders. Defaults to 16.</p> <code>16</code> <code>norm</code> <code>bool</code> <p>Whether to apply z-score normalization (standardization) to the activations. Defaults to True.</p> <code>True</code> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __init__(\n        self, \n        activations: torch.Tensor = None, \n        splits: list = [0.8, 0.2],\n        batch_size: int = 16,\n        norm: bool = True\n    ):\n    \"\"\"Initialize the builder and load activations from disk.\n\n    Args:\n        activations (torch.Tensor | str, optional): Path to a .pt file containing\n            saved activation tensors, or a tensor directly. Defaults to None.\n        splits (list, optional): List of two floats representing train and validation\n            split proportions. Must sum to 1.0. Defaults to [0.8, 0.2].\n        batch_size (int, optional): Number of samples per batch for DataLoaders.\n            Defaults to 16.\n        norm (bool, optional): Whether to apply z-score normalization (standardization)\n            to the activations. Defaults to True.\n    \"\"\"\n    self.activations = torch.load(activations, weights_only=True)\n    self.splits = splits\n    self.batch_size = batch_size\n    self.norm = norm\n    self.normalize()\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDatasetBuilder.get_dataloaders","title":"<code>get_dataloaders(ddp=False)</code>","text":"<p>Create train and validation DataLoaders from the activations.</p> <p>Splits the dataset according to the specified proportions and creates two DataLoaders with appropriate settings for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>ddp</code> <code>bool</code> <p>Turn to True if Distributed Data Parallel (DDP) training is intended. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[DataLoader, DataLoader]</code> <p>A tuple containing (train_loader, eval_loader). Training loader has shuffle=True for randomized batching, evaluation loader has shuffle=False for consistent evaluation.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def get_dataloaders(self, ddp: bool = False) -&gt; tuple[DataLoader, DataLoader]:\n    \"\"\"Create train and validation DataLoaders from the activations.\n\n    Splits the dataset according to the specified proportions and creates two DataLoaders\n    with appropriate settings for training and evaluation.\n\n    Args:\n        ddp (bool, optional): Turn to True if Distributed Data Parallel (DDP) training is\n            intended. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing (train_loader, eval_loader).\n            Training loader has shuffle=True for randomized batching, evaluation loader\n            has shuffle=False for consistent evaluation.\n    \"\"\"\n    data = self.set_tensor_dataset()\n    train, eval = random_split(data, lengths=self.splits)\n    train_loader = DataLoader(\n        train, \n        batch_size=self.batch_size, \n        shuffle=not ddp, \n        pin_memory=True,\n        sampler=DistributedSampler(train, shuffle=True) if ddp else None\n    )\n    eval_loader = DataLoader(\n        eval, \n        batch_size=self.batch_size, \n        shuffle=False, \n        pin_memory=True, \n        sampler=DistributedSampler(eval, shuffle=False) if ddp else None\n    )\n    return train_loader, eval_loader\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDatasetBuilder.normalize","title":"<code>normalize()</code>","text":"<p>Apply z-score normalization to the activations in-place.</p> <p>Standardizes the activations by subtracting the mean and dividing by the standard deviation (computed along the batch dimension). Adds small epsilon (1e-8) to prevent division by zero. Only applies if norm=True was set during initialization.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Modifies self.activations in-place.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>@torch.no_grad()\ndef normalize(self) -&gt; None:\n    \"\"\"Apply z-score normalization to the activations in-place.\n\n    Standardizes the activations by subtracting the mean and dividing by the standard\n    deviation (computed along the batch dimension). Adds small epsilon (1e-8) to prevent\n    division by zero. Only applies if norm=True was set during initialization.\n\n    Returns:\n        None: Modifies self.activations in-place.\n    \"\"\"\n    if self.norm:\n        mean = self.activations.mean(dim=0, keepdim=True)\n        std = self.activations.std(dim=0, keepdim=True)\n        self.activations = (self.activations - mean) / (std + 1e-8)\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.ActivationsDatasetBuilder.set_tensor_dataset","title":"<code>set_tensor_dataset()</code>","text":"<p>Create a PyTorch Dataset from the loaded activations.</p> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>ActivationsDataset instance wrapping the activation tensors.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def set_tensor_dataset(self) -&gt; Dataset:\n    \"\"\"Create a PyTorch Dataset from the loaded activations.\n\n    Returns:\n        Dataset: ActivationsDataset instance wrapping the activation tensors.\n    \"\"\"\n    return ActivationsDataset(self.activations)\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.AudioDatasetBuilder","title":"<code>AudioDatasetBuilder</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch Dataset for loading and preprocessing audio files with optional mel spectrogram transformation.</p> <p>This dataset handles audio files in various formats (WAV, MP3, FLAC), performs preprocessing operations like resampling, mono conversion, and padding, and optionally converts waveforms to mel spectrograms. Supports both labeled and unlabeled datasets.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>class AudioDatasetBuilder(Dataset):\n    \"\"\"PyTorch Dataset for loading and preprocessing audio files with optional mel spectrogram transformation.\n\n    This dataset handles audio files in various formats (WAV, MP3, FLAC), performs preprocessing\n    operations like resampling, mono conversion, and padding, and optionally converts waveforms\n    to mel spectrograms. Supports both labeled and unlabeled datasets.\n    \"\"\"\n    def __init__(\n            self, \n            audio_dir: str = None, \n            annotations_file: str = None, \n            target_sample_rate: int = 22050, \n            num_samples: int = 22050, \n            device: str = \"auto\",\n            transformation_args: dict = {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}\n        ) -&gt; None:\n        \"\"\"Initialize the AudioDatasetBuilder with audio preprocessing parameters.\n\n        Args:\n            audio_dir (str, optional): Path to directory containing audio files. The directory\n                should contain files with extensions .wav, .mp3, or .flac. Defaults to None.\n            annotations_file (str, optional): Path to CSV file containing annotations/labels.\n                If None, dataset returns only audio without labels. Defaults to None.\n            target_sample_rate (int, optional): Target sampling rate in Hz for resampling.\n                All audio will be resampled to this rate. Defaults to 22050.\n            num_samples (int, optional): Target number of samples per audio clip. Audio will\n                be truncated or zero-padded to this length. Defaults to 22050.\n            device (str, optional): Device for tensor operations. Can be \"auto\" for automatic\n                selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n            transformation_args (dict, optional): Dictionary containing parameters for mel\n                spectrogram transformation. Must include keys: \"n_fft\", \"hop_length\", \"n_mels\".\n                If None, raw waveforms are returned without transformation. Defaults to\n                {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}.\n        \"\"\"\n        super().__init__()\n        self.audio_dir = audio_dir \n        self.file_list = [\n            f for f in os.listdir(self.audio_dir) \n            if f.endswith(('.wav', '.mp3', '.flac'))\n        ]\n        if annotations_file is not None:\n            self.annotations = pd.read_csv(annotations_file)\n        else:\n            self.annotations = None\n\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n        if device == \"auto\":\n            self.device = torch.device(\n                \"cuda\" if torch.cuda.is_available() \n                else \"mps\" if torch.backends.mps.is_available()\n                else \"cpu\"\n            )     \n        else:\n            self.device = torch.device(device)\n\n        self.transformation_args = transformation_args\n        if transformation_args is not None:\n            assert {\"n_fft\", \"hop_length\", \"n_mels\"}.issubset(transformation_args.keys()), \\\n            \"Missing arguments. Please provide n_fft, hop_length, and n_mels.\"\n            self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n                sample_rate=self.target_sample_rate,\n                n_fft = transformation_args[\"n_fft\"],\n                hop_length=transformation_args[\"hop_length\"],\n                n_mels=transformation_args[\"n_mels\"]\n            ).to(self.device)\n        else:\n            self.mel_spectrogram = None\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the total number of samples in the dataset.\n\n        Returns:\n            int: Number of audio samples. Returns length of annotations if provided,\n                otherwise returns number of audio files in the directory.\n        \"\"\"\n        if self.annotations is not None:\n            return len(self.annotations)\n        else:\n            return len(self.file_list)\n\n    def __getitem__(self, index) -&gt; torch.Tensor | tuple[torch.Tensor, int]:\n        \"\"\"Retrieve and preprocess an audio sample at the specified index.\n\n        Loads the audio file, applies preprocessing (resampling, mono conversion, padding/truncation),\n        optionally transforms to mel spectrogram, and returns with label if annotations are available.\n\n        Args:\n            index (int): Index of the sample to retrieve.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, int]: If annotations are provided, returns a tuple\n                of (processed_audio, label). Otherwise, returns only the processed audio tensor.\n                Audio shape depends on transformation: (1, num_samples) for waveform or\n                (1, n_mels, time_steps) for mel spectrogram.\n        \"\"\"\n        audio_sample = self._audio_sample_path(index)    \n        signal, sr = torchaudio.load(audio_sample)\n        signal = signal.to(self.device)                   \n        signal = self._resample_if_necessary(signal, sr)  \n        signal = self._mix_down_if_necessary(signal)  \n        signal = self._truncate_if_necessary(signal)    \n        signal = self._pad_if_necessary(signal)\n        if self.transformation_args is not None:\n            signal = self._apply_transformation(signal)\n        if self.annotations is not None:\n            label = self._audio_sample_label(index)        \n            return signal, label\n        else:\n            return signal\n\n    def _audio_sample_path(self, index) -&gt; str:\n        \"\"\"Construct the file path for an audio sample at the given index.\n\n        Args:\n            index (int): Index of the audio sample.\n\n        Returns:\n            str: Full path to the audio file. If annotations are provided, constructs path\n                using fold structure; otherwise, uses direct file listing.\n        \"\"\"\n        if self.annotations is not None:\n            fold = f\"fold{self.annotations.iloc[index, 5]}\"\n            path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n        else:\n            path = os.path.join(self.audio_dir, self.file_list[index])\n        return path\n\n    def _audio_sample_label(self, index) -&gt; int:\n        \"\"\"Retrieve the label for an audio sample at the given index.\n\n        Args:\n            index (int): Index of the audio sample.\n\n        Returns:\n            int: Label value from the annotations file (column 6).\n        \"\"\"\n        return self.annotations.iloc[index, 6]\n\n    @torch.no_grad()\n    def _resample_if_necessary(self, signal, sr) -&gt; torch.Tensor:\n        \"\"\"Resample audio signal to target sample rate if necessary.\n\n        Args:\n            signal (torch.Tensor): Input audio waveform.\n            sr (int): Current sample rate of the audio signal.\n\n        Returns:\n            torch.Tensor: Resampled audio at target_sample_rate, or original signal\n                if already at the target rate.\n        \"\"\"\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            resampler.to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    @torch.no_grad()\n    def _mix_down_if_necessary(self, signal) -&gt; torch.Tensor:\n        \"\"\"Convert stereo audio to mono by averaging channels if necessary.\n\n        Args:\n            signal (torch.Tensor): Input audio with shape (channels, samples).\n\n        Returns:\n            torch.Tensor: Mono audio with shape (1, samples). If input is already mono,\n                returns unchanged.\n        \"\"\"\n        if signal.shape[0] &gt; 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _truncate_if_necessary(self, signal) -&gt; torch.Tensor:\n        \"\"\"Truncate audio signal to target length if it exceeds num_samples.\n\n        Args:\n            signal (torch.Tensor): Input audio waveform.\n\n        Returns:\n            torch.Tensor: Truncated audio limited to num_samples length, or original\n                signal if already shorter.\n        \"\"\"\n        if signal.shape[1] &gt; self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    @torch.no_grad()\n    def _pad_if_necessary(self, signal) -&gt; torch.Tensor:\n        \"\"\"Zero-pad audio signal to target length if it's shorter than num_samples.\n\n        Args:\n            signal (torch.Tensor): Input audio waveform.\n\n        Returns:\n            torch.Tensor: Zero-padded audio extended to num_samples length, or original\n                signal if already long enough.\n        \"\"\"\n        length_signal = signal.shape[1]\n        if length_signal &lt; self.num_samples:\n            n_padding = self.num_samples - length_signal\n            r_pad_dim = (0, n_padding)\n            signal = torch.nn.functional.pad(signal, r_pad_dim)\n        return signal\n\n    def _apply_transformation(self, signal) -&gt; torch.Tensor:\n        \"\"\"Transform audio waveform to mel spectrogram representation.\n\n        Args:\n            signal (torch.Tensor): Input audio waveform with shape (1, num_samples).\n\n        Returns:\n            torch.Tensor: Mel spectrogram with shape (1, n_mels, time_steps), where\n                time_steps depends on n_fft and hop_length parameters.\n        \"\"\"\n        spectrogram = self.mel_spectrogram(signal)\n        return spectrogram\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.AudioDatasetBuilder.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve and preprocess an audio sample at the specified index.</p> <p>Loads the audio file, applies preprocessing (resampling, mono conversion, padding/truncation), optionally transforms to mel spectrogram, and returns with label if annotations are available.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <code>Tensor | tuple[Tensor, int]</code> <p>torch.Tensor | tuple[torch.Tensor, int]: If annotations are provided, returns a tuple of (processed_audio, label). Otherwise, returns only the processed audio tensor. Audio shape depends on transformation: (1, num_samples) for waveform or (1, n_mels, time_steps) for mel spectrogram.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __getitem__(self, index) -&gt; torch.Tensor | tuple[torch.Tensor, int]:\n    \"\"\"Retrieve and preprocess an audio sample at the specified index.\n\n    Loads the audio file, applies preprocessing (resampling, mono conversion, padding/truncation),\n    optionally transforms to mel spectrogram, and returns with label if annotations are available.\n\n    Args:\n        index (int): Index of the sample to retrieve.\n\n    Returns:\n        torch.Tensor | tuple[torch.Tensor, int]: If annotations are provided, returns a tuple\n            of (processed_audio, label). Otherwise, returns only the processed audio tensor.\n            Audio shape depends on transformation: (1, num_samples) for waveform or\n            (1, n_mels, time_steps) for mel spectrogram.\n    \"\"\"\n    audio_sample = self._audio_sample_path(index)    \n    signal, sr = torchaudio.load(audio_sample)\n    signal = signal.to(self.device)                   \n    signal = self._resample_if_necessary(signal, sr)  \n    signal = self._mix_down_if_necessary(signal)  \n    signal = self._truncate_if_necessary(signal)    \n    signal = self._pad_if_necessary(signal)\n    if self.transformation_args is not None:\n        signal = self._apply_transformation(signal)\n    if self.annotations is not None:\n        label = self._audio_sample_label(index)        \n        return signal, label\n    else:\n        return signal\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.AudioDatasetBuilder.__init__","title":"<code>__init__(audio_dir=None, annotations_file=None, target_sample_rate=22050, num_samples=22050, device='auto', transformation_args={'n_fft': 1024, 'hop_length': 512, 'n_mels': 64})</code>","text":"<p>Initialize the AudioDatasetBuilder with audio preprocessing parameters.</p> <p>Parameters:</p> Name Type Description Default <code>audio_dir</code> <code>str</code> <p>Path to directory containing audio files. The directory should contain files with extensions .wav, .mp3, or .flac. Defaults to None.</p> <code>None</code> <code>annotations_file</code> <code>str</code> <p>Path to CSV file containing annotations/labels. If None, dataset returns only audio without labels. Defaults to None.</p> <code>None</code> <code>target_sample_rate</code> <code>int</code> <p>Target sampling rate in Hz for resampling. All audio will be resampled to this rate. Defaults to 22050.</p> <code>22050</code> <code>num_samples</code> <code>int</code> <p>Target number of samples per audio clip. Audio will be truncated or zero-padded to this length. Defaults to 22050.</p> <code>22050</code> <code>device</code> <code>str</code> <p>Device for tensor operations. Can be \"auto\" for automatic selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".</p> <code>'auto'</code> <code>transformation_args</code> <code>dict</code> <p>Dictionary containing parameters for mel spectrogram transformation. Must include keys: \"n_fft\", \"hop_length\", \"n_mels\". If None, raw waveforms are returned without transformation. Defaults to {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}.</p> <code>{'n_fft': 1024, 'hop_length': 512, 'n_mels': 64}</code> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __init__(\n        self, \n        audio_dir: str = None, \n        annotations_file: str = None, \n        target_sample_rate: int = 22050, \n        num_samples: int = 22050, \n        device: str = \"auto\",\n        transformation_args: dict = {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}\n    ) -&gt; None:\n    \"\"\"Initialize the AudioDatasetBuilder with audio preprocessing parameters.\n\n    Args:\n        audio_dir (str, optional): Path to directory containing audio files. The directory\n            should contain files with extensions .wav, .mp3, or .flac. Defaults to None.\n        annotations_file (str, optional): Path to CSV file containing annotations/labels.\n            If None, dataset returns only audio without labels. Defaults to None.\n        target_sample_rate (int, optional): Target sampling rate in Hz for resampling.\n            All audio will be resampled to this rate. Defaults to 22050.\n        num_samples (int, optional): Target number of samples per audio clip. Audio will\n            be truncated or zero-padded to this length. Defaults to 22050.\n        device (str, optional): Device for tensor operations. Can be \"auto\" for automatic\n            selection, \"cuda\", \"mps\", or \"cpu\". Defaults to \"auto\".\n        transformation_args (dict, optional): Dictionary containing parameters for mel\n            spectrogram transformation. Must include keys: \"n_fft\", \"hop_length\", \"n_mels\".\n            If None, raw waveforms are returned without transformation. Defaults to\n            {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}.\n    \"\"\"\n    super().__init__()\n    self.audio_dir = audio_dir \n    self.file_list = [\n        f for f in os.listdir(self.audio_dir) \n        if f.endswith(('.wav', '.mp3', '.flac'))\n    ]\n    if annotations_file is not None:\n        self.annotations = pd.read_csv(annotations_file)\n    else:\n        self.annotations = None\n\n    self.target_sample_rate = target_sample_rate\n    self.num_samples = num_samples\n\n    if device == \"auto\":\n        self.device = torch.device(\n            \"cuda\" if torch.cuda.is_available() \n            else \"mps\" if torch.backends.mps.is_available()\n            else \"cpu\"\n        )     \n    else:\n        self.device = torch.device(device)\n\n    self.transformation_args = transformation_args\n    if transformation_args is not None:\n        assert {\"n_fft\", \"hop_length\", \"n_mels\"}.issubset(transformation_args.keys()), \\\n        \"Missing arguments. Please provide n_fft, hop_length, and n_mels.\"\n        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n            sample_rate=self.target_sample_rate,\n            n_fft = transformation_args[\"n_fft\"],\n            hop_length=transformation_args[\"hop_length\"],\n            n_mels=transformation_args[\"n_mels\"]\n        ).to(self.device)\n    else:\n        self.mel_spectrogram = None\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.AudioDatasetBuilder.__len__","title":"<code>__len__()</code>","text":"<p>Get the total number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of audio samples. Returns length of annotations if provided, otherwise returns number of audio files in the directory.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the total number of samples in the dataset.\n\n    Returns:\n        int: Number of audio samples. Returns length of annotations if provided,\n            otherwise returns number of audio files in the directory.\n    \"\"\"\n    if self.annotations is not None:\n        return len(self.annotations)\n    else:\n        return len(self.file_list)\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.GetDataLoaders","title":"<code>GetDataLoaders</code>","text":"<p>Utility class for creating train and test DataLoaders from a PyTorch Dataset.</p> <p>Handles dataset splitting and DataLoader creation with consistent parameters.</p> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>class GetDataLoaders():\n    \"\"\"Utility class for creating train and test DataLoaders from a PyTorch Dataset.\n\n    Handles dataset splitting and DataLoader creation with consistent parameters.\n    \"\"\"\n    def __init__(\n            self, \n            dataset: Dataset = None,\n            splits: list = [0.8, 0.2],\n            batch_size: int = 16\n        ) -&gt; None:\n        \"\"\"Initialize the DataLoader factory.\n\n        Args:\n            dataset (Dataset, optional): PyTorch Dataset to split and load. Defaults to None.\n            splits (list, optional): List of two floats representing train and test split\n                proportions. Must sum to 1.0. Defaults to [0.8, 0.2].\n            batch_size (int, optional): Number of samples per batch. Defaults to 16.\n        \"\"\"\n        self.dataset = dataset\n        self.splits = splits\n        self.batch_size = batch_size\n\n    def _prepare_loader(self) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"Create train and test DataLoaders with the specified configuration.\n\n        Splits the dataset according to the split proportions and creates two DataLoaders\n        with appropriate settings for training and testing.\n\n        Returns:\n            tuple[DataLoader, DataLoader]: A tuple containing (train_loader, test_loader).\n                Training loader has shuffle=True, test loader has shuffle=False. Both use\n                pin_memory=True for faster data transfer to GPU.\n        \"\"\"\n        train, test = random_split(self.dataset, self.splits)\n        train_loader = DataLoader(train, self.batch_size, shuffle=True, pin_memory=True)\n        test_loader = DataLoader(test, self.batch_size, shuffle=False, pin_memory=True)\n        return train_loader, test_loader\n</code></pre>"},{"location":"api/utils/#deeplens.utils.dataset.GetDataLoaders.__init__","title":"<code>__init__(dataset=None, splits=[0.8, 0.2], batch_size=16)</code>","text":"<p>Initialize the DataLoader factory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>PyTorch Dataset to split and load. Defaults to None.</p> <code>None</code> <code>splits</code> <code>list</code> <p>List of two floats representing train and test split proportions. Must sum to 1.0. Defaults to [0.8, 0.2].</p> <code>[0.8, 0.2]</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch. Defaults to 16.</p> <code>16</code> Source code in <code>deeplens/utils/dataset.py</code> <pre><code>def __init__(\n        self, \n        dataset: Dataset = None,\n        splits: list = [0.8, 0.2],\n        batch_size: int = 16\n    ) -&gt; None:\n    \"\"\"Initialize the DataLoader factory.\n\n    Args:\n        dataset (Dataset, optional): PyTorch Dataset to split and load. Defaults to None.\n        splits (list, optional): List of two floats representing train and test split\n            proportions. Must sum to 1.0. Defaults to [0.8, 0.2].\n        batch_size (int, optional): Number of samples per batch. Defaults to 16.\n    \"\"\"\n    self.dataset = dataset\n    self.splits = splits\n    self.batch_size = batch_size\n</code></pre>"},{"location":"api/utils/#deeplens.utils.tools","title":"<code>deeplens.utils.tools</code>","text":""},{"location":"api/utils/#deeplens.utils.tools.get_device","title":"<code>get_device(device='auto')</code>","text":"<p>Utility to set up the torch device. If 'auto', it selects the most appropriate device in your machine. </p> <p>It can be set manually to 'mps', 'cuda', or 'cpu', but 'auto' is  recommended.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device in which the given process will be allocated.  Defaults to 'auto'.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>device</code> <p>torch.device: The selected device.</p> Source code in <code>deeplens/utils/tools.py</code> <pre><code>def get_device(device: str = \"auto\") -&gt; torch.device:\n    \"\"\"Utility to set up the torch device. If 'auto', it selects\n    the most appropriate device in your machine. \n\n    It can be set manually to 'mps', 'cuda', or 'cpu', but 'auto' is \n    recommended.\n\n    Args:\n        device: The device in which the given process will be allocated. \n            Defaults to 'auto'.\n\n    Returns:\n        torch.device: The selected device.\n    \"\"\"\n    if device == \"auto\":\n        return torch.device(\n            \"cuda\" if torch.cuda.is_available() \n            else \"mps\" if torch.backends.mps.is_available()\n            else \"cpu\"\n        )\n    return torch.device(device)\n</code></pre>"},{"location":"pretrained/repository/","title":"Pretrained SAE Models","text":""},{"location":"pretrained/repository/#gpt-2-small-layer-3","title":"GPT-2 Small (Layer 3)","text":"<ul> <li>Architecture: TopK SAE</li> <li>Weight Tying: Yes</li> <li>Hidden Size: 24,576</li> <li>Non-zero Fraction: 3%</li> <li>Training Dataset: 1.1M Activations</li> <li>Context Window: 1,024</li> <li>Eval Loss: 0.06</li> </ul> <p>Download Model Download Config</p>"},{"location":"pretrained/repository/#gpt-2-small-layer-3_1","title":"GPT-2 Small (Layer 3)","text":"<ul> <li>Architecture: TopK SAE</li> <li>Weight Tying: No</li> <li>Hidden Size: 24,576</li> <li>Non-zero Fraction: 3%</li> <li>Training Dataset: 1.1M Activations</li> <li>Context Window: 1,024</li> <li>Eval Loss: 0.06</li> </ul> <p>Download Model Download Config</p>"},{"location":"pretrained/repository/#gpt-2-small-layer-12","title":"GPT-2 Small (Layer 12)","text":"<ul> <li>Architecture: TopK SAE</li> <li>Weight Tying: No</li> <li>Hidden Size: 24,576</li> <li>Non-zero Fraction: 3%</li> <li>Training Dataset: 1.1M Activations</li> <li>Context Window: 1,024</li> <li>Eval Loss: 0.14</li> </ul> <p>Download Model Download Config</p>"},{"location":"pretrained/repository/#gpt-2-medium-layer-3","title":"GPT-2 Medium (Layer 3)","text":"<ul> <li>Architecture: TopK SAE</li> <li>Weight Tying: No</li> <li>Hidden Size: 32,768</li> <li>Non-zero Fraction: 2%</li> <li>Training Dataset: 1.1M Activations</li> <li>Context Window: 1,024</li> <li>Eval Loss: TBD</li> </ul> <p>Download Model Download Config</p>"},{"location":"tutorials/analysis/","title":"Feature Analysis","text":"<p>This tutorial covers how to analyze and visualize the features learned by your sparse autoencoder. Understanding what features represent is crucial for mechanistic interpretability.</p>"},{"location":"tutorials/analysis/#overview","title":"Overview","text":"<p>DeepLens provides several utilities for feature analysis:</p> Function Description <code>generate_feature_heatmap</code> Visualize logit distributions across positions <code>plot_topk_distribution</code> Bar chart of top-k token predictions <code>get_top_k_tokens</code> Extract top predicted tokens at each position"},{"location":"tutorials/analysis/#basic-analysis-workflow","title":"Basic Analysis Workflow","text":"<p>After training an SAE, the typical analysis workflow is:</p> <ol> <li>Extract activations from a sample text</li> <li>Encode through the SAE to get feature activations</li> <li>Identify which features are active</li> <li>Investigate what those features represent</li> </ol> <pre><code>from deeplens.extractor import ExtractSingleSample\nfrom deeplens.intervene import InterveneFeatures\n\n# Extract activations\nextractor = ExtractSingleSample(hf_model=\"gpt2\", layer=3)\ntext = \"The capital of France is Paris.\"\nactivations = extractor.get_mlp_acts(text)\n\n# Load SAE and get feature activations\nintervene = InterveneFeatures(\n    sae_model=\"models/best_model.pt\",\n    sae_config=\"config.yaml\"\n)\n\n# Find active features at last token\nactive_features = intervene.get_alive_features(activations, token_position=-1)\nprint(f\"Active features: {active_features[:10]}\")\n</code></pre>"},{"location":"tutorials/analysis/#finding-active-features","title":"Finding Active Features","text":""},{"location":"tutorials/analysis/#get-all-non-zero-features","title":"Get All Non-Zero Features","text":"<pre><code># Get indices of all active features at the last token\nactive_features = intervene.get_alive_features(\n    activations,\n    token_position=-1  # -1 for last token\n)\n\nprint(f\"Number of active features: {len(active_features)}\")\nprint(f\"Feature indices: {active_features.tolist()}\")\n</code></pre>"},{"location":"tutorials/analysis/#get-top-k-most-active-features","title":"Get Top-K Most Active Features","text":"<pre><code># Get only the top 20 most active features\ntop_features = intervene.get_alive_features(\n    activations,\n    token_position=-1,\n    k=20  # Only return top 20\n)\n\nprint(f\"Top 20 features: {top_features.tolist()}\")\n</code></pre>"},{"location":"tutorials/analysis/#visualizing-logits","title":"Visualizing Logits","text":""},{"location":"tutorials/analysis/#heatmap","title":"Heatmap","text":"<p>Generate a heatmap showing how logits distribute across the vocabulary:</p> <pre><code>from deeplens.utils.analysis import AnalysisUtils\nfrom deeplens.intervene import ReinjectSingleSample\n\n# Get logits from the model\nreinject = ReinjectSingleSample(hf_model=\"gpt2\")\nanalysis = AnalysisUtils(\n    hf_model=\"gpt2\",\n    sae_model=\"MODEL/DIR\",\n    sae_config=\"CONFIG/DIR\",\n    layer=3\n)\n\n# First, get modified activations (or original ones)\n_, original_acts, modified_acts = intervene.intervene_feature(\n    activations=activations,\n    feature=active_features[0].item(),\n    alpha=1.0  # No modification, just decode\n)\n\n# Get logits\nlogits = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=original_acts,\n    layer=3,\n    generate=False\n)\n\n# Generate heatmap\nanalysis.generate_feature_heatmap(logits, k=1000, save_name=\"logits_heatmap\")\n</code></pre> <p>The heatmap shows: - X-axis: Vocabulary indices - Y-axis: Token positions - Color: Logit magnitude (brighter = higher)</p> <p></p>"},{"location":"tutorials/analysis/#top-k-token-distribution","title":"Top-K Token Distribution","text":"<p>Visualize the most likely next tokens at a specific position:</p> <pre><code>analysis.plot_topk_distribution(\n        logits=logits, \n        use_softmax=True, \n        k=50, \n        position=-1, \n        title=\"SAE Logits\"\n    )\n</code></pre> <p>Parameters: - <code>k</code>: Number of top tokens to show - <code>position</code>: Which token position to analyze (use -1 for last) - <code>use_softmax</code>: Convert logits to probabilities - <code>title</code>: Plot title - <code>save_name</code>: Filename to save (without extension)</p> <p></p>"},{"location":"tutorials/analysis/#extract-top-tokens-as-data","title":"Extract Top Tokens as Data","text":"<pre><code>results = analysis.get_top_k_tokens(\n    logits=modified_output,\n    k=5,\n    to_dataframe=True,\n    verbose=False\n)\n</code></pre>"},{"location":"tutorials/analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/analysis/#empty-active-features","title":"Empty Active Features","text":"<p>If <code>get_alive_features</code> returns an empty tensor: - Check that your SAE is trained properly - Try a different token position - Verify activations are not all zeros</p>"},{"location":"tutorials/dataset/","title":"Dataset Preparation","text":"<p>This tutorial covers how to prepare your extracted activations for training a sparse autoencoder. Proper dataset preparation is crucial for effective SAE training.</p>"},{"location":"tutorials/dataset/#overview","title":"Overview","text":"<p>DeepLens provides utilities for loading, normalizing, and creating DataLoaders from activation tensors:</p> Class Description <code>ActivationsDatasetBuilder</code> Main class for loading activations and creating train/eval DataLoaders <code>GetDataLoaders</code> General-purpose DataLoader factory for any PyTorch Dataset <code>AudioDatasetBuilder</code> Specialized Dataset for audio processing (advanced use)"},{"location":"tutorials/dataset/#loading-activations","title":"Loading Activations","text":""},{"location":"tutorials/dataset/#from-saved-feature-files","title":"From Saved Feature Files","text":"<p>After extracting features with <code>FromHuggingFace</code>, load them for training:</p> <pre><code>from deeplens.utils.dataset import ActivationsDatasetBuilder\n\n# Load saved activations\ndataset = ActivationsDatasetBuilder(\n    activations=\"saved_features/features_layer_3_100000.pt\",\n    splits=[0.8, 0.2],\n    batch_size=16,\n    norm=True\n)\n\n# Create DataLoaders\ntrain_loader, eval_loader = dataset.get_dataloaders()\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Evaluation batches: {len(eval_loader)}\")\n</code></pre>"},{"location":"tutorials/dataset/#parameters-explained","title":"Parameters Explained","text":"Parameter Description Default <code>activations</code> Path to <code>.pt</code> file containing activation tensors None <code>splits</code> Train/validation split proportions (must sum to 1.0) [0.8, 0.2] <code>batch_size</code> Number of samples per batch 16 <code>norm</code> Whether to apply z-score normalization True"},{"location":"tutorials/dataset/#normalization","title":"Normalization","text":"<p>Normalization is crucial for stable SAE training. By default, <code>ActivationsDatasetBuilder</code> applies z-score normalization (standardization).</p>"},{"location":"tutorials/dataset/#what-normalization-does","title":"What Normalization Does","text":"<pre><code># x_normalized = (x - mean) / (std + epsilon)\n</code></pre> <p>This ensures:</p> <ul> <li>Zero mean across features</li> <li>Unit variance across features</li> <li>Stable gradients during training</li> </ul>"},{"location":"tutorials/dataset/#when-to-disable-normalization","title":"When to Disable Normalization","text":"<p>In most cases, keep <code>norm=True</code>. Disable it only if: - Your activations are already normalized - You want to preserve the original scale for specific analysis</p>"},{"location":"tutorials/dataset/#choosing-batch-size","title":"Choosing Batch Size","text":"<p>Batch size affects both training speed and quality:</p> Batch Size Pros Cons Small (8-16) Lower memory, more gradient updates Slower training, noisier gradients Medium (32-64) Good balance - Large (128-256) Faster training, smoother gradients Higher memory usage"},{"location":"tutorials/dataset/#recommendations","title":"Recommendations","text":"<pre><code># For most GPUs (8-16GB VRAM)\ndataset = ActivationsDatasetBuilder(\n    activations=\"path/to/features.pt\",\n    batch_size=32\n)\n\n# For limited memory\ndataset = ActivationsDatasetBuilder(\n    activations=\"path/to/features.pt\",\n    batch_size=8\n)\n\n# For large GPUs (40GB+ VRAM)\ndataset = ActivationsDatasetBuilder(\n    activations=\"path/to/features.pt\",\n    batch_size=128\n)\n</code></pre>"},{"location":"tutorials/dataset/#trainvalidation-splits","title":"Train/Validation Splits","text":"<p>The <code>splits</code> parameter controls how data is divided:</p> <pre><code>dataset = ActivationsDatasetBuilder(\n    activations=\"path/to/features.pt\",\n    splits=[0.8, 0.2]\n)\n</code></pre> <p>Note: Splits must sum to 1.0.</p>"},{"location":"tutorials/dataset/#dataloader-best-practices","title":"DataLoader Best Practices","text":""},{"location":"tutorials/dataset/#pin-memory","title":"Pin Memory","text":"<p>Enable <code>pin_memory</code> for faster GPU transfers (automatically enabled by <code>ActivationsDatasetBuilder</code>):</p> <pre><code>train_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    pin_memory=True  # Faster CPU\u2192GPU transfer\n)\n</code></pre>"},{"location":"tutorials/dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/dataset/#out-of-memory-when-loading","title":"Out of Memory When Loading","text":"<pre><code># Load with memory mapping\nfeatures = torch.load(\"path/to/large_features.pt\", mmap=True)\n\n# Or load a subset\nfeatures = torch.load(\"path/to/features.pt\")\nfeatures = features[:50000]  # Use first 50k samples\n</code></pre>"},{"location":"tutorials/dataset/#next-steps","title":"Next Steps","text":"<p>Now that your dataset is ready:</p> <ol> <li>Train a Sparse Autoencoder - Train your SAE on the prepared data</li> <li>Analyze Features - Understand what your SAE learned</li> <li>Feature Interventions - Test causal effects of features</li> </ol>"},{"location":"tutorials/extraction/","title":"Activation Extraction","text":"<p>This tutorial covers how to extract MLP activations from transformer models using DeepLens. Activation extraction is the first step in the mechanistic interpretability pipeline\u2014you need to collect the neural network's internal representations before you can train a sparse autoencoder to discover interpretable features.</p>"},{"location":"tutorials/extraction/#overview","title":"Overview","text":"<p>DeepLens provides two main classes for extracting activations:</p> Class Use Case <code>FromHuggingFace</code> Extract activations from large datasets for SAE training <code>ExtractSingleSample</code> Extract activations from individual texts for analysis"},{"location":"tutorials/extraction/#extracting-from-large-datasets","title":"Extracting from Large Datasets","text":"<p>Use <code>FromHuggingFace</code> when you need to collect many activations for training a sparse autoencoder. This class streams data from HuggingFace datasets, so you don't need to download the entire dataset upfront.</p>"},{"location":"tutorials/extraction/#basic-usage","title":"Basic Usage","text":"<pre><code>from deeplens.extractor import FromHuggingFace\n\nextractor = FromHuggingFace(\n    hf_model=\"gpt2\",\n    layer=3,\n    dataset_name=\"HuggingFaceFW/fineweb\",\n    num_samples=25000,\n    seq_length=1024,\n    inference_batch_size=16,\n    device=\"auto\",\n    save_features=True\n)\n\nfeatures = extractor.extract_features()\nprint(f\"Shape: {features.shape}\")  # (total_tokens, hidden_dim)\n# Saved to: saved_features/features_layer_3_XXXXX.pt\n</code></pre>"},{"location":"tutorials/extraction/#parameters-explained","title":"Parameters Explained","text":"Parameter Description Default <code>hf_model</code> HuggingFace model identifier (e.g., \"gpt2\", \"gpt2-medium\") \"gpt2\" <code>layer</code> Transformer layer index to extract from (0-indexed) 6 <code>dataset_name</code> HuggingFace dataset to stream \"HuggingFaceFW/fineweb\" <code>num_samples</code> Number of text samples to process 25000 <code>seq_length</code> Maximum sequence length for tokenization 128 <code>inference_batch_size</code> Batch size for model inference 16 <code>device</code> \"auto\", \"cuda\", \"mps\", or \"cpu\" \"auto\" <code>save_features</code> Whether to save extracted features to disk True"},{"location":"tutorials/extraction/#choosing-the-right-layer","title":"Choosing the Right Layer","text":"<p>Different layers capture different types of features:</p> <ul> <li>Early layers (0-3): Low-level features like token identity and basic syntax</li> <li>Middle layers (4-8): Compositional features and semantic relationships</li> <li>Late layers (9-11): Task-specific and high-level abstract features</li> </ul>"},{"location":"tutorials/extraction/#memory-optimization","title":"Memory Optimization","text":"<p>If you're running out of memory, try these strategies:</p> <pre><code># Reduce batch size for lower memory usage\nextractor = FromHuggingFace(\n    hf_model=\"gpt2\",\n    inference_batch_size=4,  # Lower batch size\n    seq_length=512,          # Shorter sequences\n    num_samples=2500         # Collect less features\n)\n</code></pre>"},{"location":"tutorials/extraction/#custom-datasets","title":"Custom Datasets","text":"<p>You can use any HuggingFace dataset with a <code>text</code> field:</p> <pre><code># Use a different dataset\nextractor = FromHuggingFace(\n    hf_model=\"gpt2\",\n    dataset_name=\"wikitext/wikitext-103-v1\",\n    num_samples=1000\n)\n</code></pre>"},{"location":"tutorials/extraction/#extracting-from-single-samples","title":"Extracting from Single Samples","text":"<p>Use <code>ExtractSingleSample</code> when you want to analyze specific texts, such as when testing feature interventions or debugging.</p>"},{"location":"tutorials/extraction/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from deeplens.extractor import ExtractSingleSample\n\nextractor = ExtractSingleSample(\n    hf_model=\"gpt2\",\n    layer=3,\n    max_length=1024,\n    device=\"auto\"\n)\n\n# Extract activations from a specific text\ntext = \"The capital of France is Paris.\"\nactivations = extractor.get_mlp_acts(text)\nprint(f\"Shape: {activations.shape}\")  # (seq_length, hidden_dim)\n</code></pre>"},{"location":"tutorials/extraction/#parameters-explained_1","title":"Parameters Explained","text":"Parameter Description Default <code>hf_model</code> HuggingFace model identifier \"gpt2\" <code>layer</code> Transformer layer to extract from 3 <code>max_length</code> Maximum sequence length 1024 <code>device</code> \"auto\", \"cuda\", \"mps\", or \"cpu\" \"auto\""},{"location":"tutorials/extraction/#analyzing-token-level-activations","title":"Analyzing Token-Level Activations","text":"<p>Each position in the output corresponds to a token:</p> <pre><code>extractor = ExtractSingleSample(hf_model=\"gpt2\", layer=3)\n\ntext = \"Hello world\"\nactivations = extractor.get_mlp_acts(text)\n\n# Access tokenizer to see which tokens correspond to which positions\ntokens = extractor.tokenizer.tokenize(text)\nprint(f\"Tokens: {tokens}\")\nprint(f\"Activation shape: {activations.shape}\")\n\n# activations[0] = activation for \"Hello\"\n# activations[1] = activation for \" world\"\n</code></pre>"},{"location":"tutorials/extraction/#next-steps","title":"Next Steps","text":"<p>Now that you have extracted activations, you can:</p> <ol> <li>Create a Dataset - Prepare your activations for training</li> <li>Train a Sparse Autoencoder - Discover interpretable features</li> <li>Analyze Features - Visualize and understand what you've learned</li> </ol>"},{"location":"tutorials/extraction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/extraction/#out-of-memory","title":"Out of Memory","text":"<pre><code># Solutions:\n# 1. Reduce batch size\nextractor = FromHuggingFace(inference_batch_size=4)\n# 2. Use shorter sequences\nextractor = FromHuggingFace(seq_length=128)\n# 3. Process fewer samples\nextractor = FromHuggingFace(num_samples=1000)\n</code></pre>"},{"location":"tutorials/extraction/#slow-extraction","title":"Slow Extraction","text":"<pre><code># Use GPU if available\nextractor = FromHuggingFace(device=\"cuda\")\n# Increase batch size if you have memory headroom\nextractor = FromHuggingFace(inference_batch_size=32)\n</code></pre>"},{"location":"tutorials/extraction/#dataset-not-found","title":"Dataset Not Found","text":"<p>Make sure the dataset exists on HuggingFace and has a <code>text</code> field:</p> <pre><code>from datasets import load_dataset\n\n# Check dataset structure\nds = load_dataset(\"your-dataset\", split=\"train\", streaming=True)\nsample = next(iter(ds))\nprint(sample.keys())  # Should include 'text'\n</code></pre>"},{"location":"tutorials/interventions/","title":"Feature Interventions","text":"<p>This tutorial covers how to perform causal interventions on sparse autoencoder features. By modifying feature activations and observing changes in model behavior, you can establish causal relationships between features and model outputs.</p>"},{"location":"tutorials/interventions/#overview","title":"Overview","text":"<p>DeepLens provides two main classes for feature intervention:</p> Class Description <code>InterveneFeatures</code> Modify feature activations in the SAE latent space <code>ReinjectSingleSample</code> Reinject modified activations back into the model <p>The intervention workflow: 1. Extract activations from input text 2. Encode through SAE to get feature activations 3. Modify specific features (amplify, suppress, or ablate) 4. Decode back to activation space 5. Reinject into model to observe effects</p>"},{"location":"tutorials/interventions/#basic-intervention","title":"Basic Intervention","text":""},{"location":"tutorials/interventions/#setup","title":"Setup","text":"<pre><code>from deeplens.extractor import ExtractSingleSample\nfrom deeplens.intervene import InterveneFeatures, ReinjectSingleSample\n\n# Initialize components\nextractor = ExtractSingleSample(hf_model=\"gpt2\", layer=3)\nintervene = InterveneFeatures(\n    sae_model=\"models/best_model.pt\",\n    sae_config=\"config.yaml\"\n)\nreinject = ReinjectSingleSample(hf_model=\"gpt2\")\n\n# Extract activations\ntext = \"The capital of France is\"\nactivations = extractor.get_mlp_acts(text)\n</code></pre>"},{"location":"tutorials/interventions/#amplify-a-feature","title":"Amplify a Feature","text":"<p>Increase a feature's activation to strengthen its effect:</p> <pre><code># Find active features at the last token\nactive_features = intervene.get_alive_features(activations, token_position=-1)\nfeature_to_modify = active_features[0].item()\n\n# Amplify the feature by 5x\n_, original_acts, modified_acts = intervene.intervene_feature(\n    activations=activations,\n    feature=feature_to_modify,\n    alpha=5.0,  # Multiply by 5\n    token_positions=-1  # Only modify last token\n)\n</code></pre>"},{"location":"tutorials/interventions/#see-the-effect","title":"See the Effect","text":"<pre><code># Generate with original activations\noriginal_output = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=original_acts,\n    layer=3,\n    generate=True,\n    max_new_tokens=10\n)\n\n# Generate with modified activations\nmodified_output = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=modified_acts,\n    layer=3,\n    generate=True,\n    max_new_tokens=10\n)\n\nprint(f\"Original: {original_output}\")\nprint(f\"Modified: {modified_output}\")\n</code></pre>"},{"location":"tutorials/interventions/#intervention-types","title":"Intervention Types","text":""},{"location":"tutorials/interventions/#amplification-alpha-1","title":"Amplification (alpha &gt; 1)","text":"<p>Strengthen a feature's influence:</p> <pre><code># Amplify by different amounts\nfor alpha in [2.0, 5.0, 10.0, 20.0]:\n    _, _, modified = intervene.intervene_feature(\n        activations, feature=0, alpha=alpha, token_positions=-1\n    )\n    output = reinject.reinject_and_generate(\n        text, modified, layer=3, generate=True, max_new_tokens=10\n    )\n    print(f\"Alpha {alpha:5.1f}: {output}\")\n</code></pre>"},{"location":"tutorials/interventions/#suppression-0-alpha-1","title":"Suppression (0 &lt; alpha &lt; 1)","text":"<p>Weaken a feature's influence:</p> <pre><code># Suppress the feature\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=0.1,  # Reduce to 10%\n    token_positions=-1\n)\n</code></pre>"},{"location":"tutorials/interventions/#ablation-alpha-0","title":"Ablation (alpha = 0)","text":"<p>Completely remove a feature's contribution:</p> <pre><code># Zero out the feature\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=0.0,  # Complete ablation\n    token_positions=-1\n)\n</code></pre>"},{"location":"tutorials/interventions/#negation-alpha-0","title":"Negation (alpha &lt; 0)","text":"<p>Reverse a feature's effect:</p> <pre><code># Negate the feature\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=-1.0,  # Flip sign\n    token_positions=-1\n)\n</code></pre>"},{"location":"tutorials/interventions/#token-position-control","title":"Token Position Control","text":""},{"location":"tutorials/interventions/#single-position","title":"Single Position","text":"<p>Modify only one token position:</p> <pre><code># Modify only the last token\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=5.0,\n    token_positions=-1  # Last token only\n)\n</code></pre>"},{"location":"tutorials/interventions/#multiple-positions","title":"Multiple Positions","text":"<p>Modify several specific positions:</p> <pre><code># Modify positions 2, 3, and 4\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=5.0,\n    token_positions=[2, 3, 4]\n)\n</code></pre>"},{"location":"tutorials/interventions/#all-positions","title":"All Positions","text":"<p>Apply modification to the entire sequence:</p> <pre><code># Modify all tokens\n_, _, modified = intervene.intervene_feature(\n    activations,\n    feature=feature_to_modify,\n    alpha=5.0,\n    token_positions=None  # All positions\n)\n</code></pre>"},{"location":"tutorials/interventions/#comparing-logits","title":"Comparing Logits","text":"<p>Instead of generating text, compare the model's logit distributions:</p> <pre><code># Get logits (not text)\noriginal_logits = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=original_acts,\n    layer=3,\n    generate=False  # Return logits instead\n)\n\nmodified_logits = reinject.reinject_and_generate(\n    text=text,\n    modified_activations=modified_acts,\n    layer=3,\n    generate=False\n)\n\nprint(f\"Original logits shape: {original_logits.shape}\")\nprint(f\"Modified logits shape: {modified_logits.shape}\")\n</code></pre>"},{"location":"tutorials/interventions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/interventions/#no-effect-from-intervention","title":"No Effect from Intervention","text":"<ul> <li>Feature may not be important for this context</li> <li>Try larger alpha values (10-20x)</li> <li>Check that the feature is actually active at your chosen position</li> </ul>"},{"location":"tutorials/interventions/#incoherent-generated-text","title":"Incoherent Generated Text","text":"<ul> <li>Lower alpha values (strong interventions can break coherence)</li> <li>Try intervening on fewer positions</li> <li>Use lower temperature for generation</li> </ul>"},{"location":"tutorials/interventions/#memory-issues","title":"Memory Issues","text":"<pre><code># Clear CUDA cache between experiments\nimport torch\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"tutorials/interventions/#feature-index-out-of-range","title":"Feature Index Out of Range","text":"<pre><code># Check valid range\nprint(f\"Number of features: {intervene.model.encoder.out_features}\")\n# Feature indices: 0 to (n_features - 1)\n</code></pre>"},{"location":"tutorials/sae/","title":"Training Sparse Autoencoders","text":"<p>This tutorial covers how to train sparse autoencoders (SAEs) on extracted activations using DeepLens. SAEs learn to discover interpretable features from neural network representations.</p>"},{"location":"tutorials/sae/#overview","title":"Overview","text":"<p>DeepLens provides two main classes for SAE training:</p> Class Description <code>SparseAutoencoder</code> The SAE model architecture <code>SAETrainer</code> Training framework with logging, checkpointing, and scheduling"},{"location":"tutorials/sae/#sae-architecture","title":"SAE Architecture","text":"<p>The <code>SparseAutoencoder</code> class implements a standard sparse autoencoder with:</p> <ul> <li>Encoder: Projects activations to a higher-dimensional sparse feature space</li> <li>Decoder: Reconstructs activations from sparse features</li> <li>Sparsity: Either TopK selection or L1 regularization</li> </ul> <pre><code>Input (d_model) \u2192 Encoder \u2192 Activation \u2192 Sparsity \u2192 Decoder \u2192 Output (d_model)\n   768/3072         \u00d74-8x      ReLU     TopK/L1       \u00d74-8x      768/3072\n</code></pre>"},{"location":"tutorials/sae/#basic-training","title":"Basic Training","text":""},{"location":"tutorials/sae/#step-1-create-configuration","title":"Step 1: Create Configuration","text":"<p>Create a <code>config.yaml</code> file with your SAE hyperparameters:</p> <pre><code>input_dims: 3072        # GPT-2 MLP intermediate dimension\nn_features: 24576       # Number of learned features (8x expansion)\nactivation: 'relu'      # Activation function\ninput_norm: True        # Apply LayerNorm to inputs\nk: 768                  # TopK sparsity (keep top 768 features)\nbeta_l1: null           # L1 coefficient (null when using TopK)\ntie_weights: False      # Whether to tie encoder/decoder weights\nunit_norm_decoder: True # Constrain decoder columns to unit norm\n</code></pre>"},{"location":"tutorials/sae/#step-2-prepare-data","title":"Step 2: Prepare Data","text":"<pre><code>from deeplens.utils.dataset import ActivationsDatasetBuilder\n\ndataset = ActivationsDatasetBuilder(\n    activations=\"saved_features/features_layer_3_100000.pt\",\n    splits=[0.8, 0.2],\n    batch_size=32,\n    norm=True\n)\n\ntrain_loader, eval_loader = dataset.get_dataloaders()\n</code></pre>"},{"location":"tutorials/sae/#step-3-initialize-model-and-optimizer","title":"Step 3: Initialize Model and Optimizer","text":"<pre><code>from deeplens.sae import SparseAutoencoder\nfrom deeplens.train import SAETrainer\nimport torch\n\n# Load config and create model\nconfig = SAETrainer.config_from_yaml(\"config.yaml\")\nmodel = SparseAutoencoder(**config)\n\n# Initialize optimizer\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=3e-4,\n    betas=(0.9, 0.99),\n    weight_decay=0\n)\n</code></pre>"},{"location":"tutorials/sae/#step-4-train","title":"Step 4: Train","text":"<pre><code>trainer = SAETrainer(\n    model=model,\n    model_name=\"gpt2_layer3_sae\",\n    train_dataloader=train_loader,\n    eval_dataloader=eval_loader,\n    optim=optimizer,\n    epochs=3,\n    bf16=True,\n    device=\"auto\",\n    save_checkpoints=True,\n    eval_steps=5000,\n    log_to_wandb=False\n)\n\ntrainer.train()\n</code></pre>"},{"location":"tutorials/sae/#configuration-deep-dive","title":"Configuration Deep Dive","text":""},{"location":"tutorials/sae/#input-dimensions","title":"Input Dimensions","text":"<p>Match <code>input_dims</code> to your model's MLP intermediate size:</p> Model Layer MLP Dimension GPT-2 3072 GPT-2 Medium 4096 GPT-2 Large 5120 GPT-2 XL 6400"},{"location":"tutorials/sae/#feature-expansion","title":"Feature Expansion","text":"<p>The ratio <code>n_features / input_dims</code> is your expansion factor:</p> <pre><code># Expansion factor examples\nn_features = 8 * input_dims   # 8x expansion (common choice)\nn_features = 4 * input_dims   # 4x expansion (fewer features)\nn_features = 16 * input_dims  # 16x expansion (more features)\n</code></pre> <p>Recommendations:</p> <ul> <li>Start with 8x expansion</li> <li>Use 4x for faster training/smaller models</li> <li>Use 16x for richer feature spaces (requires more data)</li> </ul>"},{"location":"tutorials/sae/#sparsity-methods","title":"Sparsity Methods","text":"<p>DeepLens supports two sparsity approaches:</p>"},{"location":"tutorials/sae/#topk-sparsity-recommended","title":"TopK Sparsity (Recommended)","text":"<p>Keeps only the k largest activations per sample:</p> <pre><code>k: 768         # Keep top 768 features active\nbeta_l1: null  # Disable L1\n</code></pre> <p>Choosing k:</p> <ul> <li>Good starting point: ~3% of active features</li> <li>Lower k = sparser, more interpretable features</li> <li>Higher k = better reconstruction, less sparse</li> </ul> <pre><code># TopK configuration\nmodel = SparseAutoencoder(\n    input_dims=3072,\n    n_features=24576,\n    k=768,            # ~3% of features active\n    beta_l1=None\n)\n</code></pre>"},{"location":"tutorials/sae/#l1-regularization","title":"L1 Regularization","text":"<p>Adds L1 penalty to encourage sparsity:</p> <pre><code>k: None        # Disable TopK\nbeta_l1: 0.001 # L1 coefficient\n</code></pre> <pre><code># L1 configuration\nmodel = SparseAutoencoder(\n    input_dims=3072,\n    n_features=24576,\n    k=None,\n    beta_l1=0.001\n)\n</code></pre> <p>Tuning beta_l1:</p> <ul> <li>Too high \u2192 features become too sparse, poor reconstruction</li> <li>Too low \u2192 features not sparse enough</li> <li>Start with 0.001 and adjust based on reconstruction loss and sparsity</li> </ul>"},{"location":"tutorials/sae/#weight-tying","title":"Weight Tying","text":"<p>Controls whether decoder weights are tied to encoder weights:</p> <pre><code># Untied weights (recommended)\nmodel = SparseAutoencoder(\n    tie_weights=False,\n    unit_norm_decoder=True\n)\n\n# Tied weights (fewer parameters)\nmodel = SparseAutoencoder(\n    tie_weights=True,\n    unit_norm_decoder=False  # No separate decoder to normalize\n)\n</code></pre> <p>Recommendations:</p> <ul> <li>Use <code>tie_weights=False</code> for better reconstruction</li> <li>Use <code>tie_weights=True</code> to reduce parameters (faster training)</li> </ul>"},{"location":"tutorials/sae/#activation-function","title":"Activation Function","text":"<pre><code># ReLU (standard, recommended)\nmodel = SparseAutoencoder(activation='relu')\n\n# SiLU (may improve reconstruction)\nmodel = SparseAutoencoder(activation='silu')\n</code></pre>"},{"location":"tutorials/sae/#trainer-configuration","title":"Trainer Configuration","text":""},{"location":"tutorials/sae/#basic-trainer-setup","title":"Basic Trainer Setup","text":"<pre><code>trainer = SAETrainer(\n    model=model,\n    model_name=\"my_sae\",\n    train_dataloader=train_loader,\n    eval_dataloader=eval_loader,\n    optim=optimizer,\n    epochs=3,\n    device=\"auto\"\n)\n</code></pre>"},{"location":"tutorials/sae/#all-trainer-parameters","title":"All Trainer Parameters","text":"Parameter Description Default <code>model</code> SparseAutoencoder instance None <code>model_name</code> Name for saving checkpoints \"sae\" <code>train_dataloader</code> Training data DataLoader None <code>eval_dataloader</code> Evaluation data DataLoader None <code>optim</code> Optimizer instance Adam <code>epochs</code> Number of training epochs 20 <code>bf16</code> Enable bfloat16 mixed precision False <code>random_seed</code> Seed for reproducibility 42 <code>save_checkpoints</code> Save model checkpoints True <code>device</code> Training device \"auto\" <code>grad_clip_norm</code> Gradient clipping threshold None <code>lrs_type</code> LR scheduler type None <code>eval_steps</code> Steps between evaluations 5000 <code>warmup_fraction</code> Warmup fraction for LR 0.1 <code>save_best_only</code> Only save best checkpoint True <code>log_to_wandb</code> Enable W&amp;B logging True"},{"location":"tutorials/sae/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>DeepLens supports three learning rate schedulers:</p> <pre><code># Cosine annealing with warmup (recommended)\ntrainer = SAETrainer(\n    lrs_type='cosine',\n    warmup_fraction=0.1  # 10% warmup\n)\n\n# Linear decay with warmup\ntrainer = SAETrainer(\n    lrs_type='linear',\n    warmup_fraction=0.1\n)\n\n# Reduce on plateau\ntrainer = SAETrainer(\n    lrs_type='plateau'  # Reduces LR when loss plateaus\n)\n</code></pre>"},{"location":"tutorials/sae/#gradient-clipping","title":"Gradient Clipping","text":"<p>Prevent gradient explosion:</p> <pre><code>trainer = SAETrainer(\n    grad_clip_norm=3.0  # Clip gradients to max norm of 3.0\n)\n</code></pre>"},{"location":"tutorials/sae/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Enable bfloat16 for faster training on modern GPUs:</p> <pre><code>trainer = SAETrainer(\n    bf16=True  # Requires CUDA with bf16 support\n)\n</code></pre>"},{"location":"tutorials/sae/#monitoring-training","title":"Monitoring Training","text":""},{"location":"tutorials/sae/#console-output","title":"Console Output","text":"<p>Training progress is printed every 100 steps:</p> <pre><code>Step [0/1000] - train_loss: 0.245 - train_nz_frac: 0.032 - lr: 3.00e-04\nStep [100/1000] - train_loss: 0.189 - train_nz_frac: 0.031 - lr: 3.00e-04\n</code></pre> <p>Key metrics:</p> <ul> <li>train_loss: Reconstruction MSE (lower is better)</li> <li>train_nz_frac: Fraction of non-zero features (sparsity indicator)</li> <li>lr: Current learning rate</li> </ul>"},{"location":"tutorials/sae/#weights-biases","title":"Weights &amp; Biases","text":"<p>Enable W&amp;B for detailed tracking:</p> <pre><code>trainer = SAETrainer(\n    log_to_wandb=True\n)\n</code></pre> <p>Logged metrics:</p> <ul> <li><code>train/loss</code>: Training reconstruction loss</li> <li><code>train/non_zero_frac</code>: Sparsity level</li> <li><code>train/lr</code>: Learning rate</li> <li><code>eval/loss</code>: Evaluation loss</li> </ul>"},{"location":"tutorials/sae/#interpreting-metrics","title":"Interpreting Metrics","text":"<p>Good training signs:</p> <ul> <li>Reconstruction loss decreases steadily</li> <li>Non-zero fraction stays around target sparsity</li> <li>Evaluation loss tracks training loss (no overfitting)</li> </ul> <p>Warning signs:</p> <ul> <li>Loss increases or oscillates wildly \u2192 reduce learning rate</li> <li>Non-zero fraction approaches 1.0 \u2192 increase sparsity (lower k or higher L1)</li> <li>Non-zero fraction near 0 \u2192 decrease sparsity (higher k or lower L1)</li> </ul>"},{"location":"tutorials/sae/#checkpointing","title":"Checkpointing","text":""},{"location":"tutorials/sae/#automatic-checkpointing","title":"Automatic Checkpointing","text":"<p>Models are saved when evaluation loss improves:</p> <pre><code>trainer = SAETrainer(\n    save_checkpoints=True,\n    save_best_only=True,  # Only keep best model\n    eval_steps=5000       # Evaluate every 5000 steps\n)\n</code></pre> <p>Checkpoints saved to: <code>saved_models/{model_name}/run_{timestamp}/best_model.pt</code></p>"},{"location":"tutorials/sae/#advanced-training","title":"Advanced Training","text":""},{"location":"tutorials/sae/#longer-training","title":"Longer Training","text":"<p>For more training data, increase epochs:</p> <pre><code>trainer = SAETrainer(\n    epochs=10,\n    eval_steps=10000  # Adjust eval frequency\n)\n</code></pre>"},{"location":"tutorials/sae/#model-architecture-details","title":"Model Architecture Details","text":""},{"location":"tutorials/sae/#forward-pass","title":"Forward Pass","text":"<pre><code># The model's forward pass\ndef forward(self, x):\n    z_pre = self.encode(x)       # Encode to latent space\n    z = self.topk_mask(z_pre)    # Apply sparsity (if using TopK)\n    x_hat = self.decode(z)       # Decode back\n    return x_hat, z, z_pre\n</code></pre>"},{"location":"tutorials/sae/#loss-function","title":"Loss Function","text":"<pre><code># The model's loss computation\nloss = MSE(x_hat, x) + beta_l1 * L1(z)  # L1 mode\n# or\nloss = MSE(x_hat, x)                     # TopK mode\n</code></pre>"},{"location":"tutorials/sae/#unit-norm-decoder","title":"Unit Norm Decoder","text":"<p>The decoder weights are normalized after each step:</p> <pre><code># After each optimizer step\nmodel.post_step()  # Renormalizes decoder columns to unit norm\n</code></pre> <p>This is automatically handled by <code>SAETrainer</code>.</p>"},{"location":"tutorials/sae/#full-training-script","title":"Full Training Script","text":"<p>Here's a complete training script:</p> <pre><code>from deeplens.sae import SparseAutoencoder\nfrom deeplens.train import SAETrainer\nfrom deeplens.utils.dataset import ActivationsDatasetBuilder\nimport torch\n\n# 1. Load data\ndataset = ActivationsDatasetBuilder(\n    activations=\"saved_features/features_layer_3_1171436.pt\",\n    splits=[0.8, 0.2],\n    batch_size=16,\n    norm=True\n)\ntrain, eval = dataset.get_dataloaders(ddp=False)\n\nconfig = SAETrainer.config_from_yaml('demo/config.yaml')\nmodel = SparseAutoencoder(**config)\n\n# 3. Setup optimizer\noptimizer = torch.optim.Adam(\n    model.parameters(), \n    lr=0.0003, \n    betas=(0.9,0.99),\n    weight_decay=1e-4 # Just when using untied weights! Else set to 0\n)\n\n# 4. Train\ntrainer = SAETrainer(\n    model=model,\n    model_name=\"gpt2_layer3_sae\",\n    train_dataloader=train,\n    eval_dataloader=eval,\n    optim=optimizer,\n    epochs=3,\n    bf16=True,\n    random_seed=42,\n    save_checkpoints=True,\n    device=\"auto\",\n    grad_clip_norm=3.0,\n    lrs_type='cosine',\n    eval_steps=5000,\n    warmup_fraction=0.1,\n    save_best_only=True,\n    log_to_wandb=True\n)\n\ntrainer.train()\n</code></pre>"},{"location":"tutorials/sae/#next-steps","title":"Next Steps","text":"<p>After training your SAE:</p> <ol> <li>Feature Analysis - Analyze and visualize learned features</li> <li>Feature Interventions - Test causal effects of features</li> </ol>"},{"location":"tutorials/sae/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/sae/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<ul> <li>Lower learning rate</li> <li>Check data normalization</li> <li>Increase model capacity (more features)</li> </ul>"},{"location":"tutorials/sae/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Reduce batch size</li> <li>Reduce <code>n_features</code></li> <li>Enable <code>bf16=True</code></li> </ul>"},{"location":"tutorials/sae/#nan-loss","title":"NaN Loss","text":"<ul> <li>Enable gradient clipping: <code>grad_clip_norm=1.0</code></li> <li>Lower learning rate</li> <li>Check input data for NaN/Inf values</li> </ul>"},{"location":"tutorials/sae/#poor-reconstruction","title":"Poor Reconstruction","text":"<ul> <li>Increase <code>k</code> (more active features)</li> <li>Decrease <code>beta_l1</code></li> <li>Train longer (more epochs)</li> <li>Use more training data</li> </ul>"}]}